{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG 시스템을 위한 컨텍스트 압축\n",
    "이 노트북에서는 RAG 시스템의 효율성을 향상시키기 위한 컨텍스트 압축 기술을 구현합니다. 검색된 텍스트 청크를 필터링하고 압축하여 가장 관련성 높은 부분만 유지함으로써 노이즈를 줄이고 응답 품질을 향상시킵니다.\n",
    "\n",
    "RAG를 위해 문서를 검색할 때, 종종 관련 정보와 관련 없는 정보가 모두 포함된 청크를 얻게 됩니다. 컨텍스트 압축은 다음을 돕습니다:\n",
    "\n",
    "- 관련 없는 문장 및 단락 제거\n",
    "- 질의와 관련된 정보에만 집중\n",
    "- 컨텍스트 창에서 유용한 신호 극대화\n",
    "\n",
    "이 접근 방식을 처음부터 구현해 봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\") # OpenAI API 키 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구축\n",
    "FAISS를 사용할 수 없으므로 간단한 벡터 저장소를 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []  # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 변환하여 벡터 리스트에 추가\n",
    "        self.texts.append(text)  # 원본 텍스트를 텍스트 리스트에 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터를 메타데이터 리스트에 추가 (None이면 빈 딕셔너리 사용)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 질의 임베딩 벡터.\n",
    "        k (int): 반환할 결과 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트 및 메타데이터와 함께 상위 k개의 가장 유사한 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 인덱스에 해당하는 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 인덱스에 해당하는 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 결과 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text,  model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    # 문자열 및 리스트 입력을 모두 처리하기 위해 input_text가 항상 리스트가 되도록 보장\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 단일 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 그렇지 않으면 입력 텍스트 목록에 대한 모든 임베딩 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG를 위해 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "    chunk_size (int): 각 청크의 문자 크기.\n",
    "    chunk_overlap (int): 청크 간 문자 중첩.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: 문서 청크와 해당 임베딩을 포함하는 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 추출된 텍스트를 더 작은 세그먼트로 청킹\n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    # 각 텍스트 청크에 대한 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 청크와 해당 임베딩을 저장할 간단한 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 해당 임베딩을 벡터 저장소에 추가\n",
    "    for i, (chunk_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컨텍스트 압축 구현\n",
    "이것이 우리 접근 방식의 핵심입니다. LLM을 사용하여 검색된 콘텐츠를 필터링하고 압축합니다.\n",
    "컨텍스트 압축은 검색된 청크에서 실제로 사용자의 질의와 관련된 부분만을 추출하거나 요약하여, LLM에 전달되는 컨텍스트의 양을 줄이고 관련성을 높이는 기법입니다. 이를 통해 LLM이 더 적은 정보로도 정확한 답변을 생성하도록 유도하고, 토큰 사용량을 줄여 비용 효율성도 높일 수 있습니다.\n",
    "압축 방식에는 여러 가지가 있을 수 있습니다:\n",
    "- **선택적(Selective) 압축**: 질의와 직접적으로 관련된 문장이나 단락만 추출합니다.\n",
    "- **요약(Summary) 압축**: 질의와 관련된 정보만을 중심으로 청크 내용을 간결하게 요약합니다.\n",
    "- **추출(Extraction) 압축**: 질의와 관련된 문장을 원본 텍스트에서 그대로 인용하여 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunk(chunk, query, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 관련된 부분만 유지하도록 검색된 청크를 압축합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunk (str): 압축할 텍스트 청크\n",
    "        query (str): 사용자 질의\n",
    "        compression_type (str): 압축 유형 (\"selective\", \"summary\" 또는 \"extraction\")\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 압축된 청크\n",
    "    \"\"\"\n",
    "    # 다양한 압축 접근 방식에 대한 시스템 프롬프트 정의\n",
    "    if compression_type == \"selective\":\n",
    "        system_prompt = \"\"\"당신은 정보 필터링 전문가입니다. \n",
    "        당신의 임무는 문서 청크를 분석하고 사용자의 질의와 직접적으로 관련된 문장이나 단락만을 추출하는 것입니다. 관련 없는 모든 콘텐츠를 제거하십시오.\n",
    "\n",
    "        당신의 출력은 다음을 따라야 합니다:\n",
    "        1. 질의에 답변하는 데 도움이 되는 텍스트만 포함하십시오.\n",
    "        2. 관련 문장의 정확한 표현을 보존하십시오 (의역하지 마십시오).\n",
    "        3. 텍스트의 원래 순서를 유지하십시오.\n",
    "        4. 중복되어 보이더라도 모든 관련 콘텐츠를 포함하십시오.\n",
    "        5. 질의와 관련 없는 텍스트는 제외하십시오.\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정하십시오.\"\"\"\n",
    "    elif compression_type == \"summary\":\n",
    "        system_prompt = \"\"\"당신은 요약 전문가입니다. \n",
    "        당신의 임무는 사용자의 질의와 관련된 정보에만 초점을 맞춘 제공된 청크의 간결한 요약을 만드는 것입니다.\n",
    "\n",
    "        당신의 출력은 다음을 따라야 합니다:\n",
    "        1. 질의 관련 정보에 대해 간결하지만 포괄적이어야 합니다.\n",
    "        2. 질의와 관련된 정보에만 독점적으로 초점을 맞추십시오.\n",
    "        3. 관련 없는 세부 정보는 생략하십시오.\n",
    "        4. 중립적이고 사실적인 어조로 작성하십시오.\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정하십시오.\"\"\"\n",
    "    else:  # extraction (추출)\n",
    "        system_prompt = \"\"\"당신은 정보 추출 전문가입니다.\n",
    "        당신의 임무는 사용자의 질의에 답변하는 데 관련된 정보가 포함된 문서 청크에서 정확한 문장만을 추출하는 것입니다.\n",
    "\n",
    "        당신의 출력은 다음을 따라야 합니다:\n",
    "        1. 원본 텍스트에서 관련 문장의 직접적인 인용만 포함하십시오.\n",
    "        2. 원본 표현을 보존하십시오 (텍스트를 수정하지 마십시오).\n",
    "        3. 질의와 직접적으로 관련된 문장만 포함하십시오.\n",
    "        4. 추출된 문장을 줄 바꿈으로 구분하십시오.\n",
    "        5. 해설이나 추가 텍스트를 추가하지 마십시오.\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정하십시오.\"\"\"\n",
    "\n",
    "    # 질의와 문서 청크를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "        질의: {query}\n",
    "\n",
    "        문서 청크:\n",
    "        {chunk}\n",
    "\n",
    "        이 질의에 답변하는 데 관련된 콘텐츠만 추출하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenAI API를 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 결정론적 출력을 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 응답에서 압축된 청크 추출\n",
    "    compressed_chunk = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 압축률 계산\n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    # 분모가 0이 되는 경우 방지\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100 if original_length > 0 else 0\n",
    "    \n",
    "    return compressed_chunk, compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 압축 구현\n",
    "효율성을 위해 가능한 경우 한 번에 여러 청크를 압축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compress_chunks(chunks, query, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    여러 청크를 개별적으로 압축합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): 압축할 텍스트 청크 목록\n",
    "        query (str): 사용자 질의\n",
    "        compression_type (str): 압축 유형 (\"selective\", \"summary\" 또는 \"extraction\")\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: 압축률과 함께 압축된 청크 목록\n",
    "    \"\"\"\n",
    "    print(f\"{len(chunks)}개의 청크 압축 중...\")  # 압축할 청크 수 출력\n",
    "    results = []  # 결과를 저장할 빈 리스트 초기화\n",
    "    total_original_length = 0  # 청크의 총 원본 길이를 저장할 변수 초기화\n",
    "    total_compressed_length = 0  # 청크의 총 압축 길이를 저장할 변수 초기화\n",
    "    \n",
    "    # 각 청크 반복\n",
    "    for i, chunk_content in enumerate(chunks): # chunk를 chunk_content로 변경\n",
    "        print(f\"청크 {i+1}/{len(chunks)} 압축 중...\")  # 압축 진행 상황 출력\n",
    "        # 청크를 압축하고 압축된 청크와 압축률 가져오기\n",
    "        compressed_chunk, compression_ratio = compress_chunk(chunk_content, query, compression_type, model)\n",
    "        results.append((compressed_chunk, compression_ratio))  # 결과를 results 리스트에 추가\n",
    "        \n",
    "        total_original_length += len(chunk_content)  # 원본 청크 길이를 총 원본 길이에 추가\n",
    "        total_compressed_length += len(compressed_chunk)  # 압축된 청크 길이를 총 압축 길이에 추가\n",
    "    \n",
    "    # 전체 압축률 계산 (분모가 0인 경우 방지)\n",
    "    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100 if total_original_length > 0 else 0\n",
    "    print(f\"전체 압축률: {overall_ratio:.2f}%\")  # 전체 압축률 출력\n",
    "    \n",
    "    return results  # 압축률과 함께 압축된 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 응답 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 압축된 청크의 컨텍스트 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # AI의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 도움이 되는 AI 어시스턴트입니다. 제공된 컨텍스트만을 기반으로 사용자의 질문에 답변하십시오.\n",
    "    컨텍스트에서 답변을 찾을 수 없으면 정보가 충분하지 않다고 명시하십시오.\"\"\"\n",
    "            \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "        컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질문: {query}\n",
    "\n",
    "        위에 제공된 컨텍스트만을 기반으로 포괄적인 답변을 제공하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenAI API를 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컨텍스트 압축을 사용한 전체 RAG 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_compression(pdf_path, query, k=10, compression_type=\"selective\", model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    컨텍스트 압축을 사용한 전체 RAG 파이프라인.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        k (int): 초기에 검색할 청크 수\n",
    "        compression_type (str): 압축 유형\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        dict: 질의, 압축된 청크 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 컨텍스트 압축을 사용한 RAG ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    print(f\"압축 유형: {compression_type}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청킹 및 임베딩 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 질의에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 질의 임베딩을 기반으로 상위 k개의 가장 유사한 청크 검색\n",
    "    print(f\"상위 {k}개 청크 검색 중...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크에 압축 적용\n",
    "    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model)\n",
    "    compressed_chunks_content = [result[0] for result in compressed_results] # compressed_chunks를 compressed_chunks_content로 변경\n",
    "    compression_ratios = [result[1] for result in compressed_results]\n",
    "    \n",
    "    # 비어 있는 압축된 청크 필터링\n",
    "    filtered_chunks_data = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks_content, compression_ratios) if chunk.strip()] # compressed_chunks를 compressed_chunks_content로 변경\n",
    "    \n",
    "    if not filtered_chunks_data:\n",
    "        # 모든 청크가 빈 문자열로 압축되면 원본 청크 사용\n",
    "        print(\"경고: 모든 청크가 빈 문자열로 압축되었습니다. 원본 청크를 사용합니다.\")\n",
    "        final_compressed_chunks = retrieved_chunks # filtered_chunks를 final_compressed_chunks로 변경\n",
    "        final_compression_ratios = [0.0] * len(retrieved_chunks) # filtered_chunks를 final_compression_ratios로 변경\n",
    "    else:\n",
    "        final_compressed_chunks, final_compression_ratios = zip(*filtered_chunks_data) # compressed_chunks를 final_compressed_chunks로, compression_ratios를 final_compression_ratios로 변경\n",
    "    \n",
    "    # 압축된 청크에서 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(final_compressed_chunks) # compressed_chunks를 final_compressed_chunks로 변경\n",
    "    \n",
    "    # 압축된 청크를 기반으로 응답 생성\n",
    "    print(\"압축된 청크를 기반으로 응답 생성 중...\")\n",
    "    response_text = generate_response(query, context, model) # response를 response_text로 변경\n",
    "    \n",
    "    # 결과 딕셔너리 준비\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"original_chunks\": retrieved_chunks,\n",
    "        \"compressed_chunks\": list(final_compressed_chunks), # compressed_chunks를 final_compressed_chunks로 변경하고 list로 변환\n",
    "        \"compression_ratios\": list(final_compression_ratios), # compression_ratios를 final_compression_ratios로 변경하고 list로 변환\n",
    "        \"context_length_reduction\": f\"{sum(final_compression_ratios)/len(final_compression_ratios) if final_compression_ratios else 0:.2f}%\", # compression_ratios를 final_compression_ratios로 변경하고 비어있을 경우 0 처리\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 압축 사용 RAG와 미사용 RAG 비교\n",
    "압축 강화 버전과 표준 RAG를 비교하는 함수를 만들어 보겠습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(pdf_path, query, k=10, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    압축 없는 표준 RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        k (int): 검색할 청크 수\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        dict: 질의, 청크 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 표준 RAG ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청킹 및 임베딩 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 질의에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 질의 임베딩을 기반으로 상위 k개의 가장 유사한 청크 검색\n",
    "    print(f\"상위 {k}개 청크 검색 중...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크에서 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # 검색된 청크를 기반으로 응답 생성\n",
    "    print(\"응답 생성 중...\")\n",
    "    response_text = generate_response(query, context, model) # response를 response_text로 변경\n",
    "    \n",
    "    # 결과 딕셔너리 준비\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 접근 방식 평가\n",
    "이제 응답을 평가하고 비교하는 함수를 구현해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, responses, reference_answer):\n",
    "    \"\"\"\n",
    "    참조 답변과 여러 응답을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        responses (Dict[str, str]): 방법별 응답 딕셔너리\n",
    "        reference_answer (str): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 평가 텍스트\n",
    "    \"\"\"\n",
    "    # 평가를 위한 AI의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 RAG 응답의 객관적인 평가자입니다. 동일한 질의에 대한 여러 응답을 비교하고\n",
    "    어떤 것이 가장 정확하고, 포괄적이며, 질의와 관련성이 높은지 결정하십시오.\"\"\"\n",
    "    \n",
    "    # 질의와 참조 답변을 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "    질의: {query}\n",
    "\n",
    "    참조 답변: {reference_answer}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # 각 응답을 프롬프트에 추가\n",
    "    for method, response_text in responses.items(): # response를 response_text로 변경\n",
    "        user_prompt += f\"\\n{method.capitalize()} 응답:\\n{response_text}\\n\"\n",
    "    \n",
    "    # 평가 기준을 사용자 프롬프트에 추가\n",
    "    user_prompt += \"\"\"\n",
    "    다음을 기준으로 이러한 응답을 평가하십시오:\n",
    "    1. 참조 답변과 비교한 사실적 정확성\n",
    "    2. 포괄성 - 질의에 얼마나 완전하게 답변하는가\n",
    "    3. 간결성 - 관련 없는 정보를 피하는가\n",
    "    4. 전반적인 품질\n",
    "\n",
    "    상세한 설명과 함께 응답을 최고에서 최악 순으로 순위를 매기십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenAI API를 사용하여 평가 응답 생성\n",
    "    evaluation_response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 응답에서 평가 텍스트 반환\n",
    "    return evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression(pdf_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
    "    \"\"\"\n",
    "    표준 RAG와 다양한 압축 기술을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        reference_answer (str): 선택적 참조 답변\n",
    "        compression_types (List[str]): 평가할 압축 유형\n",
    "        \n",
    "    Returns:\n",
    "        dict: 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 컨텍스트 압축 평가 ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 압축 없는 표준 RAG 실행\n",
    "    standard_result = standard_rag(pdf_path, query)\n",
    "    \n",
    "    # 다양한 압축 기술 결과를 저장할 딕셔너리\n",
    "    compression_results = {}\n",
    "    \n",
    "    # 각 압축 기술로 RAG 실행\n",
    "    for comp_type in compression_types:\n",
    "        print(f\"\\n{comp_type} 압축 테스트 중...\")\n",
    "        compression_results[comp_type] = rag_with_compression(pdf_path, query, compression_type=comp_type)\n",
    "    \n",
    "    # 평가를 위한 응답 수집\n",
    "    responses_to_evaluate = {\n",
    "        \"standard\": standard_result[\"response\"]\n",
    "    }\n",
    "    for comp_type in compression_types:\n",
    "        responses_to_evaluate[comp_type] = compression_results[comp_type][\"response\"]\n",
    "    \n",
    "    # 참조 답변이 제공되면 응답 평가\n",
    "    if reference_answer:\n",
    "        evaluation_text = evaluate_responses(query, responses_to_evaluate, reference_answer) # evaluation을 evaluation_text로 변경\n",
    "        print(\"\\n=== 평가 결과 ===\")\n",
    "        print(evaluation_text)\n",
    "    else:\n",
    "        evaluation_text = \"평가를 위한 참조 답변이 제공되지 않았습니다.\"\n",
    "    \n",
    "    # 각 압축 유형에 대한 메트릭 계산\n",
    "    metrics = {}\n",
    "    for comp_type in compression_types:\n",
    "        avg_comp_ratio = sum(compression_results[comp_type]['compression_ratios']) / len(compression_results[comp_type]['compression_ratios']) if compression_results[comp_type]['compression_ratios'] else 0\n",
    "        metrics[comp_type] = {\n",
    "            \"avg_compression_ratio\": f\"{avg_comp_ratio:.2f}%\",\n",
    "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n",
    "            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n",
    "        }\n",
    "    \n",
    "    # 평가 결과, 응답 및 메트릭 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"responses\": responses_to_evaluate,\n",
    "        \"evaluation\": evaluation_text,\n",
    "        \"metrics\": metrics,\n",
    "        \"standard_result\": standard_result,\n",
    "        \"compression_results\": compression_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 시스템 실행 (사용자 정의 질의)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATING CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "\n",
      "=== STANDARD RAG ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Generating response...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "1. Bias and Fairness: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "2. Transparency and Explainability: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions, which can lead to a lack of trust and accountability.\n",
      "3. Privacy and Data Protection: AI systems often rely on large amounts of data, raising concerns about privacy and data protection, and ensuring responsible data handling is crucial.\n",
      "4. Accountability and Responsibility: Establishing accountability and responsibility for AI systems is essential for addressing potential harms and ensuring ethical behavior.\n",
      "5. Unintended Consequences: As AI systems become more autonomous, questions arise about control, accountability, and the potential for unintended consequences.\n",
      "\n",
      "These concerns highlight the need for careful consideration of the ethical implications of AI in decision-making, including:\n",
      "\n",
      "* Ensuring fairness and mitigating bias in AI systems\n",
      "* Enhancing transparency and explainability in AI decision-making\n",
      "* Protecting sensitive information and ensuring responsible data handling\n",
      "* Establishing clear guidelines and ethical frameworks for AI development and deployment\n",
      "* Addressing the potential for unintended consequences and ensuring accountability and responsibility\n",
      "\n",
      "By addressing these concerns, we can build trust in AI systems and ensure that they are developed and deployed in a way that aligns with societal values and promotes the well-being of individuals and society.\n",
      "\n",
      "Testing selective compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: selective\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 39.93%\n",
      "Generating response based on compressed chunks...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "1. Bias and Fairness: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "2. Transparency and Explainability: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions, which can lead to a lack of trust and accountability.\n",
      "3. Privacy and Data Protection: AI systems often rely on large amounts of data, raising concerns about privacy and data protection, and ensuring responsible data handling is crucial.\n",
      "4. Safety and Control: As AI systems become more autonomous, questions arise about control, accountability, and the potential for unintended consequences.\n",
      "5. Accountability and Responsibility: Establishing accountability and responsibility for AI systems is essential for addressing potential harms and ensuring ethical behavior.\n",
      "6. Economic and Social Impacts: Addressing the potential economic and social impacts of AI-driven automation is a key challenge.\n",
      "7. Respect for Human Rights: AI systems should be designed to respect human rights, including privacy, non-discrimination, and beneficence.\n",
      "8. Robustness and Reliability: Ensuring that AI systems are robust and reliable is essential for building trust.\n",
      "9. Empowerment of Users: Empowering users with control over AI systems and providing them with agency in their interactions with AI enhances trust.\n",
      "10. Ethical Considerations in Design and Development: Incorporating ethical considerations into the design and development of AI systems is crucial for building trust.\n",
      "\n",
      "These concerns highlight the need for a comprehensive approach to addressing the ethical implications of AI in decision-making, including the development of clear guidelines, ethical frameworks, and regulations to ensure that AI systems are designed and deployed in a responsible and trustworthy manner.\n",
      "\n",
      "Testing summary compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: summary\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 63.87%\n",
      "Generating response based on compressed chunks...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "1. Bias and Fairness: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "2. Transparency and Explainability: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions.\n",
      "3. Privacy and Security: The reliance on large amounts of data raises concerns about data security and the potential for unauthorized access or misuse.\n",
      "4. Job Displacement: The potential for AI systems to automate repetitive or routine tasks raises concerns about job displacement and the impact on workers.\n",
      "5. Control, Accountability, and Unintended Consequences: As AI systems become more autonomous, there are concerns about who is responsible for their actions, and the potential for unintended consequences.\n",
      "6. Need for Clear Guidelines and Ethical Frameworks: There is a need for clear guidelines and ethical frameworks to ensure that AI systems are developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "These concerns highlight the importance of addressing the ethical implications of AI in decision-making, and the need for a balanced approach that promotes innovation while protecting human rights, privacy, and well-being.\n",
      "\n",
      "Testing extraction compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: extraction\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 54.41%\n",
      "Generating response based on compressed chunks...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "1. Bias and Fairness: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes. Ensuring fairness and mitigating bias in AI systems is a critical challenge.\n",
      "\n",
      "2. Lack of Transparency and Explainability: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. This lack of transparency and explainability can lead to a lack of trust in AI systems.\n",
      "\n",
      "3. Accountability and Responsibility: Establishing accountability and responsibility for AI systems is essential for addressing potential harms and ensuring ethical behavior. This includes defining roles and responsibilities for developers, deployers, and users of AI systems.\n",
      "\n",
      "4. Respect for Human Rights, Privacy, and Non-Discrimination: AI systems must be designed and deployed in a way that respects human rights, protects privacy, and avoids non-discrimination.\n",
      "\n",
      "5. Beneficence: AI systems should be designed and deployed in a way that promotes the well-being and benefit of society.\n",
      "\n",
      "6. Addressing Bias in Data Collection, Algorithm Design, and Ongoing Monitoring and Evaluation: Addressing bias requires careful data collection, algorithm design, and ongoing monitoring and evaluation.\n",
      "\n",
      "7. Ensuring Robustness and Reliability: Ensuring that AI systems are robust and reliable is essential for building trust. This includes testing and validating AI models, monitoring their performance, and addressing potential vulnerabilities.\n",
      "\n",
      "8. Empowering Users with Control: Empowering users with control over AI systems and providing them with agency in their interactions with AI enhances trust. This includes allowing users to customize AI settings, understand how their data is used, and opt out of AI-driven features.\n",
      "\n",
      "9. International Discussions and Regulations: The potential use of AI in autonomous weapons systems raises significant ethical and security concerns, and international discussions and regulations are needed to address the risks associated with AI-powered weapons.\n",
      "\n",
      "10. Public Perception and Trust: Public perception and trust in AI are essential for its widespread adoption and positive social impact.\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Based on the evaluation criteria, here are the rankings from best to worst:\n",
      "\n",
      "1. **Reference Answer**: This response is the most accurate, comprehensive, and relevant to the query. It provides a clear and concise overview of the ethical concerns surrounding the use of AI in decision-making, covering topics such as bias, transparency, privacy, accountability, and job displacement. The response is well-structured and easy to follow, making it an excellent example of a high-quality response.\n",
      "\n",
      "2. **Standard Response**: This response is comprehensive and covers all the key ethical concerns surrounding AI in decision-making. It provides a clear and concise overview of the issues, including bias, transparency, privacy, accountability, and job displacement. The response is well-organized and easy to follow, making it a strong contender for the top spot.\n",
      "\n",
      "3. **Selective Response**: This response is comprehensive, but it lacks some of the key points mentioned in the reference answer. It covers bias, transparency, privacy, and accountability, but misses out on job displacement and control. The response is still well-organized and easy to follow, but it falls short of the reference answer in terms of comprehensiveness.\n",
      "\n",
      "4. **Summary Response**: This response is concise and covers the key points, but it lacks some of the detail and depth of the reference answer. It provides a good overview of the ethical concerns, but it doesn't delve as deeply into the issues as the reference answer. The response is still clear and easy to follow, but it falls short of the standard response in terms of comprehensiveness.\n",
      "\n",
      "5. **Extraction Response**: This response is concise and covers the key points, but it lacks some of the detail and depth of the reference answer. It provides a good overview of the ethical concerns, but it doesn't delve as deeply into the issues as the reference answer. The response is still clear and easy to follow, but it falls short of the standard response in terms of comprehensiveness.\n",
      "\n",
      "Ranking Criteria:\n",
      "\n",
      "* Factual accuracy: Reference Answer (9/10), Standard Response (8.5/10), Selective Response (8/10), Summary Response (7.5/10), Extraction Response (7/10)\n",
      "* Comprehensiveness: Reference Answer (9/10), Standard Response (8.5/10), Selective Response (8/10), Summary Response (7.5/10), Extraction Response (7/10)\n",
      "* Conciseness: Summary Response (8/10), Extraction Response (7.5/10), Selective Response (7/10), Standard Response (6.5/10), Reference Answer (6/10)\n",
      "* Overall quality: Reference Answer (9/10), Standard Response (8.5/10), Selective Response (8/10), Summary Response (7.5/10), Extraction Response (7/10)\n",
      "\n",
      "Note: The scores are subjective and based on the evaluation criteria. They are intended to provide a general ranking of the responses rather than a precise numerical score.\n"
     ]
    }
   ],
   "source": [
    "# AI 윤리에 대한 정보가 포함된 PDF 문서 경로  \n",
    "pdf_path = \"data/AI_Information.pdf\" \n",
    "\n",
    "# 문서에서 관련 정보를 추출하기 위한 질의  \n",
    "query = \"의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가?\"  \n",
    "\n",
    "# 평가를 위한 선택적 참조 답변  \n",
    "reference_answer = \"\"\"  \n",
    "의사 결정에 AI를 사용하는 것은 몇 가지 윤리적 우려를 제기합니다.  \n",
    "- AI 모델의 편향은 특히 고용, 대출, 법 집행과 같은 중요한 영역에서 불공정하거나 차별적인 결과를 초래할 수 있습니다.  \n",
    "- AI 기반 의사 결정의 투명성과 설명 가능성 부족은 개인이 불공정한 결과에 이의를 제기하기 어렵게 만듭니다.  \n",
    "- AI 시스템이 방대한 양의 개인 데이터를 처리함에 따라 개인 정보 보호 위험이 발생하며, 종종 명시적인 동의 없이 이루어집니다.  \n",
    "- 자동화로 인한 잠재적인 일자리 감소는 사회적, 경제적 우려를 야기합니다.  \n",
    "- AI 의사 결정은 또한 소수의 대형 기술 회사에 권력을 집중시켜 책임성 문제를 야기할 수 있습니다.  \n",
    "- AI 시스템의 윤리적 배포를 위해서는 공정성, 책임성 및 투명성을 보장하는 것이 필수적입니다.  \n",
    "\"\"\"  \n",
    "\n",
    "# 다양한 압축 기술을 사용하여 평가 실행  \n",
    "# 압축 유형:  \n",
    "# - \"selective\": 덜 관련된 부분을 생략하면서 주요 세부 정보 유지  \n",
    "# - \"summary\": 정보의 간결한 버전 제공  \n",
    "# - \"extraction\": 문서에서 관련 문장을 그대로 추출  \n",
    "results = evaluate_compression(  \n",
    "    pdf_path=pdf_path,  \n",
    "    query=query,  \n",
    "    reference_answer=reference_answer,  \n",
    "    compression_types=[\"selective\", \"summary\", \"extraction\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 압축 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_compression_results(evaluation_results):\n",
    "    \"\"\"\n",
    "    다양한 압축 기술의 결과를 시각화합니다.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results (Dict): evaluate_compression 함수의 결과\n",
    "    \"\"\"\n",
    "    # 평가 결과에서 질의와 표준 청크 추출\n",
    "    query = evaluation_results[\"query\"]\n",
    "    standard_chunks = evaluation_results[\"standard_result\"][\"chunks\"]\n",
    "    \n",
    "    # 질의 출력\n",
    "    print(f\"질의: {query}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 시각화를 위해 샘플 청크 가져오기 (첫 번째 청크 사용)\n",
    "    original_chunk = standard_chunks[0]\n",
    "    \n",
    "    # 각 압축 유형을 반복하며 비교 표시\n",
    "    for comp_type in evaluation_results[\"compression_results\"].keys():\n",
    "        compressed_chunks_list = evaluation_results[\"compression_results\"][comp_type][\"compressed_chunks\"] # 변수명 변경\n",
    "        compression_ratios_list = evaluation_results[\"compression_results\"][comp_type][\"compression_ratios\"] # 변수명 변경\n",
    "        \n",
    "        # 해당 압축된 청크와 압축률 가져오기 (리스트가 비어있지 않은 경우에만)\n",
    "        if compressed_chunks_list:\n",
    "            compressed_chunk_content = compressed_chunks_list[0] # 변수명 변경\n",
    "            compression_ratio_value = compression_ratios_list[0] # 변수명 변경\n",
    "        else:\n",
    "            compressed_chunk_content = \"\" # 빈 문자열로 초기화\n",
    "            compression_ratio_value = 0.0 # 0으로 초기화\n",
    "\n",
    "        print(f\"\\n=== {comp_type.upper()} 압축 예시 ===\\n\")\n",
    "        \n",
    "        # 원본 청크 표시 (너무 길면 잘림)\n",
    "        print(\"원본 청크:\")\n",
    "        print(\"-\" * 40)\n",
    "        if len(original_chunk) > 800:\n",
    "            print(original_chunk[:800] + \"... [잘림]\")\n",
    "        else:\n",
    "            print(original_chunk)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"길이: {len(original_chunk)} 문자\\n\")\n",
    "        \n",
    "        # 압축된 청크 표시\n",
    "        print(\"압축된 청크:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(compressed_chunk_content)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"길이: {len(compressed_chunk_content)} 문자\")\n",
    "        print(f\"압축률: {compression_ratio_value:.2f}%\\n\")\n",
    "        \n",
    "        # 이 압축 유형에 대한 전체 통계 표시\n",
    "        avg_ratio = sum(compression_ratios_list) / len(compression_ratios_list) if compression_ratios_list else 0\n",
    "        print(f\"모든 청크에 대한 평균 압축률: {avg_ratio:.2f}%\")\n",
    "        print(f\"총 컨텍스트 길이 감소: {evaluation_results['metrics'][comp_type]['avg_compression_ratio']}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # 압축 기술 요약 테이블 표시\n",
    "    print(\"\\n=== 압축 요약 ===\\n\")\n",
    "    print(f\"{'기술':<15} {'평균 압축률':<15} {'압축 후 길이':<15} {'원본 길이':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 각 압축 유형에 대한 메트릭 출력\n",
    "    for comp_type, metrics_data in evaluation_results[\"metrics\"].items(): # metrics를 metrics_data로 변경\n",
    "        print(f\"{comp_type:<15} {metrics_data['avg_compression_ratio']:<15} {metrics_data['total_context_length']:<15} {metrics_data['original_context_length']:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== SELECTIVE COMPRESSION EXAMPLE ===\n",
      "\n",
      "ORIGINAL CHUNK:\n",
      "----------------------------------------\n",
      "inability \n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      " \n",
      " \n",
      "Privacy and Security \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
      "Protecting sensitive information and ensuring responsible data handling are essential. \n",
      "Job Displacement \n",
      "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
      "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
      "of AI-driven automation is a key challenge. \n",
      "Autonomy and Control \n",
      "As AI systems become more autonomous, questions arise about ... [truncated]\n",
      "----------------------------------------\n",
      "Length: 1000 characters\n",
      "\n",
      "COMPRESSED CHUNK:\n",
      "----------------------------------------\n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability.\n",
      "\n",
      "Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial.\n",
      "\n",
      "Protecting sensitive information and ensuring responsible data handling are essential.\n",
      "\n",
      "Addressing the potential economic and social impacts of AI-driven automation is a key challenge.\n",
      "\n",
      "As AI systems become more autonomous, questions arise about control, accountability, and the \n",
      "potential for unintended consequences.\n",
      "----------------------------------------\n",
      "Length: 654 characters\n",
      "Compression ratio: 34.60%\n",
      "\n",
      "Average compression across all chunks: 39.93%\n",
      "Total context length reduction: 39.93%\n",
      "================================================================================\n",
      "\n",
      "=== SUMMARY COMPRESSION EXAMPLE ===\n",
      "\n",
      "ORIGINAL CHUNK:\n",
      "----------------------------------------\n",
      "inability \n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      " \n",
      " \n",
      "Privacy and Security \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
      "Protecting sensitive information and ensuring responsible data handling are essential. \n",
      "Job Displacement \n",
      "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
      "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
      "of AI-driven automation is a key challenge. \n",
      "Autonomy and Control \n",
      "As AI systems become more autonomous, questions arise about ... [truncated]\n",
      "----------------------------------------\n",
      "Length: 1000 characters\n",
      "\n",
      "COMPRESSED CHUNK:\n",
      "----------------------------------------\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "- Lack of transparency and explainability in AI decision-making processes\n",
      "- Privacy and data security concerns due to reliance on large amounts of data\n",
      "- Potential for job displacement, particularly in industries with repetitive or routine tasks\n",
      "- Questions about control, accountability, and unintended consequences as AI systems become more autonomous\n",
      "- Need for clear guidelines and ethical frameworks for AI development and deployment\n",
      "----------------------------------------\n",
      "Length: 514 characters\n",
      "Compression ratio: 48.60%\n",
      "\n",
      "Average compression across all chunks: 63.87%\n",
      "Total context length reduction: 63.87%\n",
      "================================================================================\n",
      "\n",
      "=== EXTRACTION COMPRESSION EXAMPLE ===\n",
      "\n",
      "ORIGINAL CHUNK:\n",
      "----------------------------------------\n",
      "inability \n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      " \n",
      " \n",
      "Privacy and Security \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
      "Protecting sensitive information and ensuring responsible data handling are essential. \n",
      "Job Displacement \n",
      "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
      "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
      "of AI-driven automation is a key challenge. \n",
      "Autonomy and Control \n",
      "As AI systems become more autonomous, questions arise about ... [truncated]\n",
      "----------------------------------------\n",
      "Length: 1000 characters\n",
      "\n",
      "COMPRESSED CHUNK:\n",
      "----------------------------------------\n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      "\n",
      "Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial.\n",
      "----------------------------------------\n",
      "Length: 335 characters\n",
      "Compression ratio: 66.50%\n",
      "\n",
      "Average compression across all chunks: 54.41%\n",
      "Total context length reduction: 54.41%\n",
      "================================================================================\n",
      "\n",
      "=== COMPRESSION SUMMARY ===\n",
      "\n",
      "기술              평균 압축률         압축 후 길이        원본 길이       \n",
      "------------------------------------------------------------\n",
      "selective       39.93%          6025            10018          \n",
      "summary         63.87%          3631            10018          \n",
      "extraction      54.41%          4577            10018          \n"
     ]
    }
   ],
   "source": [
    "# 압축 결과 시각화\n",
    "visualize_compression_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 10_contextual_compression.ipynb]
