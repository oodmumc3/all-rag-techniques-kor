{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG 시스템을 위한 적응형 검색\n",
    "\n",
    "이 노트북에서는 질의 유형에 따라 가장 적절한 검색 전략을 동적으로 선택하는 적응형 검색(Adaptive Retrieval) 시스템을 구현합니다. 이 접근 방식은 다양한 질문 범위에 걸쳐 정확하고 관련성 높은 응답을 제공하는 RAG 시스템의 능력을 크게 향상시킵니다.\n",
    "모든 질문에 동일한 검색 방식을 적용하는 대신, 질문의 특성을 파악하여 그에 맞는 최적의 검색 전략을 사용하는 것이 이 기법의 핵심입니다. 예를 들어, 단순 사실을 묻는 질문과 깊이 있는 분석을 요구하는 질문은 서로 다른 방식으로 정보를 찾아야 더 좋은 결과를 얻을 수 있습니다.\n",
    "\n",
    "다양한 질문에는 다양한 검색 전략이 필요합니다. 우리 시스템은 다음을 수행합니다:\n",
    "\n",
    "1. 질의 유형 분류 (사실적, 분석적, 의견, 문맥적)\n",
    "2. 적절한 검색 전략 선택\n",
    "3. 특화된 검색 기술 실행\n",
    "4. 맞춤형 응답 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import re # 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현\n",
    "문서 청크와 해당 임베딩을 관리하기 위한 기본적인 벡터 저장소를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []  # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 변환하여 벡터 리스트에 추가\n",
    "        self.texts.append(text)  # 원본 텍스트를 텍스트 리스트에 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터를 메타데이터 리스트에 추가 (None이면 빈 딕셔너리 사용)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 질의 임베딩 벡터.\n",
    "        k (int): 반환할 결과 수.\n",
    "        filter_func (callable, optional): 결과를 필터링하는 함수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트 및 메타데이터와 함께 상위 k개의 가장 유사한 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 제공된 경우 필터 적용\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 결과 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    # 문자열 및 리스트 입력을 모두 처리하기 위해 input_text가 항상 리스트가 되도록 보장\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 단일 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 그렇지 않으면 텍스트 목록에 대한 모든 임베딩 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    적응형 검색과 함께 사용할 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "    chunk_size (int): 각 청크의 문자 크기.\n",
    "    chunk_overlap (int): 청크 간 문자 중첩.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: 문서 청크 및 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 추출된 텍스트 청킹\n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    # 텍스트 청크에 대한 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 해당 임베딩을 메타데이터와 함께 벡터 저장소에 추가\n",
    "    for i, (chunk_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    \n",
    "    # 청크와 벡터 저장소 반환\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 분류\n",
    "LLM을 사용하여 질의를 '사실적(Factual)', '분석적(Analytical)', '의견(Opinion)', '문맥적(Contextual)' 네 가지 유형 중 하나로 분류합니다. 이 분류 결과에 따라 다른 검색 전략을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의를 사실적, 분석적, 의견 또는 문맥적의 네 가지 범주 중 하나로 분류합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 질의 범주\n",
    "    \"\"\"\n",
    "    # AI의 분류를 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 질문 분류 전문가입니다. \n",
    "        주어진 질의를 다음 범주 중 정확히 하나로 분류하십시오:\n",
    "        - 사실적: 구체적이고 검증 가능한 정보를 찾는 질의.\n",
    "        - 분석적: 포괄적인 분석이나 설명을 요구하는 질의.\n",
    "        - 의견: 주관적인 문제에 대한 질의 또는 다양한 관점을 찾는 질의.\n",
    "        - 문맥적: 사용자별 컨텍스트에 의존하는 질의.\n",
    "\n",
    "        설명이나 추가 텍스트 없이 범주 이름만 반환하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 분류할 질의를 포함하는 사용자 프롬프트 생성\n",
    "    user_prompt = f\"이 질의를 분류하십시오: {query}\"\n",
    "    \n",
    "    # AI 모델로부터 분류 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 일관된 분류를 위해 온도를 0으로 설정\n",
    "    )\n",
    "    \n",
    "    # 응답에서 범주 추출 및 공백 제거\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 유효한 범주 목록 정의\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "    # 반환된 범주가 유효한지 확인\n",
    "    for valid_cat in valid_categories: # valid를 valid_cat으로 변경\n",
    "        if valid_cat in category:\n",
    "            return valid_cat\n",
    "    \n",
    "    # 분류 실패 시 기본값으로 \"Factual\" 반환\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특화된 검색 전략 구현\n",
    "### 1. 사실적 전략 - 정밀도 집중\n",
    "사실적 질문의 경우, LLM을 사용하여 질문을 더 명확하고 구체적으로 재작성한 후 검색하여 정확도를 높입니다. 검색된 결과에 대해서도 LLM으로 다시 한번 관련성을 평가하여 순위를 매깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    정밀도에 초점을 맞춘 사실적 질의에 대한 검색 전략입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"'{query}'에 대한 사실적 검색 전략 실행 중...\")\n",
    "    \n",
    "    # 더 나은 정밀도를 위해 LLM을 사용하여 질의 향상\n",
    "    system_prompt = \"\"\"당신은 검색 질의 향상 전문가입니다.\n",
    "        당신의 임무는 정보 검색을 위해 주어진 사실적 질의를 더 정확하고 구체적으로 재구성하는 것입니다. 주요 엔티티와 그 관계에 초점을 맞추십시오.\n",
    "\n",
    "        설명 없이 향상된 질의만 제공하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"이 사실적 질의를 향상시키십시오: {query}\"\n",
    "    \n",
    "    # LLM을 사용하여 향상된 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 향상된 질의 추출 및 출력\n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"향상된 질의: {enhanced_query}\")\n",
    "    \n",
    "    # 향상된 질의에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # 초기 유사도 검색을 수행하여 문서 검색 (재정렬을 위해 k*2개 검색)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # 순위 매겨진 결과를 저장할 리스트 초기화\n",
    "    ranked_results = []\n",
    "    \n",
    "    # LLM을 사용하여 관련성을 기준으로 문서 점수 매기기 및 순위 지정\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # 관련성 점수를 기준으로 결과 내림차순 정렬\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 k개 결과 반환\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 분석적 전략 - 포괄적 범위\n",
    "분석적 질문의 경우, 원래 질문을 여러 하위 질문으로 분해합니다. 각 하위 질문에 대한 검색 결과를 종합하여 다양한 측면을 다루는 포괄적인 정보를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    포괄적인 범위에 초점을 맞춘 분석적 질의에 대한 검색 전략입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"'{query}'에 대한 분석적 검색 전략 실행 중...\")\n",
    "    \n",
    "    # 하위 질문 생성을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 복잡한 질문 분해 전문가입니다.\n",
    "    주요 분석적 질의의 다양한 측면을 탐구하는 하위 질문을 생성하십시오.\n",
    "    이러한 하위 질문은 주제의 폭을 다루고 포괄적인 정보를 검색하는 데 도움이 되어야 합니다.\n",
    "\n",
    "    한 줄에 하나씩 정확히 3개의 하위 질문 목록을 반환하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 주요 질의를 포함하는 사용자 프롬프트 생성\n",
    "    user_prompt = f\"이 분석적 질의에 대한 하위 질문을 생성하십시오: {query}\"\n",
    "    \n",
    "    # LLM을 사용하여 하위 질문 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3 # 약간의 다양성을 위해 온도를 0.3으로 설정\n",
    "    )\n",
    "    \n",
    "    # 하위 질문 추출 및 정리\n",
    "    sub_queries_text = response.choices[0].message.content.strip().split('\\n') # 변수명 변경\n",
    "    sub_queries = [q.strip() for q in sub_queries_text if q.strip()]\n",
    "    print(f\"생성된 하위 질의: {sub_queries}\")\n",
    "    \n",
    "    # 각 하위 질의에 대한 문서 검색\n",
    "    all_results = []\n",
    "    for sub_query_text in sub_queries: # sub_query를 sub_query_text로 변경\n",
    "        # 하위 질의에 대한 임베딩 생성\n",
    "        sub_query_embedding = create_embeddings(sub_query_text)\n",
    "        # 하위 질의에 대한 유사도 검색 수행\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 다양한 하위 질의 결과에서 선택하여 다양성 보장\n",
    "    # 중복 제거 (동일한 텍스트 내용)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result_item in all_results: # result를 result_item으로 변경\n",
    "        if result_item[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result_item[\"text\"])\n",
    "            diverse_results.append(result_item)\n",
    "    \n",
    "    # k개에 도달하기 위해 더 많은 결과가 필요하면 초기 결과에서 추가\n",
    "    if len(diverse_results) < k:\n",
    "        # 주요 질의에 대한 직접 검색\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result_item in main_results: # result를 result_item으로 변경\n",
    "            if result_item[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result_item[\"text\"])\n",
    "                diverse_results.append(result_item)\n",
    "    \n",
    "    # 상위 k개의 다양한 결과 반환\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 의견 전략 - 다양한 관점\n",
    "의견을 묻는 질문의 경우, LLM을 사용하여 해당 주제에 대한 여러 가능한 관점을 생성합니다. 그런 다음 각 관점과 원래 질문을 조합하여 검색함으로써 다양한 의견을 포함하는 정보를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    다양한 관점에 초점을 맞춘 의견 질의에 대한 검색 전략입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"'{query}'에 대한 의견 검색 전략 실행 중...\")\n",
    "    \n",
    "    # 다양한 관점을 식별하도록 AI를 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 주제에 대한 다양한 관점을 식별하는 전문가입니다.\n",
    "        의견이나 관점에 대한 주어진 질의에 대해 사람들이 이 주제에 대해 가질 수 있는 다양한 관점을 식별하십시오.\n",
    "\n",
    "        한 줄에 하나씩 정확히 3개의 서로 다른 관점 각도 목록을 반환하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 주요 질의를 포함하는 사용자 프롬프트 생성\n",
    "    user_prompt = f\"다음에 대한 다양한 관점을 식별하십시오: {query}\"\n",
    "    \n",
    "    # LLM을 사용하여 다양한 관점 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 관점 추출 및 정리\n",
    "    viewpoints_text = response.choices[0].message.content.strip().split('\\n') # viewpoints를 viewpoints_text로 변경\n",
    "    viewpoints = [v.strip() for v in viewpoints_text if v.strip()]\n",
    "    print(f\"식별된 관점: {viewpoints}\")\n",
    "    \n",
    "    # 각 관점을 나타내는 문서 검색\n",
    "    all_results = []\n",
    "    for viewpoint_text in viewpoints: # viewpoint를 viewpoint_text로 변경\n",
    "        # 주요 질의와 관점 결합\n",
    "        combined_query = f\"{query} {viewpoint_text}\"\n",
    "        # 결합된 질의에 대한 임베딩 생성\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        # 결합된 질의에 대한 유사도 검색 수행\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # 결과를 나타내는 관점으로 결과 표시\n",
    "        for result_item in results: # result를 result_item으로 변경\n",
    "            result_item[\"viewpoint\"] = viewpoint_text\n",
    "        \n",
    "        # 모든 결과 리스트에 결과 추가\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 다양한 의견 범위 선택\n",
    "    # 가능하면 각 관점에서 하나 이상의 문서 가져오기\n",
    "    selected_results = []\n",
    "    for viewpoint_text in viewpoints: # viewpoint를 viewpoint_text로 변경\n",
    "        # 관점별로 문서 필터링\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint_text]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # 나머지 슬롯을 가장 높은 유사도 문서로 채우기\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # 나머지 문서를 유사도 기준으로 정렬\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # 상위 k개 결과 반환\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 문맥적 전략 - 사용자 문맥 통합\n",
    "문맥적 질문의 경우, 제공된 사용자 문맥(또는 LLM이 추론한 문맥)을 원래 질문에 통합하여 더 개인화되고 관련성 높은 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    사용자 컨텍스트를 통합하는 문맥적 질의에 대한 검색 전략입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "        user_context (str): 추가 사용자 컨텍스트\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"'{query}'에 대한 문맥적 검색 전략 실행 중...\")\n",
    "    \n",
    "    # 사용자 컨텍스트가 제공되지 않으면 질의에서 추론 시도\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"당신은 질문에서 암시된 컨텍스트를 이해하는 전문가입니다.\n",
    "주어진 질의에 대해 관련성이 있거나 명시적으로 언급되지는 않았지만 암시된 문맥 정보를 추론하십시오. 이 질의에 답변하는 데 어떤 배경이 도움이 될지에 초점을 맞추십시오.\n",
    "\n",
    "암시된 컨텍스트에 대한 간략한 설명을 반환하십시오.\"\"\"\n",
    "\n",
    "        user_prompt = f\"이 질의에서 암시된 컨텍스트를 추론하십시오: {query}\"\n",
    "        \n",
    "        # LLM을 사용하여 추론된 컨텍스트 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # 추론된 컨텍스트 추출 및 출력\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"추론된 컨텍스트: {user_context}\")\n",
    "    \n",
    "    # 컨텍스트를 통합하도록 질의 재구성\n",
    "    system_prompt = \"\"\"당신은 컨텍스트를 사용하여 질문을 재구성하는 전문가입니다.\n",
    "    질의와 일부 문맥 정보가 주어지면 더 관련성 높은 정보를 얻기 위해 컨텍스트를 통합하는 더 구체적인 질의를 만드십시오.\n",
    "\n",
    "    설명 없이 재구성된 질의만 반환하십시오.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    질의: {query}\n",
    "    컨텍스트: {user_context}\n",
    "\n",
    "    이 컨텍스트를 통합하도록 질의를 재구성하십시오:\"\"\"\n",
    "    \n",
    "    # LLM을 사용하여 문맥화된 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 문맥화된 질의 추출 및 출력\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"문맥화된 질의: {contextualized_query}\")\n",
    "    \n",
    "    # 문맥화된 질의를 기반으로 문서 검색\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # 관련성과 사용자 컨텍스트를 모두 고려하여 문서 순위 매기기\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # 컨텍스트를 고려한 문서 관련성 점수 매기기\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # 컨텍스트 관련성을 기준으로 정렬하고 상위 k개 결과 반환\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 점수 매기기를 위한 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_relevance(query, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 질의에 대한 문서 관련성을 점수 매깁니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        document (str): 문서 텍스트\n",
    "        model (str): LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        float: 0-10점 척도의 관련성 점수\n",
    "    \"\"\"\n",
    "    # 모델에 관련성 평가 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 문서 관련성 평가 전문가입니다.\n",
    "        질의에 대한 문서의 관련성을 0에서 10까지의 척도로 평가하십시오. 여기서:\n",
    "        0 = 완전히 관련 없음\n",
    "        10 = 질의를 완벽하게 다룸\n",
    "\n",
    "        다른 내용 없이 0에서 10 사이의 숫자 점수만 반환하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 문서가 너무 길면 자르기\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 질의와 문서 미리보기를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "        질의: {query}\n",
    "\n",
    "        문서: {doc_preview}\n",
    "\n",
    "        관련성 점수 (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 점수 추출\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 정규 표현식을 사용하여 숫자 점수 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 점수가 0-10 범위 내에 있도록 보장\n",
    "    else:\n",
    "        # 추출 실패 시 기본 점수\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_context_relevance(query, context, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 컨텍스트를 모두 고려하여 문서 관련성을 점수 매깁니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 사용자 컨텍스트\n",
    "        document (str): 문서 텍스트\n",
    "        model (str): LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        float: 0-10점 척도의 관련성 점수\n",
    "    \"\"\"\n",
    "    # 모델에 컨텍스트를 고려한 관련성 평가 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 컨텍스트를 고려한 문서 관련성 평가 전문가입니다.\n",
    "        제공된 컨텍스트를 고려할 때 질의를 얼마나 잘 다루는지에 따라 0에서 10까지의 척도로 문서를 평가하십시오. 여기서:\n",
    "        0 = 완전히 관련 없음\n",
    "        10 = 주어진 컨텍스트에서 질의를 완벽하게 다룸\n",
    "\n",
    "        다른 내용 없이 0에서 10 사이의 숫자 점수만 반환하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 문서가 너무 길면 자르기\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 질의, 컨텍스트 및 문서 미리보기를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    질의: {query}\n",
    "    컨텍스트: {context}\n",
    "\n",
    "    문서: {doc_preview}\n",
    "\n",
    "    컨텍스트를 고려한 관련성 점수 (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 점수 추출\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 정규 표현식을 사용하여 숫자 점수 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 점수가 0-10 범위 내에 있도록 보장\n",
    "    else:\n",
    "        # 추출 실패 시 기본 점수\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 적응형 검색기\n",
    "질의를 분류하고 적절한 검색 전략을 선택하여 실행하는 메인 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    적절한 전략을 선택하고 실행하여 적응형 검색을 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 검색할 문서 수\n",
    "        user_context (str): 문맥적 질의를 위한 선택적 사용자 컨텍스트\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    # 질의 유형을 결정하기 위해 질의 분류\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"질의 분류: {query_type}\")\n",
    "    \n",
    "    # 질의 유형에 따라 적절한 검색 전략 선택 및 실행\n",
    "    if query_type == \"Factual\":\n",
    "        # 정확한 정보를 위한 사실적 검색 전략 사용\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # 포괄적인 범위를 위한 분석적 검색 전략 사용\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # 다양한 관점을 위한 의견 검색 전략 사용\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # 사용자 컨텍스트를 통합하는 문맥적 검색 전략 사용\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # 분류 실패 시 기본적으로 사실적 검색 전략 사용\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    \n",
    "    return results  # 검색된 문서 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 응답 생성\n",
    "검색된 문서와 질의 유형에 따라 맞춤형 프롬프트를 사용하여 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, query_type, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의, 검색된 문서 및 질의 유형을 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 검색된 문서\n",
    "        query_type (str): 질의 유형\n",
    "        model (str): LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # 검색된 문서의 텍스트를 구분 기호로 결합하여 컨텍스트 준비\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "    \n",
    "    # 질의 유형에 따라 사용자 정의 시스템 프롬프트 생성\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"당신은 사실적 정보를 제공하는 도움이 되는 어시스턴트입니다.\n",
    "    제공된 컨텍스트를 기반으로 질문에 답변하십시오. 정확성과 정밀도에 중점을 두십시오.\n",
    "    컨텍스트에 필요한 정보가 포함되어 있지 않으면 한계를 인정하십시오.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"당신은 분석적 통찰력을 제공하는 도움이 되는 어시스턴트입니다.\n",
    "    제공된 컨텍스트를 기반으로 주제에 대한 포괄적인 분석을 제공하십시오.\n",
    "    설명에서 다양한 측면과 관점을 다루십시오.\n",
    "    컨텍스트에 공백이 있으면 가능한 최상의 분석을 제공하면서 이를 인정하십시오.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"당신은 여러 관점을 가진 주제를 논의하는 도움이 되는 어시스턴트입니다.\n",
    "    제공된 컨텍스트를 기반으로 주제에 대한 다양한 관점을 제시하십시오.\n",
    "    편견을 보이지 않고 다양한 의견을 공정하게 표현하도록 하십시오.\n",
    "    컨텍스트가 제한된 관점을 제시하는 경우 이를 인정하십시오.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"당신은 문맥적으로 관련된 정보를 제공하는 도움이 되는 어시스턴트입니다.\n",
    "    질의와 해당 컨텍스트를 모두 고려하여 질문에 답변하십시오.\n",
    "    질의 컨텍스트와 제공된 문서의 정보 간의 연결을 만드십시오.\n",
    "    컨텍스트가 특정 상황을 완전히 다루지 못하면 한계를 인정하십시오.\"\"\"\n",
    "        \n",
    "    else:\n",
    "        system_prompt = \"\"\"당신은 도움이 되는 어시스턴트입니다. 제공된 컨텍스트를 기반으로 질문에 답변하십시오. 컨텍스트에서 답변할 수 없으면 한계를 인정하십시오.\"\"\"\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "    컨텍스트:\n",
    "    {context}\n",
    "\n",
    "    질문: {query}\n",
    "\n",
    "    컨텍스트를 기반으로 도움이 되는 응답을 제공하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenAI 클라이언트를 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2 # 약간의 창의성을 허용하면서 일관성 유지\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적응형 검색을 사용한 전체 RAG 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    적응형 검색을 사용한 전체 RAG 파이프라인입니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        k (int): 검색할 문서 수\n",
    "        user_context (str): 선택적 사용자 컨텍스트\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 검색된 문서, 질의 유형 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 적응형 검색을 사용한 RAG ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청킹 및 임베딩 생성\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 질의 유형을 결정하기 위해 질의 분류\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"질의 분류: {query_type}\")\n",
    "    \n",
    "    # 질의 유형에 따라 적응형 검색 전략을 사용하여 문서 검색\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # 질의, 검색된 문서 및 질의 유형을 기반으로 응답 생성\n",
    "    response_text = generate_response(query, retrieved_docs, query_type) # response를 response_text로 변경\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 프레임워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    테스트 질의 세트에 대해 적응형 검색과 표준 검색을 비교합니다.\n",
    "    \n",
    "    이 함수는 문서를 처리하고 각 테스트 질의에 대해 표준 및 적응형 검색 방법을 모두 실행하며\n",
    "    성능을 비교합니다. 참조 답변이 제공되면 이러한 참조에 대해 응답 품질도 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 지식 소스로 처리할 PDF 문서 경로\n",
    "        test_queries (List[str]): 두 검색 방법을 모두 평가할 테스트 질의 목록\n",
    "        reference_answers (List[str], optional): 평가 메트릭을 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 개별 질의 결과와 전체 비교를 포함하는 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"=== 적응형 대 표준 검색 평가 ===\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크 생성 및 벡터 저장소 구축\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 비교 결과를 저장하기 위한 컬렉션 초기화\n",
    "    results_comparison = [] # results를 results_comparison으로 변경\n",
    "    \n",
    "    # 각 테스트 질의를 두 검색 방법으로 모두 처리\n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n\\n질의 {i+1}: {query_text}\")\n",
    "        \n",
    "        # --- 표준 검색 접근 방식 ---\n",
    "        print(\"\\n--- 표준 검색 ---\")\n",
    "        # 질의에 대한 임베딩 생성\n",
    "        query_embedding = create_embeddings(query_text)\n",
    "        # 간단한 벡터 유사도를 사용하여 문서 검색\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        # 일반적인 접근 방식을 사용하여 응답 생성\n",
    "        standard_response_text = generate_response(query_text, standard_docs, \"General\") # standard_response를 standard_response_text로 변경\n",
    "        \n",
    "        # --- 적응형 검색 접근 방식 ---\n",
    "        print(\"\\n--- 적응형 검색 ---\")\n",
    "        # 질의 유형(사실적, 분석적, 의견, 문맥적)을 결정하기 위해 질의 분류\n",
    "        query_type = classify_query(query_text)\n",
    "        # 이 질의 유형에 적합한 전략을 사용하여 문서 검색\n",
    "        adaptive_docs = adaptive_retrieval(query_text, vector_store, k=4)\n",
    "        # 질의 유형에 맞게 조정된 응답 생성\n",
    "        adaptive_response_text = generate_response(query_text, adaptive_docs, query_type) # adaptive_response를 adaptive_response_text로 변경\n",
    "        \n",
    "        # 이 질의에 대한 전체 결과 저장\n",
    "        result_item = { # result를 result_item으로 변경\n",
    "            \"query\": query_text,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response_text\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response_text\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 이 질의에 사용할 수 있는 경우 참조 답변 추가\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result_item[\"reference_answer\"] = reference_answers[i]\n",
    "            \n",
    "        results_comparison.append(result_item)\n",
    "        \n",
    "        # 빠른 비교를 위해 두 응답의 미리보기 표시\n",
    "        print(\"\\n--- 응답 ---\")\n",
    "        print(f\"표준: {standard_response_text[:200]}...\")\n",
    "        print(f\"적응형: {adaptive_response_text[:200]}...\")\n",
    "    \n",
    "    # 참조 답변을 사용할 수 있는 경우 비교 메트릭 계산\n",
    "    comparison_text = \"평가를 위한 참조 답변이 제공되지 않았습니다.\" # comparison을 comparison_text로 변경\n",
    "    if reference_answers:\n",
    "        comparison_text = compare_responses(results_comparison)\n",
    "        print(\"\\n=== 평가 결과 ===\")\n",
    "        print(comparison_text)\n",
    "    \n",
    "    # 전체 평가 결과 반환\n",
    "    return {\n",
    "        \"results\": results_comparison,\n",
    "        \"comparison\": comparison_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    참조 답변과 표준 및 적응형 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): 두 가지 유형의 응답을 모두 포함하는 결과\n",
    "        \n",
    "    Returns:\n",
    "        str: 비교 분석\n",
    "    \"\"\"\n",
    "    # 응답 비교를 안내하는 AI에 대한 시스템 프롬프트 정의\n",
    "    comparison_prompt_template = \"\"\"당신은 정보 검색 시스템의 전문 평가자입니다.\n",
    "    각 질의에 대한 표준 검색 응답과 적응형 검색 응답을 비교하십시오.\n",
    "    정확성, 관련성, 포괄성 및 참조 답변과의 일치와 같은 요소를 고려하십시오.\n",
    "    각 접근 방식의 강점과 약점에 대한 자세한 분석을 제공하십시오.\"\"\" # 변수명 변경\n",
    "    \n",
    "    # 헤더로 비교 텍스트 초기화\n",
    "    full_comparison_text = \"# 표준 대 적응형 검색 평가\\n\\n\" # 변수명 변경\n",
    "    \n",
    "    # 각 결과를 반복하여 응답 비교\n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        # 질의에 대한 참조 답변이 없는 경우 건너뛰기\n",
    "        if \"reference_answer\" not in result_item:\n",
    "            continue\n",
    "            \n",
    "        # 비교 텍스트에 질의 세부 정보 추가\n",
    "        full_comparison_text += f\"## 질의 {i+1}: {result_item['query']}\\n\"\n",
    "        full_comparison_text += f\"*질의 유형: {result_item['query_type']}*\\n\\n\"\n",
    "        full_comparison_text += f\"**참조 답변:**\\n{result_item['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # 비교 텍스트에 표준 검색 응답 추가\n",
    "        full_comparison_text += f\"**표준 검색 응답:**\\n{result_item['standard_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # 비교 텍스트에 적응형 검색 응답 추가\n",
    "        full_comparison_text += f\"**적응형 검색 응답:**\\n{result_item['adaptive_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # AI가 응답을 비교하도록 하는 사용자 프롬프트 생성\n",
    "        user_prompt = f\"\"\"\n",
    "        참조 답변: {result_item['reference_answer']}\n",
    "        \n",
    "        표준 검색 응답: {result_item['standard_retrieval']['response']}\n",
    "        \n",
    "        적응형 검색 응답: {result_item['adaptive_retrieval']['response']}\n",
    "        \n",
    "        두 응답에 대한 자세한 비교를 제공하십시오.\n",
    "        \"\"\"\n",
    "        \n",
    "        # OpenAI 클라이언트를 사용하여 비교 분석 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt_template},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # 비교 텍스트에 AI의 비교 분석 추가\n",
    "        full_comparison_text += f\"**비교 분석:**\\n{response.choices[0].message.content}\\n\\n\"\n",
    "    \n",
    "    return full_comparison_text  # 전체 비교 분석 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적응형 검색 시스템 평가 (사용자 정의 질의)\n",
    "\n",
    "적응형 RAG 평가 시스템을 사용하는 마지막 단계는 PDF 문서와 테스트 질의로 `evaluate_adaptive_vs_standard()` 함수를 호출하는 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식 소스 문서 경로\n",
    "# 이 PDF 파일에는 RAG 시스템이 사용할 정보가 포함되어 있습니다.\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 다양한 질의 의도를 처리하는 적응형 검색 방법을 보여주기 위해 다양한 질의 유형을 포함하는 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"설명 가능한 AI(XAI)란 무엇인가?\",                                              # 사실적 질의 - 정의/특정 정보 검색\n",
    "    # \"AI 윤리 및 거버넌스 프레임워크는 잠재적인 사회적 영향을 어떻게 다루는가?\",  # 분석적 질의 - 포괄적인 분석 필요\n",
    "    # \"AI 개발이 적절한 규제를 하기에는 너무 빠르게 진행되고 있는가?\",                   # 의견 질의 - 다양한 관점 검색\n",
    "    # \"설명 가능한 AI가 의료 결정에 어떻게 도움이 될 수 있는가?\",                     # 문맥적 질의 - 컨텍스트 인식의 이점\n",
    "]\n",
    "\n",
    "# 보다 철저한 평가를 위한 참조 답변\n",
    "# 알려진 표준에 대해 응답 품질을 객관적으로 평가하는 데 사용할 수 있습니다.\n",
    "reference_answers = [\n",
    "    \"설명 가능한 AI(XAI)는 의사 결정 방식을 명확하게 설명하여 AI 시스템을 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. 이를 통해 사용자는 AI 기술을 신뢰하고 효과적으로 관리할 수 있습니다.\",\n",
    "    # \"AI 윤리 및 거버넌스 프레임워크는 AI 시스템이 책임감 있게 개발되고 사용되도록 지침과 원칙을 수립하여 잠재적인 사회적 영향을 다룹니다. 이러한 프레임워크는 위험을 완화하고 유익한 결과를 증진하기 위해 공정성, 책임성, 투명성 및 인권 보호에 중점을 둡니다.5.\",\n",
    "    # \"AI 개발이 적절한 규제를 하기에는 너무 빠르게 진행되고 있다는 의견은 다양합니다. 일부에서는 급격한 발전이 규제 노력을 앞질러 잠재적인 위험과 윤리적 우려를 야기한다고 주장합니다. 다른 사람들은 혁신이 현재 속도로 계속되어야 하며, 새로운 과제를 해결하기 위해 규제가 함께 발전해야 한다고 생각합니다.\",\n",
    "    # \"설명 가능한 AI는 AI 기반 권장 사항에 대한 투명하고 이해하기 쉬운 통찰력을 제공함으로써 의료 결정을 크게 지원할 수 있습니다. 이러한 투명성은 의료 전문가가 AI 시스템을 신뢰하고, 정보에 입각한 결정을 내리며, AI 제안의 근거를 이해함으로써 환자 결과를 개선하는 데 도움이 됩니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 적응형 대 표준 검색 평가 ===\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청킹 중...\n",
      "생성된 텍스트 청크 수: 42\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크 추가됨\n",
      "\n",
      "\n",
      "질의 1: 설명 가능한 AI(XAI)란 무엇인가?\n",
      "\n",
      "--- 표준 검색 ---\n",
      "\n",
      "--- 적응형 검색 ---\n",
      "질의 분류: Factual\n",
      "질의 분류: Factual\n",
      "'설명 가능한 AI(XAI)란 무엇인가?'에 대한 사실적 검색 전략 실행 중...\n",
      "향상된 질의: 설명 가능한 인공 지능(XAI)의 정의와 핵심 원칙은 무엇이며, 기계 학습 모델의 투명성과 해석 가능성을 어떻게 향상시키는가?\n",
      "\n",
      "--- 응답 ---\n",
      "표준: 제공된 컨텍스트를 기반으로 설명 가능한 AI(XAI)는 AI 결정을 더 이해하기 쉽게 만들어 사용자가 공정성과 정확성을 평가할 수 있도록 하는 기술 집합입니다. XAI의 목표는 AI 모델이 어떻게 결정을 내리는지에 대한 통찰력을 제공하여 AI 시스템에 대한 신뢰와 책임성을 향상시키는 것입니다. 여기에는 사용자가 AI 기반 결과의 신뢰성과 공정성을 이해하는 데 도움이 되는 AI 결정 설명 방법을 개발하는 것이 포함됩니다....\n",
      "적응형: 설명 가능한 AI(XAI)는 AI 시스템을 더 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 인공 지능(AI)의 하위 분야입니다. XAI의 주요 목표는 AI 모델이 어떻게 결정을 내리는지에 대한 통찰력을 제공하여 AI 시스템에 대한 신뢰, 책임성 및 공정성을 향상시키는 것입니다.\n",
      "\n",
      "XAI 기술은 AI 결정의 근거를 설명하기 위해 개발되고 있으며, 사용자가 AI 결과의 신뢰성과 공정성을 평가할 수 있도록 합니다. 이는 의료 진단, 금융, 운송 및 제조와 같이 AI 오류의 결과가 심각할 수 있는 고위험 응용 분야에서 특히 중요합니다.\n",
      "\n",
      "AI 시스템을 더 설명 가능하게 만듦으로써 XAI 기술은 AI 개발 및 배포에서 의도하지 않은 결과, 책임성 및 책임에 대한 우려를 해결하는 데 도움이 될 수 있습니다....\n",
      "\n",
      "=== 평가 결과 ===\n",
      "# 표준 대 적응형 검색 평가\n",
      "\n",
      "## 질의 1: 설명 가능한 AI(XAI)란 무엇인가?\n",
      "*질의 유형: Factual*\n",
      "\n",
      "**참조 답변:**\n",
      "설명 가능한 AI(XAI)는 의사 결정 방식을 명확하게 설명하여 AI 시스템을 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. 이를 통해 사용자는 AI 기술을 신뢰하고 효과적으로 관리할 수 있습니다.\n",
      "\n",
      "**표준 검색 응답:**\n",
      "제공된 컨텍스트를 기반으로 설명 가능한 AI(XAI)는 AI 결정을 더 이해하기 쉽게 만들어 사용자가 공정성과 정확성을 평가할 수 있도록 하는 기술 집합입니다. XAI의 목표는 AI 모델이 어떻게 결정을 내리는지에 대한 통찰력을 제공하여 AI 시스템에 대한 신뢰와 책임성을 향상시키는 것입니다. 여기에는 사용자가 AI 기반 결과의 신뢰성과 공정성을 이해하는 데 도움이 되는 AI 결정 설명 방법을 개발하는 것이 포함됩니다.\n",
      "\n",
      "**적응형 검색 응답:**\n",
      "설명 가능한 AI(XAI)는 AI 시스템을 더 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 인공 지능(AI)의 하위 분야입니다. XAI의 주요 목표는 AI 모델이 어떻게 결정을 내리는지에 대한 통찰력을 제공하여 AI 시스템에 대한 신뢰, 책임성 및 공정성을 향상시키는 것입니다.\n",
      "\n",
      "XAI 기술은 AI 결정의 근거를 설명하기 위해 개발되고 있으며, 사용자가 AI 결과의 신뢰성과 공정성을 평가할 수 있도록 합니다. 이는 의료 진단, 금융, 운송 및 제조와 같이 AI 오류의 결과가 심각할 수 있는 고위험 응용 분야에서 특히 중요합니다.\n",
      "\n",
      "AI 시스템을 더 설명 가능하게 만듦으로써 XAI 기술은 AI 개발 및 배포에서 의도하지 않은 결과, 책임성 및 책임에 대한 우려를 해결하는 데 도움이 될 수 있습니다.\n",
      "\n",
      "**비교 분석:**\n",
      "**표준 검색 응답과 적응형 검색 응답 비교**\n",
      "\n",
      "두 응답은 설명 가능한 AI(XAI)에 대해 유사한 정보를 제공하지만 어조, 구조 및 세부 수준이 다릅니다. 두 응답에 대한 자세한 비교는 다음과 같습니다.\n",
      "\n",
      "**정확성 및 관련성**\n",
      "\n",
      "* 두 응답 모두 AI 시스템을 더 투명하고 이해하기 쉽게 만드는 XAI의 주요 아이디어를 정확하게 전달합니다.\n",
      "* 그러나 표준 검색 응답은 목표 및 응용 프로그램을 포함하여 XAI에 대한 더 많은 컨텍스트와 배경 정보를 제공합니다.\n",
      "* 적응형 검색 응답은 더 간결하고 요점만 전달하지만 표준 검색 응답의 깊이와 세부 정보가 부족합니다.\n",
      "\n",
      "**포괄성**\n",
      "\n",
      "* 표준 검색 응답은 기술, 목표 및 응용 프로그램을 포함하여 XAI에 대한 보다 포괄적인 개요를 제공합니다.\n",
      "* 적응형 검색 응답은 XAI의 정의와 목적에 주로 초점을 맞추고 XAI에 대한 추가 컨텍스트나 정보를 많이 제공하지 않습니다.\n",
      "* 표준 검색 응답은 또한 의료 진단, 금융, 운송 및 제조와 같은 고위험 응용 분야에서 XAI의 중요성을 강조합니다.\n",
      "\n",
      "**참조 답변과의 일치**\n",
      "\n",
      "* 두 응답 모두 참조 답변과 일치하지만 표준 검색 응답은 XAI에 대한 보다 상세하고 포괄적인 설명으로 인해 더 밀접하게 일치합니다.\n",
      "* 적응형 검색 응답은 더 간결하고 요점만 전달하지만 XAI의 미묘함과 복잡성을 완전히 포착하지 못할 수 있습니다.\n",
      "\n",
      "**강점과 약점**\n",
      "\n",
      "**표준 검색 응답**\n",
      "\n",
      "강점:\n",
      "\n",
      "* XAI에 대한 보다 포괄적인 개요 제공\n",
      "* 더 많은 컨텍스트와 배경 정보 제공\n",
      "* 참조 답변과 밀접하게 일치\n",
      "\n",
      "약점:\n",
      "\n",
      "* 일부 독자에게는 너무 길거나 장황할 수 있음\n",
      "* 적응형 검색 응답의 간결하고 요점만 전달하는 스타일 부족\n",
      "\n",
      "**적응형 검색 응답**\n",
      "\n",
      "강점:\n",
      "\n",
      "* 간결하고 요점만 전달\n",
      "* XAI에 대한 명확하고 직접적인 정의 제공\n",
      "* 간략한 개요를 선호하는 독자에게 더 적합할 수 있음\n",
      "\n",
      "약점:\n",
      "\n",
      "* 깊이와 세부 정보 부족\n",
      "* XAI에 대한 추가 컨텍스트나 정보를 많이 제공하지 못함\n",
      "* XAI의 미묘함과 복잡성을 완전히 포착하지 못할 수 있음\n",
      "\n",
      "**결론**\n",
      "\n",
      "표준 검색 응답은 XAI에 대한 보다 포괄적이고 상세한 개요를 제공하는 반면 적응형 검색 응답은 더 간결하고 요점만 전달합니다. 두 응답 모두 참조 답변과 일치하지만 표준 검색 응답은 XAI에 대한 보다 상세하고 포괄적인 설명으로 인해 더 밀접하게 일치합니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 적응형 대 표준 검색 비교 평가 실행\n",
    "# 이 함수는 두 가지 방법을 사용하여 각 질의를 처리하고 결과를 비교합니다.\n",
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_path=pdf_path,                  # 지식 추출을 위한 원본 문서\n",
    "    test_queries=test_queries,          # 평가할 테스트 질의 목록\n",
    "    reference_answers=reference_answers  # 비교를 위한 선택적 정답\n",
    ")\n",
    "\n",
    "# 결과에는 다양한 질의 유형에 걸쳐 표준 검색과 적응형 검색의 성능을 비교하는\n",
    "# 상세한 내용이 포함되며, 적응형 전략이 개선된 결과를 제공하는 부분을 강조합니다.\n",
    "print(evaluation_results[\"comparison\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 12_adaptive_rag.ipynb]
