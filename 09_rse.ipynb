{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG를 위한 관련 세그먼트 추출 (RSE)\n",
    "\n",
    "이 노트북에서는 RAG 시스템의 컨텍스트 품질을 향상시키기 위한 관련 세그먼트 추출(Relevant Segment Extraction, RSE) 기술을 구현합니다. 단순히 분리된 청크 모음을 검색하는 대신, 언어 모델에 더 나은 컨텍스트를 제공하는 연속적인 텍스트 세그먼트를 식별하고 재구성합니다.\n",
    "기존의 청크 단위 검색은 정보의 단편화를 야기할 수 있습니다. RSE는 관련성이 높은 청크들이 문서 내에서 서로 가까이 위치하는 경향이 있다는 점에 착안하여, 이러한 청크 클러스터를 찾아내고 연속성을 유지함으로써 LLM이 더 일관성 있고 풍부한 문맥 정보를 바탕으로 답변을 생성하도록 돕습니다.\n",
    "\n",
    "## 핵심 개념\n",
    "\n",
    "관련 청크는 문서 내에서 함께 클러스터링되는 경향이 있습니다. 이러한 클러스터를 식별하고 연속성을 보존함으로써 LLM이 작업할 수 있는 더 일관된 컨텍스트를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re # 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다.\n",
    "RSE의 경우, 세그먼트를 적절히 재구성하기 위해 일반적으로 겹치지 않는 청크를 사용합니다. 이는 각 청크가 고유한 내용을 담고 있도록 하여, 관련 청크들을 연결했을 때 정보의 중복 없이 자연스러운 문맥을 형성하도록 하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=0):\n",
    "    \"\"\"\n",
    "    텍스트를 겹치지 않는 청크로 분할합니다.\n",
    "    RSE의 경우, 세그먼트를 적절히 재구성할 수 있도록 일반적으로 겹치지 않는 청크를 원합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 청킹할 입력 텍스트\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        overlap (int): 청크 간 문자 중첩 (RSE에서는 보통 0으로 설정)\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 텍스트 청크 목록\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # 간단한 문자 기반 청킹\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk:  # 빈 청크를 추가하지 않도록 확인\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구축\n",
    "간단한 벡터 저장소를 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 가벼운 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=1536): # 사용 모델의 임베딩 차원에 맞춰 설정 (예: OpenAI text-embedding-ada-002는 1536)\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \n",
    "        Args:\n",
    "            dimension (int): 임베딩 차원\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, vectors=None, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 문서를 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): 문서 청크 목록\n",
    "            vectors (List[List[float]], optional): 임베딩 벡터 목록\n",
    "            metadata (List[Dict], optional): 메타데이터 딕셔너리 목록\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata.append(meta)\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        가장 유사한 문서를 검색합니다.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (List[float]): 질의 임베딩 벡터\n",
    "            top_k (int): 반환할 결과 수\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 문서, 점수 및 메타데이터를 포함하는 결과 목록\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # 질의 벡터를 numpy 배열로 변환\n",
    "        query_array = np.array(query_vector)\n",
    "        \n",
    "        # 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "                # 코사인 유사도 계산\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 가져오기\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"document\": self.documents[i],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 청크에 대한 임베딩 생성\n",
    "임베딩은 텍스트를 숫자 벡터로 변환하여 효율적인 유사도 검색을 가능하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    텍스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 임베딩할 텍스트 목록\n",
    "        model (str): 사용할 임베딩 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 임베딩 벡터 목록\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []  # 제공된 텍스트가 없으면 빈 리스트 반환\n",
    "        \n",
    "    # 리스트가 길 경우 배치로 처리\n",
    "    batch_size = 100  # API 제한에 따라 조정\n",
    "    all_embeddings = []  # 모든 임베딩을 저장할 리스트 초기화\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 현재 텍스트 배치 가져오기\n",
    "        \n",
    "        # 지정된 모델을 사용하여 현재 배치에 대한 임베딩 생성\n",
    "        response = client.embeddings.create(\n",
    "            input=batch,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # 응답에서 임베딩 추출\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 배치 임베딩을 리스트에 추가\n",
    "        \n",
    "    return all_embeddings  # 모든 임베딩 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE를 사용한 문서 처리\n",
    "이제 RSE의 핵심 기능을 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=800):\n",
    "    \"\"\"\n",
    "    RSE와 함께 사용할 문서를 처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: 청크, 벡터 저장소 및 문서 정보\n",
    "    \"\"\"\n",
    "    print(\"문서에서 텍스트 추출 중...\")\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"텍스트를 겹치지 않는 세그먼트로 청킹 중...\")\n",
    "    # 추출된 텍스트를 겹치지 않는 세그먼트로 청킹\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n",
    "    print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    # 텍스트 청크에 대한 임베딩 생성\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # SimpleVectorStore 인스턴스 생성\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # 나중에 재구성을 위해 청크 인덱스를 포함한 메타데이터로 문서 추가\n",
    "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    # 세그먼트 재구성을 위한 원본 문서 구조 추적\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 핵심 알고리즘: 청크 값 계산 및 최적 세그먼트 찾기\n",
    "이제 문서 처리 및 청크 임베딩 생성에 필요한 함수를 정의했으므로 RSE의 핵심 알고리즘을 구현할 수 있습니다.\n",
    "핵심 아이디어는 각 청크가 질의와 얼마나 관련 있는지(유사도 점수)를 평가하고, 관련 없는 청크에는 페널티를 부여하여 '청크 값'을 계산합니다. 그런 다음, 이 청크 값들의 합이 최대가 되는 연속적인 청크들의 묶음(세그먼트)들을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    관련성과 위치를 결합하여 청크 값을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 질의 텍스트\n",
    "        chunks (List[str]): 문서 청크 목록\n",
    "        vector_store (SimpleVectorStore): 청크를 포함하는 벡터 저장소\n",
    "        irrelevant_chunk_penalty (float): 관련 없는 청크에 대한 페널티 (이 값만큼 유사도 점수를 차감)\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: 청크 값 목록\n",
    "    \"\"\"\n",
    "    # 질의 임베딩 생성\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 유사도 점수가 있는 모든 청크 가져오기\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # chunk_index를 관련성 점수에 매핑 생성\n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # 청크 값 계산 (관련성 점수 - 페널티)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        # 관련성 점수를 가져오거나 결과에 없으면 기본값 0.0으로 설정\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        # 페널티를 적용하여 관련 없는 청크가 음수 값을 갖도록 변환\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    최대 합 부분 배열 알고리즘의 변형을 사용하여 최적 세그먼트를 찾습니다.\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): 각 청크의 값\n",
    "        max_segment_length (int): 단일 세그먼트의 최대 길이\n",
    "        total_max_length (int): 모든 세그먼트에 걸친 최대 총 길이\n",
    "        min_segment_value (float): 세그먼트가 고려되기 위한 최소 값\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: 최적 세그먼트에 대한 (시작, 끝) 인덱스 목록\n",
    "    \"\"\"\n",
    "    print(\"최적의 연속 텍스트 세그먼트 찾는 중...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # 제한에 도달할 때까지 세그먼트 계속 찾기\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # 세그먼트에 대한 최소 임계값\n",
    "        best_segment = None\n",
    "        \n",
    "        # 가능한 모든 시작 위치 시도\n",
    "        for start in range(len(chunk_values)):\n",
    "            # 이 시작 위치가 이미 선택된 세그먼트에 포함되어 있으면 건너뛰기\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # 가능한 모든 세그먼트 길이 시도\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # 끝 위치가 이미 선택된 세그먼트에 포함되어 있으면 건너뛰기\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # 청크 값의 합으로 세그먼트 값 계산\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # 이 세그먼트가 더 좋으면 최적 세그먼트 업데이트\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # 좋은 세그먼트를 찾았으면 추가\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"점수 {best_score:.4f}로 세그먼트 {best_segment} 찾음\")\n",
    "        else:\n",
    "            # 더 이상 찾을 좋은 세그먼트 없음\n",
    "            break\n",
    "    \n",
    "    # 가독성을 위해 시작 위치별로 세그먼트 정렬\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG를 위한 세그먼트 재구성 및 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    청크 인덱스를 기반으로 텍스트 세그먼트를 재구성합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): 모든 문서 청크 목록\n",
    "        best_segments (List[Tuple[int, int]]): 세그먼트에 대한 (시작, 끝) 인덱스 목록\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 재구성된 텍스트 세그먼트 목록\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  # 재구성된 세그먼트를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # 이 세그먼트의 청크를 결합하여 전체 세그먼트 텍스트 형성\n",
    "        segment_text = \" \".join(chunks[start:end]) # 청크 사이에 공백 추가\n",
    "        # 세그먼트 텍스트와 해당 범위를 reconstructed_segments 리스트에 추가\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # 재구성된 텍스트 세그먼트 목록 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    LLM을 위한 컨텍스트 문자열로 세그먼트 형식을 지정합니다.\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): 세그먼트 딕셔너리 목록\n",
    "        \n",
    "    Returns:\n",
    "        str: 형식화된 컨텍스트 텍스트\n",
    "    \"\"\"\n",
    "    context = []  # 형식화된 컨텍스트를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # 각 세그먼트에 대한 헤더 생성 (인덱스 및 청크 범위 포함)\n",
    "        segment_header = f\"세그먼트 {i+1} (청크 {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # 세그먼트 헤더를 컨텍스트 리스트에 추가\n",
    "        context.append(segment['text'])  # 세그먼트 텍스트를 컨텍스트 리스트에 추가\n",
    "        context.append(\"-\" * 80)  # 가독성을 위한 구분선 추가\n",
    "    \n",
    "    # 컨텍스트 리스트의 모든 요소를 이중 줄 바꿈으로 결합하고 결과 반환\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 컨텍스트를 사용한 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 관련 세그먼트의 컨텍스트 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    print(\"관련 세그먼트를 컨텍스트로 사용하여 응답 생성 중...\")\n",
    "    \n",
    "    # AI의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 제공된 컨텍스트를 기반으로 질문에 답변하는 도움이 되는 어시스턴트입니다.\n",
    "    컨텍스트는 사용자의 질의와 관련하여 검색된 문서 세그먼트로 구성됩니다.\n",
    "    이러한 세그먼트의 정보를 사용하여 포괄적이고 정확한 답변을 제공하십시오.\n",
    "    컨텍스트에 질문에 답변할 관련 정보가 포함되어 있지 않으면 명확하게 그렇게 말하십시오.\"\"\"\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "제공된 컨텍스트를 기반으로 도움이 되는 답변을 제공하십시오.\n",
    "\"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 일관된 답변을 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 RSE 파이프라인 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    관련 세그먼트 추출을 사용한 전체 RAG 파이프라인.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        chunk_size (int): 청크 크기\n",
    "        irrelevant_chunk_penalty (float): 관련 없는 청크에 대한 페널티\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 세그먼트 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 관련 세그먼트 추출을 사용한 RAG 시작 ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청킹 및 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # 질의를 기반으로 관련성 점수 및 청크 값 계산\n",
    "    print(\"\\n관련성 점수 및 청크 값 계산 중...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    # 청크 값을 기반으로 최적 텍스트 세그먼트 찾기\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    # 최적 청크에서 텍스트 세그먼트 재구성\n",
    "    print(\"\\n청크에서 텍스트 세그먼트 재구성 중...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    # 언어 모델을 위한 컨텍스트 문자열로 세그먼트 형식 지정\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    # 컨텍스트를 사용하여 언어 모델에서 응답 생성\n",
    "    response_text = generate_response(query, context) # 변수명을 response에서 response_text로 변경\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 최종 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표준 검색과 비교\n",
    "RSE와 비교하기 위해 표준 검색 접근 방식을 구현해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
    "    \"\"\"\n",
    "    상위 k개 검색을 사용한 표준 RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        k (int): 검색할 청크 수\n",
    "        chunk_size (int): 청크 크기\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 청크 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 표준 상위 K개 검색 시작 ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청킹 및 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # 질의에 대한 임베딩 생성\n",
    "    print(\"질의 임베딩 생성 및 청크 검색 중...\")\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 질의 임베딩을 기반으로 상위 k개의 가장 관련성 높은 청크 검색\n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"document\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크를 컨텍스트 문자열로 형식 지정\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"청크 {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # 컨텍스트를 사용하여 언어 모델에서 응답 생성\n",
    "    response_text = generate_response(query, context) # 변수명을 response에서 response_text로 변경\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 최종 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    RSE를 표준 상위 k개 검색과 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        reference_answer (str, optional): 평가를 위한 참조 답변\n",
    "    \"\"\"\n",
    "    print(\"\\n========= 평가 =========\\n\")\n",
    "    \n",
    "    # 관련 세그먼트 추출(RSE) 방법을 사용한 RAG 실행\n",
    "    rse_result = rag_with_rse(pdf_path, query)\n",
    "    \n",
    "    # 표준 상위 k개 검색 방법 실행\n",
    "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
    "    \n",
    "    # 참조 답변이 제공되면 응답 평가\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== 결과 비교 중 ===\")\n",
    "        \n",
    "        # 참조 답변과 응답을 비교하기 위한 평가 프롬프트 생성\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            질의: {query}\n",
    "\n",
    "            참조 답변:\n",
    "            {reference_answer}\n",
    "\n",
    "            표준 검색 응답:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            관련 세그먼트 추출 응답:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            이 두 응답을 참조 답변과 비교하십시오. 어느 것이:\n",
    "            1. 더 정확하고 포괄적입니까?\n",
    "            2. 사용자의 질의를 더 잘 다룹니까?\n",
    "            3. 관련 없는 정보를 포함할 가능성이 적습니까?\n",
    "\n",
    "            각 항목에 대한 추론을 설명하십시오.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"참조 답변과 응답 평가 중...\")\n",
    "        \n",
    "        # 지정된 모델을 사용하여 평가 생성\n",
    "        evaluation = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 RAG 시스템 응답의 객관적인 평가자입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 평가 결과 출력\n",
    "        print(\"\\n=== 평가 결과 ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "    \n",
    "    # 두 방법의 결과 반환\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= EVALUATION =========\n",
      "\n",
      "\n",
      "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "\n",
      "Calculating relevance scores and chunk values...\n",
      "Finding optimal continuous text segments...\n",
      "Found segment (21, 41) with score 9.0718\n",
      "Found segment (0, 20) with score 8.8685\n",
      "\n",
      "Reconstructing text segments from chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "Based on the context provided, Explainable AI (XAI) refers to the development of techniques that make AI systems more transparent and understandable. The goal of XAI is to provide insights into how AI models make decisions, enhancing trust and accountability in AI systems.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Building trust**: XAI helps users understand how AI systems arrive at their decisions, which is essential for building trust in AI. When users can see how AI systems work, they are more likely to accept the results.\n",
      "2. **Addressing bias**: XAI can help identify biases in AI systems by providing insights into how they make decisions. By understanding how AI systems work, developers can identify and address biases in the data they are trained on.\n",
      "3. **Improving accountability**: XAI enables developers to take responsibility for the decisions made by AI systems. By providing explanations for AI decisions, developers can be held accountable for any errors or biases in the system.\n",
      "4. **Enhancing transparency**: XAI provides insights into how AI systems work, which is essential for transparency in AI decision-making. This is particularly important in high-stakes applications, such as healthcare or finance, where users need to understand how AI systems arrive at their decisions.\n",
      "\n",
      "Overall, XAI is considered important because it addresses the need for transparency, accountability, and trust in AI systems. By providing insights into how AI models make decisions, XAI can help build trust, address bias, and improve accountability in AI development and deployment.\n",
      "\n",
      "=== STARTING STANDARD TOP-K RETRIEVAL ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "Creating query embedding and retrieving chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "Based on the provided context, Explainable AI (XAI) is a technique that aims to make AI decisions more understandable, enabling users to assess their fairness and accuracy. XAI techniques are designed to provide insights into how AI systems arrive at their decisions, enhancing transparency and explainability.\n",
      "\n",
      "XAI is considered important for several reasons:\n",
      "\n",
      "1. **Building trust in AI**: By making AI decisions more understandable, XAI helps build trust in AI systems, which is essential for their widespread adoption.\n",
      "2. **Addressing potential harms**: XAI can help identify potential biases and errors in AI decision-making, allowing for more effective mitigation and prevention of harms.\n",
      "3. **Ensuring accountability**: XAI provides a way to establish accountability for AI decisions, which is crucial for addressing potential consequences and ensuring ethical behavior.\n",
      "4. **Improving fairness and accuracy**: By providing insights into AI decision-making, XAI can help identify and address biases and errors, leading to more fair and accurate outcomes.\n",
      "\n",
      "Overall, Explainable AI is a critical aspect of developing trustworthy, fair, and accurate AI systems, and its importance will only continue to grow as AI becomes increasingly pervasive in various domains.\n",
      "\n",
      "=== COMPARING RESULTS ===\n",
      "Evaluating responses against reference answer...\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Based on the comparison, I would conclude that:\n",
      "\n",
      "1. **The Response from Standard Retrieval is more accurate and comprehensive:**\n",
      "   The Response from Standard Retrieval provides a clear definition of Explainable AI (XAI) and its importance. It explains the goals of XAI, its key aspects (transparency and understandability), and its benefits. The explanation highlights the reasons why XAI is considered important, which includes building trust, addressing potential harms, ensuring accountability, and improving fairness and accuracy.\n",
      "\n",
      "   On the other hand, the Response from Relevant Segment Extraction provides a simplified explanation of XAI but focuses more on the aspects of trust, bias, accountability, and transparency. While it does provide a clear overview of XAI, it is more concise and somewhat less detailed than the Response from Standard Retrieval.\n",
      "\n",
      "2. **The Response from Relevant Segment Extraction is better at addressing the user's query:**\n",
      "   The Response from Standard Retrieval not only answers the question but also provides additional context and importance. The Response from Relevant Segment Extraction, however, more closely addresses the original question by focusing on the core aspects of XAI and its advantages.\n",
      "\n",
      "3. **The Response from Standard Retrieval is less likely to include irrelevant information:**\n",
      "   This response is less likely to contain unnecessary details. The Response from Standard Retrieval provides a clear and concise answer with a clear structure, focusing on the key points of XAI and its importance. In contrast, the Response from Relevant Segment Extraction might include some details about AI systems, updates, or other related information that is not essential to answering the original question.\n",
      "\n",
      "However, the Response from Standard Retrieval includes a more relevant and comprehensive discussion of the reference answer's points, than the Response from Relevant Segment Extraction, which is closer to the reference answer but strays closer to an ensuing span-specific update concerning explainable AI underpoint of the start precisuliar generation machinery.\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 첫 번째 질의 추출\n",
    "query = data[0]['question']\n",
    "\n",
    "# 검증 데이터에서 참조 답변 추출\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 평가 실행\n",
    "results = evaluate_methods(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 09_rse.ipynb]
