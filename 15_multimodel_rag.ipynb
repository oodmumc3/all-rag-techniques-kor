{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 이미지 캡셔닝을 사용한 멀티모달 RAG\n",
    "\n",
    "이 노트북에서는 문서에서 텍스트와 이미지를 모두 추출하고, 이미지에 대한 캡션을 생성하며, 두 콘텐츠 유형을 모두 사용하여 질의에 응답하는 멀티모달 RAG 시스템을 구현합니다. 이 접근 방식은 시각적 정보를 지식 기반에 통합하여 기존 RAG를 향상시킵니다.\n",
    "멀티모달(Multi-Modal) RAG는 텍스트뿐만 아니라 이미지, 오디오 등 다양한 유형의 데이터를 이해하고 활용하는 RAG 시스템을 의미합니다. 이 노트북에서는 특히 이미지 데이터를 함께 처리하는 방법에 초점을 맞춥니다. 문서 내의 이미지는 텍스트만으로는 전달하기 어려운 중요한 정보를 담고 있는 경우가 많습니다. 멀티모달 RAG는 이러한 이미지의 내용을 이해(예: 이미지 캡셔닝을 통해 텍스트로 변환)하고, 이를 텍스트 정보와 함께 검색 및 답변 생성에 활용하여 더 풍부하고 정확한 답변을 제공할 수 있도록 합니다.\n",
    "\n",
    "기존 RAG 시스템은 텍스트로만 작동하지만, 많은 문서에는 그림, 차트 및 표에 중요한 정보가 포함되어 있습니다. 이러한 시각적 요소를 캡션 처리하고 검색 시스템에 통합함으로써 다음을 수행할 수 있습니다:\n",
    "\n",
    "- 그림 및 다이어그램에 잠긴 정보에 접근\n",
    "- 텍스트를 보완하는 표 및 차트 이해\n",
    "- 보다 포괄적인 지식 기반 생성\n",
    "- 시각적 데이터에 의존하는 질문에 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz # PyMuPDF\n",
    "from PIL import Image # 이미지 처리를 위한 Pillow 라이브러리\n",
    "from openai import OpenAI\n",
    "import base64 # 이미지를 base64로 인코딩하기 위함\n",
    "import re # 정규 표현식\n",
    "import tempfile # 임시 디렉토리 생성용\n",
    "import shutil # 디렉토리 삭제용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트와 이미지를 모두 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        output_dir (str, optional): 추출된 이미지를 저장할 디렉터리\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: 텍스트 데이터 및 이미지 데이터\n",
    "    \"\"\"\n",
    "    # 제공되지 않은 경우 이미지용 임시 디렉터리 생성\n",
    "    temp_dir_created = False # 임시 디렉토리 생성 여부 플래그\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp() # 임시 디렉토리 생성\n",
    "        output_dir = temp_dir\n",
    "        temp_dir_created = True\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True) # 존재하지 않으면 디렉토리 생성\n",
    "        \n",
    "    text_data = []  # 추출된 텍스트 데이터를 저장할 리스트\n",
    "    image_paths_data = []  # 추출된 이미지 경로를 저장할 리스트 (변수명 변경: image_paths -> image_paths_data)\n",
    "    \n",
    "    print(f\"{pdf_path}에서 콘텐츠 추출 중...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # PDF의 모든 페이지 반복\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # 페이지에서 텍스트 추출\n",
    "                text_content = page.get_text().strip() # text를 text_content로 변경\n",
    "                if text_content:\n",
    "                    text_data.append({\n",
    "                        \"content\": text_content,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # 페이지에서 이미지 추출\n",
    "                image_list_info = page.get_images(full=True) # image_list를 image_list_info로 변경\n",
    "                for img_index, img_info in enumerate(image_list_info): # img를 img_info로 변경\n",
    "                    xref = img_info[0]  # 이미지의 XREF\n",
    "                    base_image_data = pdf_file.extract_image(xref) # base_image를 base_image_data로 변경\n",
    "                    \n",
    "                    if base_image_data:\n",
    "                        image_bytes_data = base_image_data[\"image\"] # image_bytes를 image_bytes_data로 변경\n",
    "                        image_ext_type = base_image_data[\"ext\"] # image_ext를 image_ext_type으로 변경\n",
    "                        \n",
    "                        # 이미지를 출력 디렉터리에 저장\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext_type}\"\n",
    "                        img_path_val = os.path.join(output_dir, img_filename) # img_path를 img_path_val로 변경\n",
    "                        \n",
    "                        with open(img_path_val, \"wb\") as img_file_obj: # img_file을 img_file_obj로 변경\n",
    "                            img_file_obj.write(image_bytes_data)\n",
    "                        \n",
    "                        image_paths_data.append({\n",
    "                            \"path\": img_path_val,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"{len(text_data)}개의 텍스트 세그먼트와 {len(image_paths_data)}개의 이미지 추출됨\")\n",
    "        return text_data, image_paths_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"콘텐츠 추출 오류: {e}\")\n",
    "        if temp_dir_created and os.path.exists(output_dir): # 임시 디렉토리가 생성되었고 존재하는 경우에만 삭제\n",
    "            shutil.rmtree(output_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 콘텐츠 청킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 중첩되는 청크로 분할합니다.\n",
    "    \n",
    "    Args:\n",
    "        text_data (List[Dict]): PDF에서 추출된 텍스트 데이터\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        overlap (int): 청크 간 문자 중첩\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 청킹된 텍스트 데이터\n",
    "    \"\"\"\n",
    "    chunked_data_list = []  # 청킹된 데이터를 저장할 빈 리스트 초기화 (chunked_data를 chunked_data_list로 변경)\n",
    "    \n",
    "    for item_data in text_data: # item을 item_data로 변경\n",
    "        text_content = item_data[\"content\"]  # 텍스트 콘텐츠 추출 (text를 text_content로 변경)\n",
    "        metadata_info = item_data[\"metadata\"]  # 메타데이터 추출 (metadata를 metadata_info로 변경)\n",
    "        \n",
    "        # 텍스트가 너무 짧으면 건너뛰기 (청크 크기의 절반보다 작으면 원본 그대로 사용)\n",
    "        if len(text_content) < chunk_size / 2:\n",
    "            chunked_data_list.append({\n",
    "                \"content\": text_content,\n",
    "                \"metadata\": metadata_info\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # 중첩을 사용하여 청크 생성\n",
    "        current_chunks = [] # chunks를 current_chunks로 변경\n",
    "        for i in range(0, len(text_content), chunk_size - overlap):\n",
    "            chunk_segment = text_content[i:i + chunk_size]  # 지정된 크기의 청크 추출 (chunk를 chunk_segment로 변경)\n",
    "            if chunk_segment:  # 빈 청크를 추가하지 않도록 확인\n",
    "                current_chunks.append(chunk_segment)\n",
    "        \n",
    "        # 업데이트된 메타데이터와 함께 각 청크 추가\n",
    "        for i, chunk_segment in enumerate(current_chunks):\n",
    "            chunk_metadata_info = metadata_info.copy()  # 원본 메타데이터 복사 (chunk_metadata를 chunk_metadata_info로 변경)\n",
    "            chunk_metadata_info[\"chunk_index\"] = i  # 메타데이터에 청크 인덱스 추가\n",
    "            chunk_metadata_info[\"chunk_count\"] = len(current_chunks)  # 메타데이터에 총 청크 수 추가\n",
    "            \n",
    "            chunked_data_list.append({\n",
    "                \"content\": chunk_segment,  # 청크 텍스트\n",
    "                \"metadata\": chunk_metadata_info  # 업데이트된 메타데이터\n",
    "            })\n",
    "    \n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunked_data_list)}\")  # 생성된 청크 수 출력\n",
    "    return chunked_data_list  # 청킹된 데이터 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Vision을 사용한 이미지 캡셔닝\n",
    "LLaVA와 같은 멀티모달 모델을 사용하여 이미지의 내용을 설명하는 캡션을 생성합니다. 이 캡션은 텍스트 정보와 마찬가지로 검색 대상이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    이미지 파일을 base64로 인코딩합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64로 인코딩된 이미지\n",
    "    \"\"\"\n",
    "    # 이미지 파일을 바이너리 읽기 모드로 열기\n",
    "    with open(image_path, \"rb\") as image_file_obj: # image_file을 image_file_obj로 변경\n",
    "        # 이미지 파일을 읽고 base64로 인코딩\n",
    "        encoded_image_bytes = base64.b64encode(image_file_obj.read()) # encoded_image를 encoded_image_bytes로 변경\n",
    "        # base64 바이트를 문자열로 디코딩하여 반환\n",
    "        return encoded_image_bytes.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    OpenAI의 비전 기능을 사용하여 이미지에 대한 캡션을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 캡션\n",
    "    \"\"\"\n",
    "    # 파일이 존재하고 이미지인지 확인\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"오류: 이미지 파일을 찾을 수 없습니다.\"\n",
    "    \n",
    "    try:\n",
    "        # 이미지 열기 및 유효성 검사\n",
    "        Image.open(image_path)\n",
    "        \n",
    "        # 이미지를 base64로 인코딩\n",
    "        base64_image_data = encode_image(image_path) # base64_image를 base64_image_data로 변경\n",
    "        \n",
    "        # 캡션 생성을 위한 API 요청 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llava-hf/llava-1.5-7b-hf\", # llava-1.5-7b 모델 사용 (OpenAI API와 호환되는 멀티모달 모델 이름으로 가정)\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"당신은 학술 논문의 이미지를 설명하는 데 특화된 어시스턴트입니다. \"\n",
    "                    \"핵심 정보를 포착하는 이미지에 대한 자세한 캡션을 제공하십시오. \"\n",
    "                    \"이미지에 차트, 표 또는 다이어그램이 포함된 경우 해당 내용과 목적을 명확하게 설명하십시오. \"\n",
    "                    \"캡션은 향후 사람들이 이 콘텐츠에 대해 질문할 때 검색에 최적화되어야 합니다.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"학문적 내용에 초점을 맞춰 이 이미지를 자세히 설명하십시오:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image_data}\" # 이미지 형식을 jpeg으로 가정, 필요시 실제 형식에 맞게 수정\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # 응답에서 캡션 추출\n",
    "        caption_text = response.choices[0].message.content # caption을 caption_text로 변경\n",
    "        return caption_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 오류 메시지 반환\n",
    "        return f\"캡션 생성 오류: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths_list):\n",
    "    \"\"\"\n",
    "    모든 이미지를 처리하고 캡션을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_paths_list (List[Dict]): 추출된 이미지 경로 목록 (변수명 변경: image_paths -> image_paths_list)\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 캡션이 있는 이미지 데이터 목록\n",
    "    \"\"\"\n",
    "    image_data_list = []  # 캡션이 있는 이미지 데이터를 저장할 빈 리스트 초기화 (image_data를 image_data_list로 변경)\n",
    "    \n",
    "    print(f\"{len(image_paths_list)}개의 이미지에 대한 캡션 생성 중...\")  # 처리할 이미지 수 출력\n",
    "    for i, img_item_data in enumerate(image_paths_list): # img_item을 img_item_data로 변경\n",
    "        print(f\"이미지 {i+1}/{len(image_paths_list)} 처리 중...\")  # 현재 처리 중인 이미지 출력\n",
    "        img_path_val = img_item_data[\"path\"]  # 이미지 경로 가져오기 (img_path를 img_path_val로 변경)\n",
    "        metadata_info = img_item_data[\"metadata\"]  # 이미지 메타데이터 가져오기 (metadata를 metadata_info로 변경)\n",
    "        \n",
    "        # 이미지에 대한 캡션 생성\n",
    "        caption_text = generate_image_caption(img_path_val) # caption을 caption_text로 변경\n",
    "        \n",
    "        # 캡션이 있는 이미지 데이터를 리스트에 추가\n",
    "        image_data_list.append({\n",
    "            \"content\": caption_text,  # 생성된 캡션\n",
    "            \"metadata\": metadata_info,  # 이미지 메타데이터\n",
    "            \"image_path\": img_path_val  # 이미지 경로\n",
    "        })\n",
    "    \n",
    "    return image_data_list  # 캡션이 있는 이미지 데이터 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현\n",
    "텍스트 청크와 이미지 캡션을 모두 저장할 수 있는 멀티모달 벡터 저장소입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    멀티모달 콘텐츠를 위한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 벡터, 콘텐츠 및 메타데이터를 저장할 리스트 초기화\n",
    "        self.vectors = []\n",
    "        self.contents = [] # 텍스트 또는 이미지 캡션 저장\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            content (str): 콘텐츠 (텍스트 또는 이미지 캡션)\n",
    "            embedding (List[float]): 임베딩 벡터\n",
    "            metadata (Dict, optional): 추가 메타데이터\n",
    "        \"\"\"\n",
    "        # 각 리스트에 임베딩 벡터, 콘텐츠 및 메타데이터 추가\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items_list, embeddings_list): # items, embeddings를 items_list, embeddings_list로 변경\n",
    "        \"\"\"\n",
    "        벡터 저장소에 여러 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            items_list (List[Dict]): 콘텐츠 항목 목록\n",
    "            embeddings_list (List[List[float]]): 임베딩 벡터 목록\n",
    "        \"\"\"\n",
    "        # 항목과 임베딩을 반복하며 각 항목을 벡터 저장소에 추가\n",
    "        for item_data, embedding_vector in zip(items_list, embeddings_list): # item, embedding을 item_data, embedding_vector로 변경\n",
    "            self.add_item(\n",
    "                content=item_data[\"content\"],\n",
    "                embedding=embedding_vector,\n",
    "                metadata=item_data.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 결과 수\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 가장 유사한 항목\n",
    "        \"\"\"\n",
    "        # 저장소에 벡터가 없으면 빈 리스트 반환\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector_item in enumerate(self.vectors): # vector를 vector_item으로 변경\n",
    "            similarity = np.dot(query_vector, vector_item) / (np.linalg.norm(query_vector) * np.linalg.norm(vector_item))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score_value = similarities[i] # score를 score_value로 변경\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score_value)  # JSON 직렬화를 위해 float으로 변환\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 입력 텍스트 목록\n",
    "        model (str): 임베딩 모델 이름\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 임베딩 벡터 목록\n",
    "    \"\"\"\n",
    "    # 빈 입력 처리\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # 필요한 경우 배치로 처리 (OpenAI API 제한)\n",
    "    batch_size = 100\n",
    "    all_embeddings_list = [] # all_embeddings를 all_embeddings_list로 변경\n",
    "    \n",
    "    # 입력 텍스트를 배치로 반복\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]  # 현재 텍스트 배치 가져오기 (batch를 batch_texts로 변경)\n",
    "        \n",
    "        # 현재 배치에 대한 임베딩 생성\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        # 응답에서 임베딩 추출\n",
    "        batch_embeddings_list = [item.embedding for item in response.data] # batch_embeddings를 batch_embeddings_list로 변경\n",
    "        all_embeddings_list.extend(batch_embeddings_list)  # 배치 임베딩을 리스트에 추가\n",
    "    \n",
    "    return all_embeddings_list  # 모든 임베딩 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 처리 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG를 위해 문서를 처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        chunk_overlap (int): 청크 간 문자 중첩\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[MultiModalVectorStore, Dict]: 벡터 저장소 및 문서 정보\n",
    "    \"\"\"\n",
    "    # 추출된 이미지를 위한 디렉터리 생성\n",
    "    image_output_dir = \"extracted_images\" # image_dir을 image_output_dir로 변경\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "    \n",
    "    # PDF에서 텍스트와 이미지 추출\n",
    "    text_data_list, image_paths_list = extract_content_from_pdf(pdf_path, image_output_dir) # text_data, image_paths 변수명 변경\n",
    "    \n",
    "    # 추출된 텍스트 청킹\n",
    "    chunked_text_data = chunk_text(text_data_list, chunk_size, chunk_overlap) # chunked_text를 chunked_text_data로 변경\n",
    "    \n",
    "    # 추출된 이미지를 처리하여 캡션 생성\n",
    "    image_caption_data = process_images(image_paths_list) # image_data를 image_caption_data로 변경\n",
    "    \n",
    "    # 모든 콘텐츠 항목 결합 (텍스트 청크 및 이미지 캡션)\n",
    "    all_content_items = chunked_text_data + image_caption_data # all_items를 all_content_items로 변경\n",
    "    \n",
    "    # 임베딩을 위한 콘텐츠 추출\n",
    "    contents_for_embedding = [item_data[\"content\"] for item_data in all_content_items] # contents를 contents_for_embedding으로, item을 item_data로 변경\n",
    "    \n",
    "    # 모든 콘텐츠에 대한 임베딩 생성\n",
    "    print(\"모든 콘텐츠에 대한 임베딩 생성 중...\")\n",
    "    content_embeddings = create_embeddings(contents_for_embedding) # embeddings를 content_embeddings로 변경\n",
    "    \n",
    "    # 벡터 저장소 구축 및 해당 임베딩과 함께 항목 추가\n",
    "    vector_store_instance = MultiModalVectorStore() # vector_store를 vector_store_instance로 변경\n",
    "    vector_store_instance.add_items(all_content_items, content_embeddings)\n",
    "    \n",
    "    # 텍스트 청크 및 이미지 캡션 수를 포함하는 문서 정보 준비\n",
    "    doc_info_summary = { # doc_info를 doc_info_summary로 변경\n",
    "        \"text_count\": len(chunked_text_data),\n",
    "        \"image_count\": len(image_caption_data),\n",
    "        \"total_items\": len(all_content_items),\n",
    "    }\n",
    "    \n",
    "    # 추가된 항목 요약 출력\n",
    "    print(f\"벡터 저장소에 {len(all_content_items)}개의 항목 추가됨 ({len(chunked_text_data)}개의 텍스트 청크, {len(image_caption_data)}개의 이미지 캡션)\")\n",
    "    \n",
    "    # 벡터 저장소와 문서 정보 반환\n",
    "    return vector_store_instance, doc_info_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 처리 및 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG 시스템에 질의합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (MultiModalVectorStore): 문서 콘텐츠가 있는 벡터 저장소\n",
    "        k (int): 검색할 결과 수\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의 결과 및 생성된 응답\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== '{query}' 질의 처리 중 ===\\n\")\n",
    "    \n",
    "    # 질의에 대한 임베딩 생성\n",
    "    query_embedding_vector = create_embeddings(query) # query_embedding을 query_embedding_vector로 변경\n",
    "    \n",
    "    # 벡터 저장소에서 관련 콘텐츠 검색\n",
    "    retrieved_results = vector_store.similarity_search(query_embedding_vector, k=k) # results를 retrieved_results로 변경\n",
    "    \n",
    "    # 텍스트 및 이미지 결과 분리\n",
    "    text_retrieval_results = [r for r in retrieved_results if r[\"metadata\"].get(\"type\") == \"text\"] # text_results를 text_retrieval_results로 변경\n",
    "    image_retrieval_results = [r for r in retrieved_results if r[\"metadata\"].get(\"type\") == \"image\"] # image_results를 image_retrieval_results로 변경\n",
    "    \n",
    "    print(f\"{len(retrieved_results)}개의 관련 항목 검색됨 ({len(text_retrieval_results)}개의 텍스트, {len(image_retrieval_results)}개의 이미지 캡션)\")\n",
    "    \n",
    "    # 검색된 콘텐츠를 사용하여 응답 생성\n",
    "    response_text = generate_response(query, retrieved_results) # response를 response_text로 변경\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": retrieved_results,\n",
    "        \"response\": response_text,\n",
    "        \"text_results_count\": len(text_retrieval_results),\n",
    "        \"image_results_count\": len(image_retrieval_results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    질의와 검색된 결과를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 검색된 콘텐츠\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # 검색된 결과에서 컨텍스트 형식 지정\n",
    "    context_str = \"\" # context를 context_str로 변경\n",
    "    \n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        # 콘텐츠 유형 결정 (텍스트 또는 이미지 캡션)\n",
    "        content_type_label = \"텍스트\" if result_item[\"metadata\"].get(\"type\") == \"text\" else \"이미지 캡션\" # content_type을 content_type_label로 변경\n",
    "        # 메타데이터에서 페이지 번호 가져오기\n",
    "        page_identifier = result_item[\"metadata\"].get(\"page\", \"알 수 없음\") # page_num을 page_identifier로 변경\n",
    "        \n",
    "        # 컨텍스트에 콘텐츠 유형 및 페이지 번호 추가\n",
    "        context_str += f\"[{content_type_label} 페이지 {page_identifier}에서]\\n\"\n",
    "        # 컨텍스트에 실제 콘텐츠 추가\n",
    "        context_str += result_item[\"content\"]\n",
    "        context_str += \"\\n\\n\"\n",
    "    \n",
    "    # AI 어시스턴트를 안내하는 시스템 메시지\n",
    "    system_message_text = \"\"\"당신은 텍스트와 이미지를 모두 포함하는 문서에 대한 질문에 답변하는 데 특화된 AI 어시스턴트입니다. \n",
    "    문서에서 관련 텍스트 구절과 이미지 캡션을 받았습니다. \n",
    "    이 정보를 사용하여 질의에 대한 포괄적이고 정확한 응답을 제공하십시오.\n",
    "    정보가 이미지나 차트에서 나온 경우 답변에 언급하십시오.\n",
    "    검색된 정보가 질의에 완전히 답변하지 못하면 한계를 인정하십시오.\"\"\" # system_message를 system_message_text로 변경\n",
    "\n",
    "    # 질의와 형식화된 컨텍스트를 포함하는 사용자 메시지\n",
    "    user_message_text = f\"\"\"질의: {query}\n",
    "\n",
    "    검색된 콘텐츠:\n",
    "    {context_str}\n",
    "\n",
    "    검색된 콘텐츠를 기반으로 질의에 답변하십시오.\n",
    "    \"\"\" # user_message를 user_message_text로 변경\n",
    "    \n",
    "    # OpenAI API를 사용하여 응답 생성\n",
    "    api_response = client.chat.completions.create( # response를 api_response로 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message_text},\n",
    "            {\"role\": \"user\", \"content\": user_message_text}\n",
    "        ],\n",
    "        temperature=0.1 # 약간의 창의성을 허용\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 반환\n",
    "    return api_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전용 RAG와 비교 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    비교를 위해 텍스트 전용 벡터 저장소를 구축합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        chunk_overlap (int): 청크 간 문자 중첩\n",
    "        \n",
    "    Returns:\n",
    "        MultiModalVectorStore: 텍스트 전용 벡터 저장소 (동일한 클래스 사용, 내용만 텍스트)\n",
    "    \"\"\"\n",
    "    # PDF에서 텍스트 추출 (이미지는 무시하고 함수 재사용)\n",
    "    text_data_list, _ = extract_content_from_pdf(pdf_path, None) # text_data를 text_data_list로 변경\n",
    "    \n",
    "    # 텍스트 청킹\n",
    "    chunked_text_data = chunk_text(text_data_list, chunk_size, chunk_overlap) # chunked_text를 chunked_text_data로 변경\n",
    "    \n",
    "    # 임베딩을 위한 콘텐츠 추출\n",
    "    contents_for_embedding = [item_data[\"content\"] for item_data in chunked_text_data] # contents를 contents_for_embedding으로, item을 item_data로 변경\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    print(\"텍스트 전용 콘텐츠에 대한 임베딩 생성 중...\")\n",
    "    content_embeddings = create_embeddings(contents_for_embedding) # embeddings를 content_embeddings로 변경\n",
    "    \n",
    "    # 벡터 저장소 구축\n",
    "    text_only_vector_store = MultiModalVectorStore() # vector_store를 text_only_vector_store로 변경\n",
    "    text_only_vector_store.add_items(chunked_text_data, content_embeddings)\n",
    "    \n",
    "    print(f\"텍스트 전용 벡터 저장소에 {len(chunked_text_data)}개의 텍스트 항목 추가됨\")\n",
    "    return text_only_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG와 텍스트 전용 RAG를 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        test_queries (List[str]): 테스트 질의\n",
    "        reference_answers (List[str], optional): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"=== 멀티모달 RAG 대 텍스트 전용 RAG 평가 ===\\n\")\n",
    "    \n",
    "    # 멀티모달 RAG용 문서 처리\n",
    "    print(\"\\n멀티모달 RAG용 문서 처리 중...\")\n",
    "    mm_vector_store_instance, mm_doc_info_summary = process_document(pdf_path) # mm_vector_store, mm_doc_info 변수명 변경\n",
    "    \n",
    "    # 텍스트 전용 저장소 구축\n",
    "    print(\"\\n텍스트 전용 RAG용 문서 처리 중...\")\n",
    "    text_only_vector_store_instance = build_text_only_store(pdf_path) # text_vector_store를 text_only_vector_store_instance로 변경\n",
    "    \n",
    "    # 각 질의에 대한 평가 실행\n",
    "    evaluation_results_list = [] # results를 evaluation_results_list로 변경\n",
    "    \n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n\\n=== 질의 {i+1} 평가 중: {query_text} ===\")\n",
    "        \n",
    "        # 사용 가능한 경우 참조 답변 가져오기\n",
    "        reference_text = None # reference를 reference_text로 변경\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference_text = reference_answers[i]\n",
    "        \n",
    "        # 멀티모달 RAG 실행\n",
    "        print(\"\\n멀티모달 RAG 실행 중...\")\n",
    "        mm_query_result = query_multimodal_rag(query_text, mm_vector_store_instance) # mm_result를 mm_query_result로 변경\n",
    "        \n",
    "        # 텍스트 전용 RAG 실행\n",
    "        print(\"\\n텍스트 전용 RAG 실행 중...\")\n",
    "        text_query_result = query_multimodal_rag(query_text, text_only_vector_store_instance) # text_result를 text_query_result로 변경\n",
    "        \n",
    "        # 응답 비교\n",
    "        comparison_analysis = compare_responses(query_text, mm_query_result[\"response\"], text_query_result[\"response\"], reference_text) # comparison을 comparison_analysis로 변경\n",
    "        \n",
    "        # 결과에 추가\n",
    "        evaluation_results_list.append({\n",
    "            \"query\": query_text,\n",
    "            \"multimodal_response\": mm_query_result[\"response\"],\n",
    "            \"textonly_response\": text_query_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_query_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_query_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference_text,\n",
    "            \"comparison\": comparison_analysis\n",
    "        })\n",
    "    \n",
    "    # 전체 분석 생성\n",
    "    overall_analysis_text = generate_overall_analysis(evaluation_results_list) # overall_analysis를 overall_analysis_text로 변경\n",
    "    \n",
    "    return {\n",
    "        \"results\": evaluation_results_list,\n",
    "        \"overall_analysis\": overall_analysis_text,\n",
    "        \"multimodal_doc_info\": mm_doc_info_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, mm_response, text_response, reference=None):\n",
    "    \"\"\"\n",
    "    멀티모달 및 텍스트 전용 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        mm_response (str): 멀티모달 응답\n",
    "        text_response (str): 텍스트 전용 응답\n",
    "        reference (str, optional): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 비교 분석\n",
    "    \"\"\"\n",
    "    # 평가자를 위한 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 다음 두 RAG 시스템을 비교하는 전문 평가자입니다:\n",
    "    1. 멀티모달 RAG: 텍스트와 이미지 캡션 모두에서 검색\n",
    "    2. 텍스트 전용 RAG: 텍스트에서만 검색\n",
    "\n",
    "    다음을 기준으로 어떤 응답이 질의에 더 잘 답변하는지 평가하십시오:\n",
    "    - 정확성 및 올바름\n",
    "    - 정보의 완전성\n",
    "    - 질의 관련성\n",
    "    - (멀티모달의 경우) 시각적 요소의 고유한 정보\"\"\"\n",
    "\n",
    "    # 질의와 응답을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "    멀티모달 RAG 응답:\n",
    "    {mm_response}\n",
    "\n",
    "    텍스트 전용 RAG 응답:\n",
    "    {text_response}\n",
    "    \"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    참조 답변:\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "    # 이전에 user_prompt가 이 지점에서 끝나고 있었으므로, 지침을 여기에 추가합니다.\n",
    "    user_prompt += \"\"\"\n",
    "    이러한 응답을 비교하고 어떤 것이 질의에 더 잘 답변하는지, 그 이유는 무엇인지 설명하십시오.\n",
    "    멀티모달 응답에서 이미지에서 나온 특정 정보를 언급하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # meta-llama/Llama-3.2-3B-Instruct를 사용하여 비교 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    멀티모달 대 텍스트 전용 RAG에 대한 전체 분석을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): 각 질의에 대한 평가 결과\n",
    "        \n",
    "    Returns:\n",
    "        str: 전체 분석\n",
    "    \"\"\"\n",
    "    # 평가자를 위한 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다. 여러 테스트 질의를 기반으로 멀티모달 RAG(텍스트 + 이미지)와 텍스트 전용 RAG를 비교하는 전체 분석을 제공하십시오.\n",
    "\n",
    "    다음에 초점을 맞추십시오:\n",
    "    1. 멀티모달 RAG가 텍스트 전용보다 뛰어난 성능을 보이는 질의 유형\n",
    "    2. 이미지 정보 통합의 구체적인 이점\n",
    "    3. 멀티모달 접근 방식의 단점 또는 한계\n",
    "    4. 각 접근 방식을 언제 사용해야 하는지에 대한 전반적인 권장 사항\"\"\"\n",
    "\n",
    "    # 평가 요약 생성\n",
    "    evaluations_summary_text = \"\" # evaluations_summary를 evaluations_summary_text로 변경\n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        evaluations_summary_text += f\"질의 {i+1}: {result_item['query']}\\n\"\n",
    "        evaluations_summary_text += f\"멀티모달은 {result_item['multimodal_results']['text_count']}개의 텍스트 청크와 {result_item['multimodal_results']['image_count']}개의 이미지 캡션을 검색했습니다.\\n\"\n",
    "        evaluations_summary_text += f\"비교 요약: {result_item['comparison'][:200]}...\\n\\n\" # 너무 길 경우를 대비해 요약본 사용\n",
    "\n",
    "    # 평가 요약을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"다음 {len(results)}개 질의에 대한 멀티모달 대 텍스트 전용 RAG 평가를 바탕으로, \n",
    "    이 두 접근 방식을 비교하는 전체 분석을 제공하십시오:\n",
    "\n",
    "{evaluations_summary_text}\n",
    "\n",
    "이미지 정보가 응답 품질에 어떻게 기여했는지(또는 기여하지 못했는지)에 특히 주의하여 멀티모달 RAG와 텍스트 전용 RAG의 상대적인 강점과 약점에 대한 포괄적인 분석을 제공하십시오.\"\"\"\n",
    "\n",
    "    # meta-llama/Llama-3.2-3B-Instruct를 사용하여 전체 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 멀티모달 RAG 대 텍스트 전용 RAG 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 멀티모달 RAG 대 텍스트 전용 RAG 평가 ===\n",
      "\n",
      "\n",
      "멀티모달 RAG용 문서 처리 중...\n",
      "data/attention_is_all_you_need.pdf에서 콘텐츠 추출 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15개의 텍스트 세그먼트와 3개의 이미지 추출됨\n",
      "생성된 텍스트 청크 수: 59\n",
      "3개의 이미지에 대한 캡션 생성 중...\n",
      "이미지 1/3 처리 중...\n",
      "이미지 2/3 처리 중...\n",
      "이미지 3/3 처리 중...\n",
      "모든 콘텐츠에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 62개의 항목 추가됨 (59개의 텍스트 청크, 3개의 이미지 캡션)\n",
      "\n",
      "텍스트 전용 RAG용 문서 처리 중...\n",
      "data/attention_is_all_you_need.pdf에서 콘텐츠 추출 중...\n",
      "15개의 텍스트 세그먼트와 3개의 이미지 추출됨\n",
      "생성된 텍스트 청크 수: 59\n",
      "텍스트 전용 콘텐츠에 대한 임베딩 생성 중...\n",
      "텍스트 전용 벡터 저장소에 59개의 텍스트 항목 추가됨\n",
      "\n",
      "\n",
      "=== 질의 1 평가 중: 트랜스포머(기본 모델)의 BLEU 점수는 무엇인가? ===\n",
      "\n",
      "멀티모달 RAG 실행 중...\n",
      "\n",
      "=== '트랜스포머(기본 모델)의 BLEU 점수는 무엇인가?' 질의 처리 중 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_14692\\2117883450.py:75: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Convert to float for JSON serialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5개의 관련 항목 검색됨 (5개의 텍스트, 0개의 이미지 캡션)\n",
      "\n",
      "텍스트 전용 RAG 실행 중...\n",
      "\n",
      "=== '트랜스포머(기본 모델)의 BLEU 점수는 무엇인가?' 질의 처리 중 ===\n",
      "\n",
      "5개의 관련 항목 검색됨 (5개의 텍스트, 0개의 이미지 캡션)\n",
      "\n",
      "=== 전체 분석 ===\n",
      "\n",
      "**전체 분석: 멀티모달 RAG 대 텍스트 전용 RAG**\n",
      "\n",
      "여러 테스트 질의에 걸쳐 멀티모달 RAG(텍스트 + 이미지)와 텍스트 전용 RAG의 성능을 비교 분석합니다. 각 접근 방식의 강점과 약점, 멀티모달 RAG가 텍스트 전용보다 뛰어난 성능을 보이는 질의 유형, 이미지 정보 통합의 이점, 멀티모달 접근 방식의 한계 등을 중점적으로 살펴봅니다.\n",
      "\n",
      "**멀티모달 RAG의 이점**\n",
      "\n",
      "1. **향상된 문맥 이해**: 멀티모달 RAG는 텍스트와 이미지 정보를 모두 활용하여 질의의 문맥을 더 잘 이해할 수 있습니다. 이는 특히 질의가 주제에 대한 더 깊은 이해를 요구할 때 더 정확하고 유익한 응답으로 이어질 수 있습니다.\n",
      "2. **강화된 시각적 단서**: 이미지는 텍스트에 명시적으로 언급되지 않은 모호한 질의를 명확히 하거나 추가적인 문맥을 제공하는 시각적 단서를 제공할 수 있습니다. 예를 들어, 질의 1에서 트랜스포머 모델의 이미지는 텍스트 전용 응답에는 없는 질의에 대한 시각적 확인을 제공할 수 있었습니다.\n",
      "3. **검색 정밀도 증가**: 멀티모달 RAG는 더 관련성 높은 텍스트 청크와 이미지 캡션을 검색하여 더 정확하고 정밀한 응답을 생성할 수 있습니다. 질의 1에서 멀티모달 RAG는 5개의 텍스트 청크와 0개의 이미지 캡션을 검색했는데, 이는 명확한 답변을 제공하기에 충분하지 않았을 수 있습니다.\n",
      "\n",
      "**멀티모달 RAG의 단점**\n",
      "\n",
      "1. **복잡성 증가**: 멀티모달 RAG는 더 복잡한 처리 및 검색 메커니즘을 필요로 하므로 계산 비용이 증가하고 시스템 훈련 및 배포가 더 어려워질 수 있습니다.\n",
      "2. **이미지 품질 및 관련성**: 이미지 캡션의 품질과 관련성은 멀티모달 RAG의 성능에 큰 영향을 미칠 수 있습니다. 품질이 낮거나 관련 없는 이미지는 최적이 아닌 응답으로 이어질 수 있습니다.\n",
      "3. **이미지 정보에 대한 과도한 의존**: 이미지 정보가 관련성이 없거나 정확하지 않은 경우 멀티모달 RAG가 너무 많이 의존하여 최적이 아닌 응답을 생성할 수 있습니다.\n",
      "\n",
      "**멀티모달 RAG가 텍스트 전용보다 뛰어난 성능을 보이는 질의 유형**\n",
      "\n",
      "1. **시공간적 질의**: 멀티모달 RAG는 질의 1(트랜스포머(기본 모델)의 BLEU 점수는 무엇인가?)과 같이 시각적 문맥에 대한 더 깊은 이해를 요구하는 시공간적 질의에서 더 나은 성능을 보일 수 있습니다.\n",
      "2. **모호한 질의**: 멀티모달 RAG는 텍스트와 이미지 정보를 모두 활용하여 모호한 질의를 명확히 하여 더 정확하고 유익한 응답을 생성하는 데 도움이 될 수 있습니다.\n",
      "3. **멀티모달 질의**: 멀티모달 RAG는 질의 1과 같이 텍스트와 이미지 정보를 모두 통합해야 하는 멀티모달 질의에서 더 나은 성능을 보일 수 있습니다.\n",
      "\n",
      "**이미지 정보 통합의 구체적인 이점**\n",
      "\n",
      "1. **시각적 확인**: 이미지는 질의에 대한 시각적 확인을 제공하여 모호한 질의를 명확히 하거나 텍스트에 명시적으로 언급되지 않은 추가 컨텍스트를 제공하는 데 도움이 될 수 있습니다.\n",
      "2. **시각적 단서**: 이미지는 모델이 질의의 컨텍스트를 이해하는 데 도움이 되는 시각적 단서를 제공하여 더 정확하고 유익한 응답을 생성할 수 있습니다.\n",
      "3. **멀티모달 융합**: 이미지를 텍스트 정보와 융합하여 질의에 대한 보다 포괄적인 이해를 제공함으로써 더 정확하고 유익한 응답을 생성할 수 있습니다.\n",
      "\n",
      "**전반적인 권장 사항**\n",
      "\n",
      "1. **간단한 질의에는 텍스트 전용 RAG 사용**: 텍스트 전용 RAG는 컨텍스트나 시각적 정보에 대한 깊은 이해가 필요하지 않은 간단한 질의에 충분합니다.\n",
      "2. **복잡한 질의에는 멀티모달 RAG 사용**: 컨텍스트, 시각적 정보 또는 둘 다에 대한 더 깊은 이해가 필요한 복잡한 질의에는 멀티모달 RAG를 권장합니다.\n",
      "3. **모호한 질의에는 멀티모달 RAG 사용**: 멀티모달 RAG는 텍스트와 이미지 정보를 모두 활용하여 모호한 질의를 명확히 하여 더 정확하고 유익한 응답을 생성하는 데 도움이 될 수 있습니다.\n",
      "\n",
      "결론적으로 멀티모달 RAG는 특정 유형의 질의, 특히 컨텍스트, 시각적 정보 또는 둘 다에 대한 더 깊은 이해가 필요한 질의에서 텍스트 전용 RAG보다 뛰어난 성능을 보일 수 있습니다. 그러나 멀티모달 접근 방식에는 복잡성 증가 및 이미지 정보에 대한 과도한 의존 가능성과 같은 한계도 있습니다. 접근 방식 선택은 특정 사용 사례와 처리 중인 질의 유형에 따라 달라집니다.\n"
     ]
    }
   ],
   "source": [
    "# PDF 문서 경로\n",
    "pdf_path = \"data/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# 텍스트 및 시각적 콘텐츠를 모두 대상으로 하는 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"트랜스포머(기본 모델)의 BLEU 점수는 무엇인가?\",\n",
    "]\n",
    "\n",
    "# 평가를 위한 선택적 참조 답변\n",
    "reference_answers = [\n",
    "    \"트랜스포머(기본 모델)는 WMT 2014 영어-독일어 번역 작업에서 27.3점, WMT 2014 영어-프랑스어 번역 작업에서 38.1점의 BLEU 점수를 달성합니다.\",\n",
    "]\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# 전체 분석 출력\n",
    "print(\"\\n=== 전체 분석 ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 15_multimodel_rag.ipynb]
