{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Self-RAG: RAG의 동적 접근 방식\n",
    "\n",
    "이 노트북에서는 검색된 정보를 언제 어떻게 사용할지 동적으로 결정하는 고급 RAG 시스템인 Self-RAG를 구현합니다. 기존 RAG 접근 방식과 달리 Self-RAG는 검색 및 생성 프로세스 전반에 걸쳐 \"성찰 지점(reflection points)\"을 도입하여 더 높은 품질과 신뢰성을 갖는 응답을 생성합니다.\n",
    "Self-RAG는 단순히 정보를 검색하고 생성하는 것을 넘어, 스스로 판단하고 조절하는 능력을 갖춘 RAG 모델입니다. 예를 들어, 질문의 난이도나 유형에 따라 검색이 필요한지 여부를 결정하고, 검색된 정보가 답변 생성에 충분한지, 또는 생성된 답변이 검색된 정보에 의해 잘 뒷받침되는지 등을 스스로 평가합니다. 이러한 \"자기 성찰\" 과정을 통해 불필요한 검색을 줄이고, 답변의 질을 높이며, 환각(hallucination) 현상을 줄일 수 있습니다.\n",
    "\n",
    "## Self-RAG의 주요 구성 요소\n",
    "\n",
    "1. **검색 결정 (Retrieval Decision)**: 주어진 질의에 대해 검색이 필요한지 여부를 결정합니다.\n",
    "2. **문서 검색 (Document Retrieval)**: 필요할 때 잠재적으로 관련된 문서를 가져옵니다.  \n",
    "3. **관련성 평가 (Relevance Evaluation)**: 검색된 각 문서가 얼마나 관련성이 있는지 평가합니다.\n",
    "4. **응답 생성 (Response Generation)**: 관련 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "5. **근거 평가 (Support Assessment)**: 응답이 컨텍스트에 의해 적절하게 뒷받침되는지 평가합니다.\n",
    "6. **유용성 평가 (Utility Evaluation)**: 생성된 응답의 전반적인 유용성을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import re # 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현\n",
    "문서 청크와 해당 임베딩을 관리하기 위한 기본적인 벡터 저장소를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []  # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 변환하여 벡터 리스트에 추가\n",
    "        self.texts.append(text)  # 원본 텍스트를 텍스트 리스트에 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터를 메타데이터 리스트에 추가 (None이면 빈 딕셔너리 사용)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 질의 임베딩 벡터.\n",
    "        k (int): 반환할 결과 수.\n",
    "        filter_func (callable, optional): 결과를 필터링하는 함수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트 및 메타데이터와 함께 상위 k개의 가장 유사한 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 제공된 경우 필터 적용\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 결과 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    # 문자열 및 리스트 입력을 모두 처리하기 위해 input_text가 항상 리스트가 되도록 보장\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 단일 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 그렇지 않으면 텍스트 목록에 대한 모든 임베딩 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Self-RAG를 위해 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "        chunk_size (int): 각 청크의 문자 크기.\n",
    "        chunk_overlap (int): 청크 간 문자 중첩.\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: 문서 청크와 해당 임베딩을 포함하는 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 추출된 텍스트 청킹\n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    # 각 청크에 대한 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 해당 임베딩을 벡터 저장소에 추가\n",
    "    for i, (chunk_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG 구성 요소\n",
    "### 1. 검색 결정 (Retrieval Decision)\n",
    "LLM을 사용하여 현재 질문에 답변하기 위해 외부 정보 검색이 필요한지 판단합니다. 간단한 인사나 일반 상식 질문에는 검색이 불필요할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_if_retrieval_needed(query):\n",
    "    \"\"\"\n",
    "    주어진 질의에 대해 검색이 필요한지 여부를 결정합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        \n",
    "    Returns:\n",
    "        bool: 검색이 필요하면 True, 그렇지 않으면 False\n",
    "    \"\"\"\n",
    "    # AI가 검색 필요 여부를 결정하는 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 질의에 답변하기 위해 검색이 필요한지 여부를 결정하는 AI 어시스턴트입니다.\n",
    "    사실적 질문, 특정 정보 요청 또는 사건, 사람, 개념에 대한 질문에는 \"예\"라고 답변하십시오.\n",
    "    의견, 가상 시나리오 또는 일반 상식이 포함된 간단한 질의에는 \"아니요\"라고 답변하십시오.\n",
    "    \"예\" 또는 \"아니요\"로만 답변하십시오.\"\"\"\n",
    "\n",
    "    # 질의를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"질의: {query}\\n\\n이 질의에 정확하게 답변하기 위해 검색이 필요합니까?\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 결정론적 결과를 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 답변 추출 및 소문자로 변환\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    # 답변에 \"예\"가 포함되어 있으면 True 반환, 그렇지 않으면 False 반환\n",
    "    return \"예\" in answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 관련성 평가 (Relevance Evaluation)\n",
    "검색된 각 문서(청크)가 현재 질문에 얼마나 관련성이 있는지 LLM을 통해 평가합니다. 관련 없는 정보는 답변 생성 과정에서 제외될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query, context):\n",
    "    \"\"\"\n",
    "    컨텍스트의 질의 관련성을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 컨텍스트 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 'relevant' 또는 'irrelevant'\n",
    "    \"\"\"\n",
    "    # AI가 문서 관련성을 결정하는 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 문서가 질의와 관련이 있는지 결정하는 AI 어시스턴트입니다.\n",
    "    문서에 질의에 답변하는 데 도움이 되는 정보가 포함되어 있는지 고려하십시오.\n",
    "    \"Relevant\" 또는 \"Irrelevant\"로만 답변하십시오.\"\"\"\n",
    "\n",
    "    # 토큰 제한을 초과하지 않도록 컨텍스트가 너무 길면 자르기\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [잘림]\"\n",
    "\n",
    "    # 질의와 문서 내용을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "    문서 내용:\n",
    "    {context}\n",
    "\n",
    "    이 문서가 질의와 관련이 있습니까? \"Relevant\" 또는 \"Irrelevant\"로만 답변하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 답변 추출 및 소문자로 변환\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # 관련성 평가 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 근거 평가 (Support Assessment)\n",
    "생성된 답변이 제공된 컨텍스트(검색된 정보)에 의해 얼마나 잘 뒷받침되는지 LLM을 통해 평가합니다. 답변이 컨텍스트에 없는 내용을 지어내거나(환각) 컨텍스트와 모순되는 내용이 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_support(response, context):\n",
    "    \"\"\"\n",
    "    응답이 컨텍스트에 의해 얼마나 잘 뒷받침되는지 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        response (str): 생성된 응답\n",
    "        context (str): 컨텍스트 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 'fully supported', 'partially supported' 또는 'no support'\n",
    "    \"\"\"\n",
    "    # AI가 근거를 평가하는 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 응답이 주어진 컨텍스트에 의해 뒷받침되는지 결정하는 AI 어시스턴트입니다.\n",
    "    응답의 사실, 주장 및 정보가 컨텍스트에 의해 뒷받침되는지 평가하십시오.\n",
    "    다음 세 가지 옵션 중 하나로만 답변하십시오:\n",
    "    - \"Fully supported\": 응답의 모든 정보가 컨텍스트에 의해 직접적으로 뒷받침됩니다.\n",
    "    - \"Partially supported\": 응답의 일부 정보는 컨텍스트에 의해 뒷받침되지만 일부는 그렇지 않습니다.\n",
    "    - \"No support\": 응답에 컨텍스트에서 찾을 수 없거나 컨텍스트와 모순되는 중요한 정보가 포함되어 있습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 토큰 제한을 초과하지 않도록 컨텍스트가 너무 길면 자르기\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [잘림]\"\n",
    "\n",
    "    # 평가할 컨텍스트와 응답을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"컨텍스트:\n",
    "    {context}\n",
    "\n",
    "    응답:\n",
    "    {response}\n",
    "\n",
    "    이 응답이 컨텍스트에 의해 얼마나 잘 뒷받침됩니까? \"Fully supported\", \"Partially supported\" 또는 \"No support\"로만 답변하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    api_response = client.chat.completions.create( # response 변수명 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 답변 추출 및 소문자로 변환\n",
    "    answer = api_response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # 근거 평가 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 유용성 평가 (Utility Evaluation)\n",
    "생성된 답변이 사용자 질문에 대해 얼마나 유용한지 LLM을 통해 평가합니다. 답변의 완전성, 정확성, 명확성 등을 종합적으로 고려합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_utility(query, response):\n",
    "    \"\"\"\n",
    "    질의에 대한 응답의 유용성을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        response (str): 생성된 응답\n",
    "        \n",
    "    Returns:\n",
    "        int: 1에서 5까지의 유용성 평가\n",
    "    \"\"\"\n",
    "    # AI가 응답 유용성을 평가하는 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 질의에 대한 응답의 유용성을 평가하는 AI 어시스턴트입니다.\n",
    "    응답이 질의에 얼마나 잘 답변하는지, 완전성, 정확성 및 유용성을 고려하십시오.\n",
    "    유용성을 1에서 5까지의 척도로 평가하십시오. 여기서:\n",
    "    - 1: 전혀 유용하지 않음\n",
    "    - 2: 약간 유용함\n",
    "    - 3: 보통으로 유용함\n",
    "    - 4: 매우 유용함\n",
    "    - 5: 매우 유용함\n",
    "    1에서 5까지의 단일 숫자만 답변하십시오.\"\"\"\n",
    "\n",
    "    # 평가할 질의와 응답을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "    응답:\n",
    "    {response}\n",
    "\n",
    "    이 응답의 유용성을 1에서 5까지의 척도로 평가하십시오:\"\"\"\n",
    "    \n",
    "    # OpenAI 클라이언트를 사용하여 유용성 평가 생성\n",
    "    api_response = client.chat.completions.create( # response 변수명 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 모델 응답에서 평가 추출\n",
    "    rating_text = api_response.choices[0].message.content.strip() # rating을 rating_text로 변경\n",
    "    \n",
    "    # 평가에서 숫자만 추출\n",
    "    rating_match = re.search(r'[1-5]', rating_text)\n",
    "    if rating_match:\n",
    "        return int(rating_match.group())  # 추출된 평가를 정수로 반환\n",
    "    \n",
    "    return 3  # 파싱 실패 시 중간 평가 기본값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context=None):\n",
    "    \"\"\"\n",
    "    질의와 선택적 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str, optional): 컨텍스트 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # AI가 유용한 응답을 생성하는 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 도움이 되는 AI 어시스턴트입니다. 질의에 대해 명확하고, 정확하며, 유익한 응답을 제공하십시오.\"\"\"\n",
    "    \n",
    "    # 컨텍스트 제공 여부에 따라 사용자 프롬프트 생성\n",
    "    if context:\n",
    "        user_prompt = f\"\"\"컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질의: {query}\n",
    "\n",
    "        제공된 컨텍스트를 기반으로 질의에 답변하십시오.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"질의: {query}\n",
    "        \n",
    "        최선을 다해 질의에 답변하십시오.\"\"\"\n",
    "    \n",
    "    # OpenAI 클라이언트를 사용하여 응답 생성\n",
    "    api_response = client.chat.completions.create( # response 변수명 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2 # 약간의 창의성을 허용\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 텍스트 반환\n",
    "    return api_response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 Self-RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    전체 Self-RAG 파이프라인을 구현합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 문서 청크를 포함하는 벡터 저장소\n",
    "        top_k (int): 초기에 검색할 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        dict: 질의, 응답 및 Self-RAG 프로세스의 메트릭을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== '{query}' 질의에 대한 Self-RAG 시작 ===\\n\")\n",
    "    \n",
    "    # 1단계: 검색 필요 여부 결정\n",
    "    print(\"1단계: 검색 필요 여부 결정 중...\")\n",
    "    retrieval_needed = determine_if_retrieval_needed(query)\n",
    "    print(f\"검색 필요: {retrieval_needed}\")\n",
    "    \n",
    "    # Self-RAG 프로세스를 추적하기 위한 메트릭 초기화\n",
    "    metrics = {\n",
    "        \"retrieval_needed\": retrieval_needed,\n",
    "        \"documents_retrieved\": 0,\n",
    "        \"relevant_documents\": 0,\n",
    "        \"response_support_ratings\": [],\n",
    "        \"utility_ratings\": []\n",
    "    }\n",
    "    \n",
    "    best_response_text = None # best_response를 best_response_text로 변경\n",
    "    best_score = -1\n",
    "    \n",
    "    if retrieval_needed:\n",
    "        # 2단계: 문서 검색\n",
    "        print(\"\\n2단계: 관련 문서 검색 중...\")\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        metrics[\"documents_retrieved\"] = len(results)\n",
    "        print(f\"{len(results)}개의 문서 검색됨\")\n",
    "        \n",
    "        # 3단계: 각 문서의 관련성 평가\n",
    "        print(\"\\n3단계: 문서 관련성 평가 중...\")\n",
    "        relevant_contexts = []\n",
    "        \n",
    "        for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "            context_text = result_item[\"text\"] # context를 context_text로 변경\n",
    "            relevance_eval = evaluate_relevance(query, context_text) # relevance를 relevance_eval로 변경\n",
    "            print(f\"문서 {i+1} 관련성: {relevance_eval}\")\n",
    "            \n",
    "            if relevance_eval == \"relevant\":\n",
    "                relevant_contexts.append(context_text)\n",
    "        \n",
    "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
    "        print(f\"{len(relevant_contexts)}개의 관련 문서 찾음\")\n",
    "        \n",
    "        if relevant_contexts:\n",
    "            # 4단계: 각 관련 컨텍스트 처리\n",
    "            print(\"\\n4단계: 관련 컨텍스트 처리 중...\")\n",
    "            for i, context_text in enumerate(relevant_contexts): # context를 context_text로 변경\n",
    "                print(f\"\\n컨텍스트 {i+1}/{len(relevant_contexts)} 처리 중...\")\n",
    "                \n",
    "                # 컨텍스트를 기반으로 응답 생성\n",
    "                print(\"응답 생성 중...\")\n",
    "                current_response = generate_response(query, context_text) # response를 current_response로 변경\n",
    "                \n",
    "                # 응답이 컨텍스트에 의해 얼마나 잘 뒷받침되는지 평가\n",
    "                print(\"근거 평가 중...\")\n",
    "                support_rating = assess_support(current_response, context_text)\n",
    "                print(f\"근거 평가: {support_rating}\")\n",
    "                metrics[\"response_support_ratings\"].append(support_rating)\n",
    "                \n",
    "                # 응답의 유용성 평가\n",
    "                print(\"유용성 평가 중...\")\n",
    "                utility_rating = rate_utility(query, current_response)\n",
    "                print(f\"유용성 평가: {utility_rating}/5\")\n",
    "                metrics[\"utility_ratings\"].append(utility_rating)\n",
    "                \n",
    "                # 전체 점수 계산 (더 나은 근거와 유용성에 대해 더 높음)\n",
    "                support_score_map = { # support_score를 support_score_map으로 변경\n",
    "                    \"fully supported\": 3, \n",
    "                    \"partially supported\": 1, \n",
    "                    \"no support\": 0\n",
    "                }\n",
    "                current_support_score = support_score_map.get(support_rating, 0) # support_score를 current_support_score로 변경\n",
    "                \n",
    "                overall_score = current_support_score * 5 + utility_rating\n",
    "                print(f\"전체 점수: {overall_score}\")\n",
    "                \n",
    "                # 최상의 응답 추적\n",
    "                if overall_score > best_score:\n",
    "                    best_response_text = current_response\n",
    "                    best_score = overall_score\n",
    "                    print(\"새로운 최상의 응답 찾음!\")\n",
    "        \n",
    "        # 관련 컨텍스트를 찾지 못했거나 모든 응답 점수가 낮은 경우\n",
    "        if not relevant_contexts or best_score <= 0: # best_response가 아닌 best_score로 조건 변경\n",
    "            print(\"\\n적절한 컨텍스트를 찾지 못했거나 응답 점수가 낮아 검색 없이 생성 중...\")\n",
    "            best_response_text = generate_response(query)\n",
    "    else:\n",
    "        # 검색 필요 없음, 직접 생성\n",
    "        print(\"\\n검색 필요 없음, 직접 응답 생성 중...\")\n",
    "        best_response_text = generate_response(query)\n",
    "    \n",
    "    # 최종 메트릭\n",
    "    metrics[\"best_score\"] = best_score\n",
    "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0 and bool(relevant_contexts) # best_response가 아닌 best_score와 relevant_contexts 존재 여부로 조건 변경\n",
    "    \n",
    "    print(\"\\n=== Self-RAG 완료 ===\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": best_response_text,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 Self-RAG 시스템 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_rag_example():\n",
    "    \"\"\"\n",
    "    예제를 사용하여 전체 Self-RAG 시스템을 시연합니다.\n",
    "    \"\"\"\n",
    "    # 문서 처리\n",
    "    pdf_path = \"data/AI_Information.pdf\"  # PDF 문서 경로\n",
    "    print(f\"문서 처리 중: {pdf_path}\")\n",
    "    vector_store = process_document(pdf_path)  # 문서를 처리하고 벡터 저장소 생성\n",
    "    \n",
    "    # 예제 1: 검색이 필요할 가능성이 높은 질의\n",
    "    query1 = \"AI 개발의 주요 윤리적 문제는 무엇입니까?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 1: {query1}\")\n",
    "    result1 = self_rag(query1, vector_store)  # 첫 번째 질의에 대해 Self-RAG 실행\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result1[\"response\"])  # 첫 번째 질의에 대한 최종 응답 출력\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result1[\"metrics\"], indent=2, ensure_ascii=False))  # 첫 번째 질의에 대한 메트릭 출력 (ensure_ascii=False 추가)\n",
    "    \n",
    "    # 예제 2: 검색이 필요하지 않을 가능성이 높은 질의\n",
    "    query2 = \"인공 지능에 대한 짧은 시를 써주세요.\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 2: {query2}\")\n",
    "    result2 = self_rag(query2, vector_store)  # 두 번째 질의에 대해 Self-RAG 실행\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result2[\"response\"])  # 두 번째 질의에 대한 최종 응답 출력\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result2[\"metrics\"], indent=2, ensure_ascii=False))  # 두 번째 질의에 대한 메트릭 출력 (ensure_ascii=False 추가)\n",
    "    \n",
    "    # 예제 3: 문서와 일부 관련이 있지만 추가 지식이 필요한 질의\n",
    "    query3 = \"AI가 개발 도상국의 경제 성장에 어떤 영향을 미칠 수 있습니까?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 3: {query3}\")\n",
    "    result3 = self_rag(query3, vector_store)  # 세 번째 질의에 대해 Self-RAG 실행\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result3[\"response\"])  # 세 번째 질의에 대한 최종 응답 출력\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result3[\"metrics\"], indent=2, ensure_ascii=False))  # 세 번째 질의에 대한 메트릭 출력 (ensure_ascii=False 추가)\n",
    "    \n",
    "    return {\n",
    "        \"example1\": result1,\n",
    "        \"example2\": result2,\n",
    "        \"example3\": result3\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG와 기존 RAG 비교 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    비교를 위해 기존 RAG 접근 방식을 구현합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 문서 청크를 포함하는 벡터 저장소\n",
    "        top_k (int): 검색할 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== '{query}' 질의에 대한 기존 RAG 실행 중 ===\\n\")\n",
    "    \n",
    "    # 문서 검색\n",
    "    print(\"문서 검색 중...\")\n",
    "    query_embedding = create_embeddings(query)  # 질의에 대한 임베딩 생성\n",
    "    results = vector_store.similarity_search(query_embedding, k=top_k)  # 유사한 문서 검색\n",
    "    print(f\"{len(results)}개의 문서 검색됨\")\n",
    "    \n",
    "    # 검색된 문서에서 컨텍스트 결합\n",
    "    contexts = [result_item[\"text\"] for result_item in results]  # result를 result_item으로 변경하여 결과에서 텍스트 추출\n",
    "    combined_context = \"\\n\\n\".join(contexts)  # 텍스트를 단일 컨텍스트로 결합\n",
    "    \n",
    "    # 결합된 컨텍스트를 사용하여 응답 생성\n",
    "    print(\"응답 생성 중...\")\n",
    "    response_text = generate_response(query, combined_context)  # 결합된 컨텍스트를 기반으로 응답 생성 (response를 response_text로 변경)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Self-RAG와 기존 RAG를 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        test_queries (List[str]): 테스트 질의 목록\n",
    "        reference_answers (List[str], optional): 평가를 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        dict: 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"=== RAG 접근 방식 평가 ===\")\n",
    "    \n",
    "    # 벡터 저장소를 생성하기 위해 문서 처리\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    evaluation_results_list = [] # results를 evaluation_results_list로 변경\n",
    "    \n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n질의 {i+1} 처리 중: {query_text}\")\n",
    "        \n",
    "        # Self-RAG 실행\n",
    "        self_rag_result = self_rag(query_text, vector_store)  # Self-RAG에서 응답 가져오기\n",
    "        self_rag_response_text = self_rag_result[\"response\"] # self_rag_response를 self_rag_response_text로 변경\n",
    "        \n",
    "        # 기존 RAG 실행\n",
    "        trad_rag_response_text = traditional_rag(query_text, vector_store)  # 기존 RAG에서 응답 가져오기 (trad_rag_response를 trad_rag_response_text로 변경)\n",
    "        \n",
    "        # 참조 답변을 사용할 수 있는 경우 결과 비교\n",
    "        reference_text = reference_answers[i] if reference_answers and i < len(reference_answers) else None # reference를 reference_text로 변경\n",
    "        comparison_analysis = compare_responses(query_text, self_rag_response_text, trad_rag_response_text, reference_text)  # 응답 비교 (comparison을 comparison_analysis로 변경)\n",
    "        \n",
    "        evaluation_results_list.append({\n",
    "            \"query\": query_text,\n",
    "            \"self_rag_response\": self_rag_response_text,\n",
    "            \"traditional_rag_response\": trad_rag_response_text,\n",
    "            \"reference_answer\": reference_text,\n",
    "            \"comparison\": comparison_analysis,\n",
    "            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n",
    "        })\n",
    "    \n",
    "    # 전체 분석 생성\n",
    "    overall_analysis_text = generate_overall_analysis(evaluation_results_list) # overall_analysis를 overall_analysis_text로 변경\n",
    "    \n",
    "    return {\n",
    "        \"results\": evaluation_results_list,\n",
    "        \"overall_analysis\": overall_analysis_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, self_rag_response, trad_rag_response, reference=None):\n",
    "    \"\"\"\n",
    "    Self-RAG와 기존 RAG의 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        self_rag_response (str): Self-RAG의 응답\n",
    "        trad_rag_response (str): 기존 RAG의 응답\n",
    "        reference (str, optional): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 비교 분석\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다. 다음 두 가지 RAG 접근 방식의 응답을 비교하는 것이 당신의 임무입니다:\n",
    "1. Self-RAG: 검색이 필요한지 여부를 결정하고 정보 관련성 및 응답 품질을 평가하는 동적 접근 방식\n",
    "2. 기존 RAG: 항상 문서를 검색하고 이를 사용하여 응답 생성\n",
    "\n",
    "다음을 기준으로 응답을 비교하십시오:\n",
    "- 질의 관련성\n",
    "- 사실적 정확성\n",
    "- 완전성 및 유익성\n",
    "- 간결성 및 집중도\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "Self-RAG 응답:\n",
    "{self_rag_response}\n",
    "\n",
    "기존 RAG 응답:\n",
    "{trad_rag_response}\n",
    "\"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "참조 답변 (사실 확인용):\n",
    "{reference}\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "이러한 응답을 비교하고 어떤 것이 더 나은지, 그 이유는 무엇인지 설명하십시오.\n",
    "정확성, 관련성, 완전성 및 품질에 중점을 두십시오.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 평가를 위해 더 강력한 모델 사용 고려\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Self-RAG 대 기존 RAG에 대한 전체 분석을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): evaluate_rag_approaches의 결과\n",
    "        \n",
    "    Returns:\n",
    "        str: 전체 분석\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다. 여러 테스트 질의를 기반으로 Self-RAG와 기존 RAG를 비교하는 전체 분석을 제공하는 것이 당신의 임무입니다.\n",
    "\n",
    "분석에 다음 사항을 집중적으로 다루십시오:\n",
    "1. Self-RAG가 더 나은 성능을 보이는 경우와 그 이유\n",
    "2. 기존 RAG가 더 나은 성능을 보이는 경우와 그 이유\n",
    "3. Self-RAG의 동적 검색 결정의 영향\n",
    "4. Self-RAG의 관련성 및 근거 평가의 가치\n",
    "5. 다양한 유형의 질의에 어떤 접근 방식을 사용할지에 대한 전반적인 권장 사항\"\"\"\n",
    "\n",
    "    # 개별 비교 요약 준비\n",
    "    comparisons_summary = \"\"\n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        comparisons_summary += f\"질의 {i+1}: {result_item['query']}\\n\"\n",
    "        comparisons_summary += f\"Self-RAG 메트릭: 검색 필요: {result_item['self_rag_metrics']['retrieval_needed']}, \"\n",
    "        comparisons_summary += f\"관련 문서: {result_item['self_rag_metrics']['relevant_documents']}/{result_item['self_rag_metrics']['documents_retrieved']}\\n\"\n",
    "        comparisons_summary += f\"비교 요약: {result_item['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # 이전에 user_prompt가 정의되지 않았으므로, 여기서 올바르게 정의합니다.\n",
    "    user_prompt = f\"\"\"다음 {len(results)}개의 테스트 질의에 대한 비교 결과를 바탕으로 Self-RAG와 기존 RAG에 대한 전체 분석을 제공하십시오:\n",
    "\n",
    "{comparisons_summary}\n",
    "\n",
    "포괄적인 분석을 제공하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG 시스템 평가\n",
    "\n",
    "마지막 단계는 Self-RAG 시스템을 기존 RAG 접근 방식과 비교하여 평가하는 것입니다. 두 시스템에서 생성된 응답의 품질을 비교하고 다양한 시나리오에서 Self-RAG의 성능을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG 접근 방식 평가 ===\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청킹 중...\n",
      "생성된 텍스트 청크 수: 42\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크 추가됨\n",
      "\n",
      "질의 1 처리 중: AI 개발의 주요 윤리적 문제는 무엇입니까?\n",
      "\n",
      "=== 'AI 개발의 주요 윤리적 문제는 무엇입니까?' 질의에 대한 Self-RAG 시작 ===\n",
      "\n",
      "1단계: 검색 필요 여부 결정 중...\n",
      "검색 필요: True\n",
      "\n",
      "2단계: 관련 문서 검색 중...\n",
      "3개의 문서 검색됨\n",
      "\n",
      "3단계: 문서 관련성 평가 중...\n",
      "문서 1 관련성: relevant\n",
      "문서 2 관련성: relevant\n",
      "문서 3 관련성: relevant\n",
      "3개의 관련 문서 찾음\n",
      "\n",
      "4단계: 관련 컨텍스트 처리 중...\n",
      "\n",
      "컨텍스트 1/3 처리 중...\n",
      "응답 생성 중...\n",
      "근거 평가 중...\n",
      "근거 평가: fully supported\n",
      "유용성 평가 중...\n",
      "유용성 평가: 4/5\n",
      "전체 점수: 19\n",
      "새로운 최상의 응답 찾음!\n",
      "\n",
      "컨텍스트 2/3 처리 중...\n",
      "응답 생성 중...\n",
      "근거 평가 중...\n",
      "근거 평가: partially supported\n",
      "유용성 평가 중...\n",
      "유용성 평가: 4/5\n",
      "전체 점수: 9\n",
      "\n",
      "컨텍스트 3/3 처리 중...\n",
      "응답 생성 중...\n",
      "근거 평가 중...\n",
      "근거 평가: fully supported\n",
      "유용성 평가 중...\n",
      "유용성 평가: 5/5\n",
      "전체 점수: 20\n",
      "새로운 최상의 응답 찾음!\n",
      "\n",
      "=== Self-RAG 완료 ===\n",
      "\n",
      "=== 'AI 개발의 주요 윤리적 문제는 무엇입니까?' 질의에 대한 기존 RAG 실행 중 ===\n",
      "\n",
      "문서 검색 중...\n",
      "3개의 문서 검색됨\n",
      "응답 생성 중...\n",
      "\n",
      "=== 전체 분석 ===\n",
      "\n",
      "**전체 분석: Self-RAG 대 기존 RAG**\n",
      "\n",
      "\"AI 개발의 주요 윤리적 문제는 무엇입니까?\" 테스트 질의에 대한 비교 결과를 바탕으로 Self-RAG 및 기존 RAG 시스템 모두에 대한 포괄적인 분석을 제공하겠습니다.\n",
      "\n",
      "**Self-RAG가 더 나은 성능을 보이는 경우:**\n",
      "\n",
      "1. **동적 검색 결정**: Self-RAG는 질의 컨텍스트와 사용자 피드백을 기반으로 검색 결정을 동적으로 조정할 수 있으므로 여러 관련 문서가 있는 복잡한 질의에서 더 나은 결과를 얻을 수 있습니다. 질의 1의 경우 Self-RAG의 검색 필요는 True였으며, 이는 질의에 가장 관련성이 높은 문서를 식별할 수 있었음을 나타냅니다. 이는 이 시나리오에서 Self-RAG의 동적 검색 결정이 효과적이었음을 시사합니다.\n",
      "2. **관련성 및 근거 평가**: Self-RAG의 관련성 및 근거 평가는 더 정확하고 유익한 응답을 생성할 수 있습니다. 이 경우 Self-RAG의 관련 문서는 3/3이었으며, 이는 질의에 가장 관련성이 높은 문서를 식별할 수 있었음을 나타냅니다. 이는 이 시나리오에서 Self-RAG의 관련성 및 근거 평가가 효과적이었음을 시사합니다.\n",
      "\n",
      "**기존 RAG가 더 나은 성능을 보이는 경우:**\n",
      "\n",
      "1. **간단한 질의**: 기존 RAG는 단일 관련 문서가 있는 간단한 질의에서 더 나은 성능을 보일 수 있습니다. 이 경우 \"AI 개발의 주요 윤리적 문제는 무엇입니까?\"라는 질의는 기존 RAG가 효과적으로 처리하기에는 너무 복잡했을 수 있습니다.\n",
      "2. **사전 정의된 순위**: 기존 RAG의 사전 정의된 순위는 문서 순위가 중요하지 않은 시나리오에서 더 효과적일 수 있습니다. 이 경우 \"AI 개발의 주요 윤리적 문제는 무엇입니까?\"라는 질의는 높은 순위의 응답이 필요하지 않았을 수 있습니다.\n",
      "\n",
      "**Self-RAG의 동적 검색 결정의 영향:**\n",
      "\n",
      "Self-RAG의 동적 검색 결정은 여러 관련 문서가 있는 복잡한 질의에서 더 나은 결과를 얻을 수 있습니다. 그러나 이는 질의 컨텍스트와 사용자 피드백에 따라 문서의 과도한 검색 또는 부족한 검색으로 이어질 수도 있습니다. 이를 완화하려면 가장 관련성이 높은 문서가 검색되도록 Self-RAG의 동적 검색 결정을 신중하게 조정해야 합니다.\n",
      "\n",
      "**Self-RAG의 관련성 및 근거 평가의 가치:**\n",
      "\n",
      "Self-RAG의 관련성 및 근거 평가는 검색된 문서가 정확하고 유익하도록 보장하는 데 중요합니다. 각 문서의 관련성과 근거를 평가함으로써 Self-RAG는 더 정확하고 유익한 응답을 제공할 수 있습니다. 그러나 이 평가는 가장 관련성이 높은 문서가 검색되도록 신중하게 조정해야 합니다.\n",
      "\n",
      "**전반적인 권장 사항:**\n",
      "\n",
      "1. **복잡한 질의에는 Self-RAG 사용**: Self-RAG의 동적 검색 결정과 관련성 및 근거 평가는 여러 관련 문서가 있는 복잡한 질의에 더 적합합니다.\n",
      "2. **간단한 질의에는 기존 RAG 사용**: 기존 RAG의 사전 정의된 순위와 단순성은 단일 관련 문서가 있는 간단한 질의에 더 적합합니다.\n",
      "3. **Self-RAG의 동적 검색 결정 조정**: 가장 관련성이 높은 문서가 검색되도록 Self-RAG의 동적 검색 결정을 신중하게 조정해야 합니다.\n",
      "4. **Self-RAG에서 관련성 및 근거 평가**: Self-RAG의 관련성 및 근거 평가는 검색된 문서가 정확하고 유익하도록 보장하는 데 중요합니다.\n",
      "\n",
      "결론적으로 Self-RAG와 기존 RAG는 서로 다른 강점과 약점을 가지고 있으며, 어떤 시스템을 사용할지는 질의 유형과 원하는 결과에 따라 달라집니다. 각 시스템의 강점과 약점을 이해함으로써 다양한 시나리오에서 어떤 시스템을 사용할지에 대해 정보에 입각한 결정을 내릴 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# AI 정보 문서 경로\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Self-RAG의 적응형 검색을 테스트하기 위해 다양한 질의 유형을 포함하는 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"AI 개발의 주요 윤리적 문제는 무엇입니까?\",        # 문서 중심 질의\n",
    "    # \"설명 가능한 AI는 AI 시스템에 대한 신뢰를 어떻게 향상시킵니까?\",         # 문서 중심 질의\n",
    "    # \"인공 지능에 대한 시를 써주세요\",                   # 창의적 질의, 검색 불필요\n",
    "    # \"초지능 AI가 인간을 쓸모없게 만들까요?\"          # 추측성 질의, 부분적 검색 필요\n",
    "]\n",
    "\n",
    "# 보다 객관적인 평가를 위한 참조 답변\n",
    "reference_answers = [\n",
    "    \"AI 개발의 주요 윤리적 문제에는 편향과 공정성, 개인 정보 보호, 투명성, 책임성, 안전성, 그리고 오용 또는 유해한 적용 가능성이 포함됩니다.\",\n",
    "    # \"설명 가능한 AI는 AI 의사 결정 과정을 사용자에게 투명하고 이해하기 쉽게 만들어 신뢰를 향상시키며, 공정성을 확인하고 잠재적 편향을 식별하며 AI의 한계를 더 잘 이해하도록 돕습니다.\",\n",
    "    # \"인공 지능에 대한 양질의 시는 AI의 능력, 한계, 인류와의 관계, 잠재적 미래 또는 의식과 지능에 대한 철학적 질문과 같은 주제를 창의적으로 탐구해야 합니다.\",\n",
    "    # \"초지능 AI가 인간의 적절성에 미치는 영향에 대한 견해는 매우 다양합니다. 일부 전문가는 AI가 여러 영역에서 인간의 능력을 능가하면 경제적 대체나 인간 주체성 상실로 이어질 수 있는 잠재적 위험을 경고합니다. 다른 전문가들은 인간이 보완적 기술, 감성 지능 및 AI의 목적을 정의함으로써 계속 관련성을 유지할 것이라고 주장합니다. 대부분의 전문가는 결과에 관계없이 사려 깊은 거버넌스와 인간 중심 설계가 필수적이라는 데 동의합니다.\"\n",
    "]\n",
    "\n",
    "# Self-RAG와 기존 RAG 접근 방식 비교 평가 실행\n",
    "evaluation_results = evaluate_rag_approaches(\n",
    "    pdf_path=pdf_path,                  # AI 정보가 포함된 원본 문서\n",
    "    test_queries=test_queries,          # AI 관련 테스트 질의 목록\n",
    "    reference_answers=reference_answers  # 평가를 위한 정답\n",
    ")\n",
    "\n",
    "# 전체 비교 분석 출력\n",
    "print(\"\\n=== 전체 분석 ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 13_self_rag.ipynb]

[end of 13_self_rag.ipynb]
