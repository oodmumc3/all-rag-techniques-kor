{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 간단한 RAG(검색 증강 생성) 소개\n",
    "\n",
    "RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 정보 검색(Information Retrieval)과 생성 모델(Generative Models)을 결합한 하이브리드 접근 방식입니다. 이 기술은 외부 지식을 통합하여 언어 모델의 성능을 향상시키며, 이를 통해 답변의 정확성과 사실적 올바름을 개선합니다.\n",
    "\n",
    "간단한 RAG(Simple RAG) 설정에서는 다음 단계를 따릅니다:\n",
    "\n",
    "1. **데이터 수집(Data Ingestion)**: 텍스트 데이터를 로드하고 전처리합니다.\n",
    "2. **청킹(Chunking)**: 검색 성능을 향상시키기 위해 데이터를 더 작은 조각(청크)으로 나눕니다.\n",
    "3. **임베딩 생성(Embedding Creation)**: 임베딩 모델을 사용하여 텍스트 청크를 숫자 표현(벡터)으로 변환합니다.\n",
    "4. **시맨틱 검색(Semantic Search)**: 사용자 쿼리를 기반으로 관련성 높은 청크를 검색합니다.\n",
    "5. **응답 생성(Response Generation)**: 검색된 텍스트를 기반으로 언어 모델을 사용하여 응답을 생성합니다.\n",
    "\n",
    "이 노트북은 간단한 RAG 접근 방식을 구현하고, 모델의 응답을 평가하며, 다양한 개선 방법을 탐구합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하기 위해 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    '''\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    '''\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지를 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트를 청크(Chunk)로 나누기\n",
    "추출된 텍스트가 준비되면, 검색 정확도를 높이기 위해 텍스트를 더 작고 중첩되는 조각(청크)으로 나눕니다. 청킹은 LLM이 한 번에 처리할 수 있는 컨텍스트 양의 한계를 극복하고, 질문과 가장 관련 있는 부분만 모델에 제공하여 더 정확한 답변을 생성하도록 돕습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    '''\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누되, 일정 부분을 중첩시킵니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크로 나눌 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간에 중첩될 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 리스트.\n",
    "    '''\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 스텝으로 텍스트를 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 리스트에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩 생성과 응답 생성을 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출 및 청킹\n",
    "이제 PDF를 로드하고, 텍스트를 추출한 다음, 청크로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# PDF 파일에서 텍스트 추출\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 추출된 텍스트를 1000자 크기의 세그먼트로 나누고, 200자를 중첩시킴\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# 생성된 텍스트 청크의 수 출력\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# 첫 번째 텍스트 청크 출력\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 청크에 대한 임베딩 생성\n",
    "임베딩은 텍스트를 숫자 벡터로 변환하여 효율적인 유사도 검색을 가능하게 합니다. 단어를 지도 위의 좌표처럼 만들어, 의미적으로 유사한 단어나 문장이 벡터 공간에서 서로 가깝게 위치하도록 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    '''\n",
    "    지정된 OpenAI 모델을 사용하여 주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델. 기본값은 \"BAAI/bge-en-icl\"입니다.\n",
    "\n",
    "    Returns:\n",
    "    dict: 임베딩을 포함하는 OpenAI API의 응답.\n",
    "    '''\n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response  # 임베딩을 포함하는 응답 반환\n",
    "\n",
    "# 텍스트 청크에 대한 임베딩 생성\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 검색 수행\n",
    "사용자 쿼리에 가장 관련성 높은 텍스트 청크를 찾기 위해 코사인 유사도를 구현합니다. 코사인 유사도는 두 벡터 간의 방향 유사성을 측정하여, 두 텍스트의 의미적 유사도를 계산하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    '''\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 첫 번째 벡터.\n",
    "    vec2 (np.ndarray): 두 번째 벡터.\n",
    "\n",
    "    Returns:\n",
    "    float: 두 벡터 간의 코사인 유사도.\n",
    "    '''\n",
    "    # 두 벡터의 내적을 각 벡터의 노름(크기)의 곱으로 나눔\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    '''\n",
    "    주어진 쿼리와 임베딩을 사용하여 텍스트 청크에 대한 시맨틱 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 시맨틱 검색을 위한 쿼리.\n",
    "    text_chunks (List[str]): 검색할 텍스트 청크 리스트.\n",
    "    embeddings (List[dict]): 텍스트 청크에 대한 임베딩 리스트.\n",
    "    k (int): 반환할 상위 관련 텍스트 청크의 수. 기본값은 5입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 쿼리를 기반으로 한 상위 k개의 가장 관련성 높은 텍스트 청크 리스트.\n",
    "    '''\n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # 유사도 점수를 저장할 리스트 초기화\n",
    "\n",
    "    # 쿼리 임베딩과 각 텍스트 청크 임베딩 간의 유사도 점수 계산\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # 인덱스와 유사도 점수 추가\n",
    "\n",
    "    # 유사도 점수를 내림차순으로 정렬\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # 가장 유사한 상위 k개 텍스트 청크의 인덱스 가져오기\n",
    "    top_indices = [index for index, _ in similarity_scores[:k}]\n",
    "    # 상위 k개의 가장 관련성 높은 텍스트 청크 반환\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 청크에 대해 쿼리 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy. \n",
      "Privacy and Data Protection \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
      "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
      "and complying with data protection regulations are crucial. \n",
      "Accountability and Responsibility \n",
      "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
      "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
      "developers, deployers, and users of AI systems. \n",
      "Chapter 20: Building Trust in AI \n",
      "Transparency and Explainability \n",
      "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "\n",
      "=====================================\n",
      "Context 2:\n",
      " incidents. \n",
      "Environmental Monitoring \n",
      "AI-powered environmental monitoring systems track air and water quality, detect pollution, and \n",
      "support environmental protection efforts. These systems provide real-time data, identify \n",
      "pollution sources, and inform environmental policies. \n",
      "Chapter 15: The Future of AI Research \n",
      "Advancements in Deep Learning \n",
      "Continued advancements in deep learning are expected to drive further breakthroughs in AI. \n",
      "Research is focused on developing more efficient and interpretable deep learning models, as well \n",
      "as exploring new architectures and training techniques. \n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in \n",
      "XAI focuses on developing methods for explaining AI decisions, enhancing trust, and improving \n",
      "accountability. \n",
      "AI and Neuroscience \n",
      "The intersection of AI and neuroscience is a promising area of research. Understanding the \n",
      "human brain can inspire new AI algorithms and architectures, \n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 첫 번째 쿼리 추출\n",
    "query = data[0]['question']\n",
    "\n",
    "# 시맨틱 검색을 수행하여 쿼리에 대해 가장 관련성 높은 상위 2개 텍스트 청크 찾기\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# 쿼리 출력\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# 상위 2개의 가장 관련성 높은 텍스트 청크 출력\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 응답 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 어시스턴트에 대한 시스템 프롬프트 정의\n",
    "system_prompt = \"당신은 주어진 컨텍스트를 기반으로만 엄격하게 답변하는 AI 어시스턴트입니다. 제공된 컨텍스트에서 직접적으로 답변을 도출할 수 없는 경우, '답변하기에 충분한 정보가 없습니다.'라고 응답하세요.\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    '''\n",
    "    시스템 프롬프트와 사용자 메시지를 기반으로 AI 모델의 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): AI의 행동을 안내하는 시스템 프롬프트.\n",
    "    user_message (str): 사용자의 메시지 또는 쿼리.\n",
    "    model (str): 응답 생성에 사용할 모델. 기본값은 \"meta-llama/Llama-2-7B-chat-hf\"입니다.\n",
    "\n",
    "    Returns:\n",
    "    dict: AI 모델의 응답.\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 상위 청크를 기반으로 사용자 프롬프트 생성\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# AI 응답 생성\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 응답 평가하기\n",
    "AI의 응답을 기대 답변과 비교하고 점수를 매깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the evaluation criteria, I would assign a score of 0.8 to the AI assistant's response.\n",
      "\n",
      "The AI assistant's response is very close to the true response, but there are some minor differences. The true response mentions \"transparency\" and \"accountability\" explicitly, which are not mentioned in the AI assistant's response. However, the overall meaning and content of the response are identical, and the AI assistant's response effectively conveys the importance of Explainable AI in building trust and ensuring fairness in AI systems.\n",
      "\n",
      "Therefore, the score of 0.8 reflects the AI assistant's response being very close to the true response, but not perfectly aligned.\n"
     ]
    }
   ],
   "source": [
    "# 평가 시스템에 대한 시스템 프롬프트 정의\n",
    "evaluate_system_prompt = \"당신은 AI 어시스턴트의 응답을 평가하는 지능적인 평가 시스템입니다. AI 어시스턴트의 응답이 실제 응답과 매우 가까우면 1점을 부여하세요. 응답이 실제 응답과 관련하여 부정확하거나 만족스럽지 않으면 0점을 부여하세요. 응답이 실제 응답과 부분적으로 일치하면 0.5점을 부여하세요.\"\n",
    "\n",
    "# 사용자 쿼리, AI 응답, 실제 응답 및 평가 시스템 프롬프트를 결합하여 평가 프롬프트 생성\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# 평가 시스템 프롬프트와 평가 프롬프트를 사용하여 평가 응답 생성\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 평가 응답 출력\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}