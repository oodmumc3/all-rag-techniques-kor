{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG 시스템을 위한 재정렬(Reranking)\n",
    "\n",
    "이 노트북은 RAG 시스템에서 검색 품질을 향상시키기 위한 재정렬 기술을 구현합니다. 재정렬은 초기 검색 후 두 번째 필터링 단계로 작동하여 응답 생성에 가장 관련성 높은 콘텐츠가 사용되도록 보장합니다.\n",
    "단순히 유사도 점수만으로 검색 결과를 사용하는 대신, 검색된 상위 N개의 결과에 대해 더 정교한 방법으로 관련성을 다시 평가하여 순위를 조정하는 과정입니다. 이를 통해 최종적으로 언어 모델에 전달되는 정보의 질을 높일 수 있습니다.\n",
    "\n",
    "## 재정렬의 주요 개념\n",
    "\n",
    "1. **초기 검색 (Initial Retrieval)**: 기본적인 유사도 검색을 사용한 첫 번째 단계 (정확도는 낮지만 빠름)\n",
    "2. **문서 점수 매기기 (Document Scoring)**: 검색된 각 문서가 질의와 얼마나 관련 있는지 평가\n",
    "3. **순서 변경 (Reordering)**: 관련성 점수를 기준으로 문서 정렬\n",
    "4. **선택 (Selection)**: 응답 생성을 위해 가장 관련성 높은 문서만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re # 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구축\n",
    "재정렬이 검색과 어떻게 통합되는지 보여주기 위해 간단한 벡터 저장소를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []  # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 변환하여 벡터 리스트에 추가\n",
    "        self.texts.append(text)  # 원본 텍스트를 텍스트 리스트에 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터를 메타데이터 리스트에 추가 (None이면 빈 딕셔너리 사용)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 질의 임베딩 벡터.\n",
    "        k (int): 반환할 결과 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트 및 메타데이터와 함께 상위 k개의 가장 유사한 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 질의 벡터와 저장된 벡터 간의 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 해당 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 해당 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 유사 항목 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    지정된 OpenAI 모델을 사용하여 주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    # 문자열 입력을 리스트로 변환하여 문자열 및 리스트 입력 모두 처리\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 그렇지 않으면 모든 임베딩을 벡터 리스트로 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인\n",
    "이제 필요한 함수와 클래스를 정의했으므로 문서 처리 파이프라인을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG를 위해 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "    chunk_size (int): 각 청크의 문자 크기.\n",
    "    chunk_overlap (int): 청크 간 문자 중첩.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: 문서 청크와 해당 임베딩을 포함하는 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 추출된 텍스트 청킹\n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    # 텍스트 청크에 대한 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 간단한 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 해당 임베딩을 벡터 저장소에 추가\n",
    "    for i, (chunk_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM 기반 재정렬 구현\n",
    "OpenAI API를 사용하여 LLM 기반 재정렬 함수를 구현해 보겠습니다.\n",
    "LLM을 사용하여 각 검색된 문서와 원래 질의 간의 관련성을 평가하고 점수를 매깁니다. 이 점수를 기준으로 문서들의 순서를 다시 정렬합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, results, top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    LLM 관련성 점수를 사용하여 검색 결과를 재정렬합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 초기 검색 결과\n",
    "        top_n (int): 재정렬 후 반환할 결과 수\n",
    "        model (str): 점수 매기기에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 재정렬된 결과\n",
    "    \"\"\"\n",
    "    print(f\"{len(results)}개의 문서 재정렬 중...\")  # 재정렬할 문서 수 출력\n",
    "    \n",
    "    scored_results = []  # 점수가 매겨진 결과를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # LLM에 대한 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 검색 질의에 대한 문서 관련성을 평가하는 전문가입니다.\n",
    "당신의 임무는 주어진 질의에 얼마나 잘 답변하는지를 기준으로 0에서 10까지의 척도로 문서를 평가하는 것입니다.\n",
    "\n",
    "지침:\n",
    "- 점수 0-2: 문서는 완전히 관련 없음\n",
    "- 점수 3-5: 문서는 일부 관련 정보가 있지만 질의에 직접 답변하지 않음\n",
    "- 점수 6-8: 문서는 관련성이 있으며 질의에 부분적으로 답변함\n",
    "- 점수 9-10: 문서는 매우 관련성이 높으며 질의에 직접 답변함\n",
    "\n",
    "0에서 10 사이의 단일 정수 점수만 응답해야 합니다. 다른 텍스트는 포함하지 마십시오.\"\"\"\n",
    "    \n",
    "    # 각 결과 반복\n",
    "    for i, result in enumerate(results):\n",
    "        # 5개 문서마다 진행 상황 표시\n",
    "        if i % 5 == 0:\n",
    "            print(f\"문서 {i+1}/{len(results)} 점수 매기는 중...\")\n",
    "        \n",
    "        # LLM에 대한 사용자 프롬프트 정의\n",
    "        user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "문서:\n",
    "{result['text']}\n",
    "\n",
    "0에서 10까지의 척도로 이 문서의 질의 관련성을 평가하십시오:\"\"\"\n",
    "        \n",
    "        # LLM 응답 가져오기\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0, # 일관된 점수 평가를 위해 온도를 0으로 설정\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # LLM 응답에서 점수 추출\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # 정규 표현식을 사용하여 숫자 점수 추출\n",
    "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text) # 0-9 또는 10 사이의 숫자 찾기\n",
    "        if score_match:\n",
    "            score = float(score_match.group(1))\n",
    "        else:\n",
    "            # 점수 추출 실패 시 유사도 점수를 대체 점수로 사용\n",
    "            print(f\"경고: 응답에서 점수를 추출할 수 없습니다: '{score_text}', 대신 유사도 점수를 사용합니다.\")\n",
    "            score = result[\"similarity\"] * 10 # 유사도(0~1)를 0~10점으로 변환\n",
    "        \n",
    "        # 점수가 매겨진 결과를 리스트에 추가\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": score\n",
    "        })\n",
    "    \n",
    "    # 관련성 점수를 기준으로 결과 내림차순 정렬\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 top_n개 결과 반환\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 키워드 기반 재정렬\n",
    "LLM을 사용하는 대신, 키워드 일치 및 위치를 기반으로 하는 간단한 재정렬 방법입니다. LLM 호출 비용이 부담스러울 때 사용할 수 있는 대안입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_keywords(query, results, top_n=3):\n",
    "    \"\"\"\n",
    "    키워드 일치 및 위치를 기반으로 하는 간단한 대체 재정렬 방법입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 초기 검색 결과\n",
    "        top_n (int): 재정렬 후 반환할 결과 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 재정렬된 결과\n",
    "    \"\"\"\n",
    "    # 질의에서 중요한 키워드 추출 (3글자 이상 단어)\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    \n",
    "    scored_results = []  # 점수가 매겨진 결과를 저장할 리스트 초기화\n",
    "    \n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()  # 문서 텍스트를 소문자로 변환\n",
    "        \n",
    "        # 기본 점수는 벡터 유사도의 50%로 시작\n",
    "        base_score = result[\"similarity\"] * 0.5\n",
    "        \n",
    "        # 키워드 점수 초기화\n",
    "        keyword_score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in document_text:\n",
    "                # 발견된 각 키워드에 대해 점수 추가\n",
    "                keyword_score += 0.1\n",
    "                \n",
    "                # 키워드가 문서 앞부분에 나타나면 추가 점수 부여\n",
    "                first_position = document_text.find(keyword)\n",
    "                if first_position < len(document_text) / 4:  # 텍스트의 첫 1/4에 위치\n",
    "                    keyword_score += 0.1\n",
    "                \n",
    "                # 키워드 빈도에 따라 점수 추가\n",
    "                frequency = document_text.count(keyword)\n",
    "                keyword_score += min(0.05 * frequency, 0.2)  # 최대 0.2점까지\n",
    "        \n",
    "        # 기본 점수와 키워드 점수를 결합하여 최종 점수 계산\n",
    "        final_score = base_score + keyword_score\n",
    "        \n",
    "        # 점수가 매겨진 결과를 리스트에 추가\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # 최종 관련성 점수를 기준으로 결과 내림차순 정렬\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 top_n개 결과 반환\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 검색된 컨텍스트\n",
    "        model (str): 응답 생성에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # AI의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"당신은 도움이 되는 AI 어시스턴트입니다. 제공된 컨텍스트만을 기반으로 사용자의 질문에 답변하십시오. 컨텍스트에서 답변을 찾을 수 없으면 정보가 충분하지 않다고 명시하십시오.\"\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "        컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질문: {query}\n",
    "\n",
    "        위에 제공된 컨텍스트만을 기반으로 포괄적인 답변을 제공하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 재정렬을 사용한 전체 RAG 파이프라인\n",
    "지금까지 RAG 파이프라인의 핵심 구성 요소(문서 처리, 질의응답, 재정렬 포함)를 구현했습니다. 이제 이러한 구성 요소를 결합하여 전체 RAG 파이프라인을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    재정렬을 통합한 전체 RAG 파이프라인입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        reranking_method (str): 재정렬 방법 ('llm' 또는 'keywords')\n",
    "        top_n (int): 재정렬 후 반환할 결과 수\n",
    "        model (str): 응답 생성용 모델\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 컨텍스트 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    # 질의 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 초기 검색 (재정렬을 위해 필요한 것보다 더 많이 가져오기)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=10) # 초기 검색 결과 수를 늘림 (예: 10개)\n",
    "    \n",
    "    # 재정렬 적용\n",
    "    if reranking_method == \"llm\":\n",
    "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
    "    else: # reranking_method가 None이거나 다른 값일 경우\n",
    "        # 재정렬 없음, 초기 검색 결과의 상위 결과 사용\n",
    "        reranked_results = initial_results[:top_n]\n",
    "    \n",
    "    # 재정렬된 결과에서 컨텍스트 결합\n",
    "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response_text = generate_response(query, context, model) # 변수명을 response에서 response_text로 변경\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"reranking_method\": reranking_method,\n",
    "        \"initial_results\": initial_results[:top_n], # 비교를 위해 초기 상위 결과도 포함\n",
    "        \"reranked_results\": reranked_results,\n",
    "        \"context\": context,\n",
    "        \"response\": response_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 재정렬 품질 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 첫 번째 질의 추출\n",
    "query = data[0]['question']\n",
    "\n",
    "# 검증 데이터에서 참조 답변 추출\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Comparing retrieval methods...\n",
      "\n",
      "=== STANDARD RETRIEVAL ===\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "Based on the provided context, it is clear that AI has the potential to significantly transform the way we live and work. The context highlights the various applications of AI in different industries, including:\n",
      "\n",
      "1. Automation and Job Displacement: AI can automate repetitive or routine tasks, potentially displacing some jobs, but also creating new opportunities and transforming existing roles.\n",
      "2. Reskilling and Upskilling: AI requires workers to reskill and upskill to adapt to new roles and collaborate with AI systems.\n",
      "3. Human-AI Collaboration: AI tools can augment human capabilities, automate mundane tasks, and provide insights that support decision-making, leading to increased collaboration between humans and AI systems.\n",
      "4. New Job Roles: The development and deployment of AI create new job roles in areas such as AI development, data science, AI ethics, and AI training.\n",
      "5. Ethical Considerations: AI raises ethical concerns, including ensuring fairness, transparency, and accountability in AI systems, as well as protecting worker rights and privacy.\n",
      "\n",
      "In terms of its impact on daily life, AI is transforming business operations, leading to increased efficiency, reduced costs, and improved decision-making. AI-powered tools are also enhancing customer relationship management, supply chain management, and other areas, leading to improved customer experiences and satisfaction.\n",
      "\n",
      "Furthermore, AI is being used as a creative tool, generating art, music, and literature, and assisting in design processes and scientific discovery. This suggests that AI has the potential to transform the way we live and work, not just in terms of efficiency and productivity, but also in terms of creativity and innovation.\n",
      "\n",
      "Overall, the context suggests that AI has the potential to revolutionize various aspects of our lives and work, from automation and job displacement to human-AI collaboration, new job roles, and ethical considerations.\n",
      "\n",
      "=== LLM-BASED RERANKING ===\n",
      "Reranking 10 documents...\n",
      "Scoring document 1/10...\n",
      "Scoring document 6/10...\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "Based on the provided context, it is clear that AI has the potential to significantly transform the way we live and work. The context highlights the various applications of AI in different industries, including:\n",
      "\n",
      "1. Automation and Job Displacement: AI can automate repetitive or routine tasks, potentially displacing some jobs, but also creating new opportunities and transforming existing roles.\n",
      "2. Reskilling and Upskilling: AI requires workers to reskill and upskill to adapt to new roles and collaborate with AI systems.\n",
      "3. Human-AI Collaboration: AI tools can augment human capabilities, automate mundane tasks, and provide insights that support decision-making, leading to increased collaboration between humans and AI systems.\n",
      "4. New Job Roles: The development and deployment of AI create new job roles in areas such as AI development, data science, AI ethics, and AI training.\n",
      "5. Ethical Considerations: AI raises ethical concerns, including ensuring fairness, transparency, and accountability in AI systems, as well as protecting worker rights and privacy.\n",
      "\n",
      "In terms of its impact on daily life, AI is transforming business operations, leading to increased efficiency, reduced costs, and improved decision-making. AI-powered tools are also enhancing customer relationship management, supply chain management, and other areas, leading to improved customer experiences and satisfaction.\n",
      "\n",
      "Furthermore, AI is being used as a creative tool, generating art, music, and literature, and assisting in design processes and scientific discovery. This suggests that AI has the potential to transform the way we live and work, not just in terms of efficiency and productivity, but also in terms of creativity and innovation.\n",
      "\n",
      "Overall, the context suggests that AI has the potential to revolutionize various aspects of our lives and work, from automation and job displacement to human-AI collaboration, new job roles, and ethical considerations.\n",
      "\n",
      "=== KEYWORD-BASED RERANKING ===\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "Based on the provided context, it appears that AI has the potential to significantly transform the way we live and work. The context highlights the various applications of AI in different industries, including business operations, customer service, supply chain management, and social good initiatives.\n",
      "\n",
      "AI is transforming business operations by increasing efficiency, reducing costs, and improving decision-making. It is also enhancing customer relationship management by providing personalized experiences, predicting customer behavior, and automating customer service interactions. Additionally, AI is optimizing supply chain operations by predicting demand, managing inventory, and streamlining logistics.\n",
      "\n",
      "Furthermore, AI is being used to address social and environmental challenges, such as climate change, poverty, and healthcare disparities. This suggests that AI has the potential to positively impact various aspects of our lives and work.\n",
      "\n",
      "However, the context also raises concerns about job displacement, particularly in industries with repetitive or routine tasks. To mitigate these risks, reskilling and upskilling initiatives are necessary to equip workers with the skills needed to adapt to new roles and collaborate with AI systems.\n",
      "\n",
      "Overall, the context suggests that AI has the potential to transform the way we live and work, but it is essential to address the challenges and risks associated with its development and deployment.\n"
     ]
    }
   ],
   "source": [
    "# 문서 처리\n",
    "vector_store = process_document(pdf_path)\n",
    "\n",
    "# 예제 질의 (질의 내용을 더 일반적인 것으로 변경하여 다양한 결과를 유도)\n",
    "query = \"AI는 우리의 삶과 일하는 방식을 어떻게 변화시킬 잠재력이 있는가?\"\n",
    "\n",
    "# 다양한 방법 비교\n",
    "print(\"검색 방법 비교 중...\")\n",
    "\n",
    "# 1. 표준 검색 (재정렬 없음)\n",
    "print(\"\\n=== 표준 검색 ===\")\n",
    "standard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\n",
    "print(f\"\\n질의: {query}\")\n",
    "print(f\"\\n응답:\\n{standard_results['response']}\")\n",
    "\n",
    "# 2. LLM 기반 재정렬\n",
    "print(\"\\n=== LLM 기반 재정렬 ===\")\n",
    "llm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\n",
    "print(f\"\\n질의: {query}\")\n",
    "print(f\"\\n응답:\\n{llm_results['response']}\")\n",
    "\n",
    "# 3. 키워드 기반 재정렬\n",
    "print(\"\\n=== 키워드 기반 재정렬 ===\")\n",
    "keyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\n",
    "print(f\"\\n질의: {query}\")\n",
    "print(f\"\\n응답:\\n{keyword_results['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
    "    \"\"\"\n",
    "    표준 결과와 비교하여 재정렬된 결과의 품질을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        standard_results (Dict): 표준 검색 결과\n",
    "        reranked_results (Dict): 재정렬된 검색 결과\n",
    "        reference_answer (str, optional): 비교를 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 평가 결과\n",
    "    \"\"\"\n",
    "    # AI 평가자를 위한 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다.\n",
    "    두 가지 다른 검색 방법으로 검색된 컨텍스트와 응답을 비교하십시오.\n",
    "    어떤 방법이 더 나은 컨텍스트와 더 정확하고 포괄적인 답변을 제공하는지 평가하십시오.\"\"\"\n",
    "    \n",
    "    # 잘린 컨텍스트와 응답으로 비교 텍스트 준비\n",
    "    comparison_text = f\"\"\"질의: {query}\n",
    "\n",
    "표준 검색 컨텍스트:\n",
    "{standard_results['context'][:1000]}... [잘림]\n", # 컨텍스트가 너무 길 경우를 대비해 일부만 표시\n",
    "\n",
    "표준 검색 답변:\n",
    "{standard_results['response']}\n",
    "\n",
    "재정렬된 검색 컨텍스트:\n",
    "{reranked_results['context'][:1000]}... [잘림]\n", # 컨텍스트가 너무 길 경우를 대비해 일부만 표시\n",
    "\n",
    "재정렬된 검색 답변:\n",
    "{reranked_results['response']}\"\"\"\n",
    "\n",
    "    # 참조 답변이 제공되면 비교 텍스트에 포함\n",
    "    if reference_answer:\n",
    "        comparison_text += f\"\"\"\n",
    "        \n",
    "참조 답변:\n",
    "{reference_answer}\"\"\"\n",
    "\n",
    "    # AI 평가자를 위한 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "{comparison_text}\n",
    "\n",
    "어떤 검색 방법이 다음을 제공했는지 평가하십시오:\n",
    "1. 더 관련성 높은 컨텍스트\n",
    "2. 더 정확한 답변\n",
    "3. 더 포괄적인 답변\n",
    "4. 전반적으로 더 나은 성능\n",
    "\n",
    "구체적인 예를 들어 상세하게 분석하십시오.\n",
    "\"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 평가 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 평가 결과 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "After analyzing the three retrieval methods, I will evaluate which one provides better context and a more accurate, comprehensive answer.\n",
      "\n",
      "**1. More relevant context:**\n",
      "Both Standard Retrieval Context and Reranked Retrieval Context provide relevant context, but the Standard Retrieval Context is more comprehensive. It includes a broader range of topics related to AI, such as customer service, algorithmic trading, and management, which are all relevant to the query. The Reranked Retrieval Context, on the other hand, is more focused on the specific topics of automation, job displacement, and human-AI collaboration, which are also relevant but not as comprehensive as the Standard Retrieval Context.\n",
      "\n",
      "**2. More accurate answer:**\n",
      "Both Standard Retrieval Answer and Reranked Retrieval Answer provide accurate answers, but the Standard Retrieval Answer is more comprehensive. It covers a wider range of topics related to AI, including its impact on daily life, creativity, and innovation, whereas the Reranked Retrieval Answer is more focused on the specific topics of automation, job displacement, and human-AI collaboration.\n",
      "\n",
      "**3. More comprehensive answer:**\n",
      "The Standard Retrieval Answer is more comprehensive, covering a wider range of topics related to AI, including its impact on daily life, creativity, and innovation. The Reranked Retrieval Answer is more focused on the specific topics of automation, job displacement, and human-AI collaboration, which are all relevant but not as comprehensive as the Standard Retrieval Answer.\n",
      "\n",
      "**4. Better overall performance:**\n",
      "Based on the analysis, I would rate the Standard Retrieval Context as having better overall performance. It provides a more comprehensive and relevant context, which is essential for providing an accurate and comprehensive answer. The Standard Retrieval Answer also provides a more comprehensive answer, covering a wider range of topics related to AI.\n",
      "\n",
      "**Detailed analysis with specific examples:**\n",
      "\n",
      "* The Standard Retrieval Context includes a broader range of topics related to AI, such as customer service, algorithmic trading, and management, which are all relevant to the query. For example, the context mentions \"customer service\" and \"algorithmic trading\", which are both relevant to the query. In contrast, the Reranked Retrieval Context is more focused on the specific topics of automation, job displacement, and human-AI collaboration.\n",
      "* The Standard Retrieval Answer covers a wider range of topics related to AI, including its impact on daily life, creativity, and innovation. For example, the answer mentions \"AI is being used as a creative tool, generating art, music, and literature, and assisting in design processes and scientific discovery\", which is not mentioned in the Reranked Retrieval Answer.\n",
      "* The Standard Retrieval Answer also provides more specific examples of how AI is being used in different industries, such as \"AI-powered tools are also enhancing customer relationship management, supply chain management, and other areas, leading to improved customer experiences and satisfaction\". This provides more context and insight into how AI is being used in different industries.\n",
      "\n",
      "In conclusion, the Standard Retrieval Context and Answer provide better context and a more accurate, comprehensive answer than the Reranked Retrieval Context and Answer. The Standard Retrieval Context is more comprehensive and relevant, and the Standard Retrieval Answer is more comprehensive and accurate.\n"
     ]
    }
   ],
   "source": [
    "# 표준 결과와 비교하여 재정렬된 결과의 품질 평가\n",
    "evaluation = evaluate_reranking(\n",
    "    query=query,  # 사용자 질의\n",
    "    standard_results=standard_results,  # 표준 검색 결과\n",
    "    reranked_results=llm_results,  # LLM 기반 재정렬 결과\n",
    "    reference_answer=reference_answer  # 비교를 위한 참조 답변\n",
    ")\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"\\n=== 평가 결과 ===\")\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 08_reranker.ipynb]
