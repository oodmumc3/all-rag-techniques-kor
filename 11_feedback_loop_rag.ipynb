{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# RAG에서의 피드백 루프\n",
    "\n",
    "이 노트북에서는 시간이 지남에 따라 지속적으로 개선되는 피드백 루프 메커니즘을 갖춘 RAG 시스템을 구현합니다. 사용자 피드백을 수집하고 통합함으로써 우리 시스템은 각 상호 작용을 통해 더 관련성 높고 고품질의 응답을 제공하도록 학습합니다.\n",
    "기존의 RAG 시스템은 정적입니다. 즉, 임베딩 유사성에만 기반하여 정보를 검색합니다. 피드백 루프를 사용하면 다음과 같은 동적 시스템을 만듭니다:\n",
    "\n",
    "- 무엇이 효과가 있었는지 (그리고 무엇이 효과가 없었는지) 기억합니다.\n",
    "- 시간이 지남에 따라 문서 관련성 점수를 조정합니다.\n",
    "- 성공적인 Q&A 쌍을 지식 기반에 통합합니다.\n",
    "- 각 사용자 상호 작용을 통해 더 똑똑해집니다.\n",
    "\n",
    "피드백 루프는 RAG 시스템이 사용자와의 상호작용을 통해 스스로 학습하고 발전하는 매우 중요한 개념입니다. 사용자가 시스템의 답변에 대해 평가(예: '도움이 되었어요', '관련 없어요')를 제공하면, 시스템은 이 피드백을 바탕으로 향후 더 나은 검색 결과와 답변을 제공하도록 내부적으로 조정합니다. 예를 들어, 긍정적인 피드백을 받은 정보는 관련성 점수를 높이고, 부정적인 피드백을 받은 정보는 점수를 낮추는 방식으로 작동할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime # 피드백 타임스탬프 기록용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현\n",
    "문서 청크와 해당 임베딩을 관리하기 위한 기본적인 벡터 저장소를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \n",
    "    이 클래스는 임베딩 벡터와 해당 텍스트 청크 및 메타데이터에 대한\n",
    "    인메모리 저장 및 검색 시스템을 제공합니다.\n",
    "    코사인 유사도를 사용한 기본적인 유사도 검색 기능을 지원합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터, 텍스트 및 메타데이터에 대한 빈 리스트로 벡터 저장소를 초기화합니다.\n",
    "        \n",
    "        벡터 저장소는 세 개의 병렬 리스트를 유지합니다:\n",
    "        - vectors: 임베딩 벡터의 NumPy 배열\n",
    "        - texts: 각 벡터에 해당하는 원본 텍스트 청크\n",
    "        - metadata: 각 항목에 대한 선택적 메타데이터 딕셔너리\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []    # 원본 텍스트 청크를 저장할 리스트\n",
    "        self.metadata = [] # 각 텍스트 청크에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 저장할 원본 텍스트 청크.\n",
    "            embedding (List[float]): 텍스트를 나타내는 임베딩 벡터.\n",
    "            metadata (dict, optional): 텍스트 청크에 대한 추가 메타데이터 (예: 출처, 타임스탬프 또는 관련성 점수).\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩 변환 및 저장\n",
    "        self.texts.append(text)                   # 원본 텍스트 저장\n",
    "        self.metadata.append(metadata or {})      # 메타데이터 저장 (None이면 빈 딕셔너리)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        코사인 유사도를 사용하여 질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 저장된 벡터와 비교할 질의 임베딩 벡터.\n",
    "            k (int): 반환할 가장 유사한 결과 수.\n",
    "            filter_func (callable, optional): 메타데이터를 기반으로 결과를 필터링하는 함수.\n",
    "                                             메타데이터 딕셔너리를 입력으로 받고 부울 값을 반환합니다.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 가장 유사한 항목, 각 항목은 다음을 포함합니다:\n",
    "                - text: 원본 텍스트\n",
    "                - metadata: 연관된 메타데이터\n",
    "                - similarity: 원시 코사인 유사도 점수\n",
    "                - relevance_score: 메타데이터 기반 관련성 또는 계산된 유사도\n",
    "                \n",
    "        참고: 저장된 벡터가 없거나 필터를 통과하는 항목이 없으면 빈 리스트를 반환합니다.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 벡터 저장소가 비어 있으면 빈 리스트 반환\n",
    "        \n",
    "        # 벡터 연산을 위해 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 질의와 각 저장된 벡터 간의 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 필터 기준을 통과하지 못하는 항목 건너뛰기\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산: 내적 / (노름1 * 노름2)\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 저장\n",
    "        \n",
    "        # 유사도 점수를 기준으로 결과 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 일치 항목에 대한 결과 딕셔너리 구성\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score,\n",
    "                # 사용 가능한 경우 메타데이터의 기존 관련성 점수를 사용하고, 그렇지 않으면 유사도 사용\n",
    "                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트.\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    # 균일한 처리를 위해 단일 문자열을 리스트로 변환\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 모든 입력 텍스트에 대한 임베딩을 생성하기 위해 OpenAI API 호출\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 단일 문자열 입력의 경우 첫 번째 임베딩 벡터만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 리스트 입력의 경우 모든 임베딩 벡터 목록 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 시스템 함수\n",
    "이제 핵심 피드백 시스템 구성 요소를 구현합니다.\n",
    "피드백을 수집, 저장하고 이를 바탕으로 시스템을 개선하는 함수들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    \"\"\"\n",
    "    사용자 피드백을 딕셔너리 형식으로 지정합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        response (str): 시스템 응답\n",
    "        relevance (int): 관련성 점수 (1-5)\n",
    "        quality (int): 품질 점수 (1-5)\n",
    "        comments (str): 선택적 피드백 의견\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 형식화된 피드백\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance), # 정수형으로 변환\n",
    "        \"quality\": int(quality), # 정수형으로 변환\n",
    "        \"comments\": comments,\n",
    "        \"timestamp\": datetime.now().isoformat() # 현재 시간을 ISO 형식으로 기록\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    JSON 파일에 피드백을 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        feedback (Dict): 피드백 데이터\n",
    "        feedback_file (str): 피드백 파일 경로\n",
    "    \"\"\"\n",
    "    # 추가 모드('a')로 파일을 열어 기존 내용에 피드백 추가\n",
    "    with open(feedback_file, \"a\") as f:\n",
    "        json.dump(feedback, f) # 딕셔너리를 JSON 문자열로 변환하여 파일에 쓰기\n",
    "        f.write(\"\\n\") # 각 피드백 항목을 새 줄로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    파일에서 피드백 데이터를 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        feedback_file (str): 피드백 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 피드백 항목 목록\n",
    "    \"\"\"\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        # 읽기 모드('r')로 파일 열기\n",
    "        with open(feedback_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip(): # 빈 줄이 아닌 경우에만 처리\n",
    "                    feedback_data.append(json.loads(line.strip())) # JSON 문자열을 딕셔너리로 변환하여 리스트에 추가\n",
    "    except FileNotFoundError:\n",
    "        print(\"피드백 데이터 파일을 찾을 수 없습니다. 빈 피드백으로 시작합니다.\")\n",
    "    \n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 인식을 통한 문서 처리\n",
    "피드백을 활용하여 문서 처리 방식을 개선합니다. 각 청크의 메타데이터에 초기 관련성 점수와 피드백 횟수를 기록하여, 나중에 피드백에 따라 이 값들을 업데이트할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    피드백 루프를 사용한 검색 증강 생성(RAG)을 위해 문서를 처리합니다.\n",
    "    이 함수는 전체 문서 처리 파이프라인을 처리합니다:\n",
    "    1. PDF에서 텍스트 추출\n",
    "    2. 중첩을 사용한 텍스트 청킹\n",
    "    3. 청크에 대한 임베딩 생성\n",
    "    4. 메타데이터와 함께 벡터 데이터베이스에 저장\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): 처리할 PDF 파일 경로.\n",
    "    chunk_size (int): 각 텍스트 청크의 문자 크기.\n",
    "    chunk_overlap (int): 연속된 청크 간의 중첩 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: 다음을 포함하는 튜플:\n",
    "        - 문서 청크 목록\n",
    "        - 임베딩 및 메타데이터가 채워진 벡터 저장소\n",
    "    \"\"\"\n",
    "    # 1단계: PDF 문서에서 원시 텍스트 콘텐츠 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2단계: 더 나은 컨텍스트 보존을 위해 텍스트를 관리 가능하고 중첩되는 청크로 분할\n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    # 3단계: 각 텍스트 청크에 대한 벡터 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 4단계: 청크와 해당 임베딩을 저장할 벡터 데이터베이스 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 5단계: 각 청크와 해당 임베딩을 벡터 저장소에 추가\n",
    "    # 피드백 기반 개선을 위한 메타데이터 포함\n",
    "    for i, (chunk_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                # 원본 문서에서의 위치\n",
    "                \"source\": pdf_path,        # 출처 문서 경로\n",
    "                \"relevance_score\": 1.0,    # 초기 관련성 점수 (피드백으로 업데이트됨)\n",
    "                \"feedback_count\": 0        # 이 청크에 대해 받은 피드백 수\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 기반 관련성 조정\n",
    "이전 피드백을 바탕으로 검색된 문서의 관련성 점수를 동적으로 조정합니다. 현재 질의 및 문서와 관련된 과거 피드백이 있다면, 해당 피드백의 평가(긍정/부정)를 반영하여 현재 검색된 문서의 점수를 높이거나 낮춥니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feedback_relevance(query, doc_text, feedback):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 과거 피드백 항목이 현재 질의 및 문서와 관련이 있는지 평가합니다.\n",
    "    \n",
    "    이 함수는 현재 질의, 과거 질의+피드백 및 문서 내용을 LLM에 전송하여\n",
    "    관련성 평가를 통해 현재 검색에 어떤 과거 피드백이 영향을 미쳐야 하는지 결정하는 데 도움이 됩니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 정보 검색이 필요한 현재 사용자 질의\n",
    "        doc_text (str): 평가 중인 문서의 텍스트 내용\n",
    "        feedback (Dict): 'query' 및 'response' 키를 포함하는 이전 피드백 데이터\n",
    "        \n",
    "    Returns:\n",
    "        bool: 피드백이 현재 질의/문서와 관련이 있다고 판단되면 True, 그렇지 않으면 False\n",
    "    \"\"\"\n",
    "    # LLM이 이진 관련성 판단만 하도록 지시하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 과거 피드백이 현재 질의 및 문서와 관련이 있는지 판단하는 AI 시스템입니다.\n",
    "    '예' 또는 '아니요'로만 답변하십시오. 당신의 임무는 설명을 제공하는 것이 아니라 관련성을 엄격하게 결정하는 것입니다.\"\"\"\n",
    "\n",
    "    # 현재 질의, 과거 피드백 데이터 및 잘린 문서 내용을 포함하는 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    현재 질의: {query}\n",
    "    피드백을 받은 과거 질의: {feedback['query']}\n",
    "    문서 내용: {doc_text[:500]}... [잘림]\n",
    "    피드백을 받은 과거 응답: {feedback['response'][:500]}... [잘림]\n",
    "\n",
    "    이 과거 피드백이 현재 질의 및 문서와 관련이 있습니까? (예/아니요)\n",
    "    \"\"\"\n",
    "\n",
    "    # 결정론적 출력을 위해 온도 0으로 LLM API 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  # 일관되고 결정론적인 응답을 위해 온도=0 사용\n",
    "    )\n",
    "    \n",
    "    # 응답을 추출하고 정규화하여 관련성 결정\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    return '예' in answer  # 답변에 '예'가 포함되어 있으면 True 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_relevance_scores(query, results, feedback_data):\n",
    "    \"\"\"\n",
    "    검색 품질을 향상시키기 위해 과거 피드백을 기반으로 문서 관련성 점수를 조정합니다.\n",
    "    \n",
    "    이 함수는 과거 사용자 피드백을 분석하여 검색된 문서의 관련성 점수를 동적으로 조정합니다.\n",
    "    현재 질의 컨텍스트와 관련된 피드백을 식별하고, 관련성 평가를 기반으로 점수 수정자를 계산하며,\n",
    "    그에 따라 결과 순위를 다시 매깁니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 현재 사용자 질의\n",
    "        results (List[Dict]): 원본 유사도 점수가 있는 검색된 문서\n",
    "        feedback_data (List[Dict]): 사용자 평가가 포함된 과거 피드백\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 조정된 관련성 점수가 있는 결과, 새 점수를 기준으로 정렬됨\n",
    "    \"\"\"\n",
    "    # 사용 가능한 피드백 데이터가 없으면 원본 결과 변경 없이 반환\n",
    "    if not feedback_data:\n",
    "        return results\n",
    "    \n",
    "    print(\"피드백 기록을 기반으로 관련성 점수 조정 중...\")\n",
    "    \n",
    "    # 검색된 각 문서 처리\n",
    "    for i, result in enumerate(results):\n",
    "        document_text = result[\"text\"]\n",
    "        relevant_feedback = []\n",
    "        \n",
    "        # 이 특정 문서 및 질의 조합에 대한 관련 피드백 찾기\n",
    "        # 각 과거 피드백 항목의 관련성을 평가하기 위해 LLM에 질의\n",
    "        for feedback_item in feedback_data: # 변수명을 feedback에서 feedback_item으로 변경\n",
    "            is_relevant = assess_feedback_relevance(query, document_text, feedback_item)\n",
    "            if is_relevant:\n",
    "                relevant_feedback.append(feedback_item)\n",
    "        \n",
    "        # 관련 피드백이 있으면 점수 조정 적용\n",
    "        if relevant_feedback:\n",
    "            # 모든 해당 피드백 항목의 평균 관련성 평가 계산\n",
    "            # 피드백 관련성은 1-5점 척도 (1=관련 없음, 5=매우 관련 높음)\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            \n",
    "            # 평균 관련성을 0.5-1.5 범위의 점수 수정자로 변환\n",
    "            # - 3/5 미만 점수는 원본 유사도 감소 (수정자 < 1.0)\n",
    "            # - 3/5 초과 점수는 원본 유사도 증가 (수정자 > 1.0)\n",
    "            modifier = 0.5 + (avg_relevance / 5.0)\n",
    "            \n",
    "            # 원본 유사도 점수에 수정자 적용\n",
    "            original_score = result[\"similarity\"]\n",
    "            adjusted_score = original_score * modifier\n",
    "            \n",
    "            # 새 점수와 피드백 메타데이터로 결과 딕셔너리 업데이트\n",
    "            result[\"original_similarity\"] = original_score  # 원본 점수 보존\n",
    "            result[\"similarity\"] = adjusted_score           # 기본 점수 업데이트\n",
    "            result[\"relevance_score\"] = adjusted_score      # 관련성 점수 업데이트\n",
    "            result[\"feedback_applied\"] = True               # 피드백 적용 여부 플래그\n",
    "            result[\"feedback_count\"] = len(relevant_feedback)  # 사용된 피드백 항목 수\n",
    "            \n",
    "            # 조정 세부 정보 기록\n",
    "            print(f\"  문서 {i+1}: {len(relevant_feedback)}개의 피드백을 기반으로 점수가 {original_score:.4f}에서 {adjusted_score:.4f}로 조정됨\")\n",
    "    \n",
    "    # 조정된 점수를 기준으로 결과 다시 정렬하여 고품질 일치 항목이 먼저 표시되도록 함\n",
    "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백을 사용한 인덱스 미세 조정\n",
    "시간이 지남에 따라 검색 품질을 향상시키기 위해 고품질 피드백으로 벡터 저장소를 강화합니다. 성공적인 Q&A 쌍으로부터 새로운 검색 항목을 만들어 관련성 가중치를 높여 벡터 저장소에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(current_store, chunks, feedback_data):\n",
    "    \"\"\"\n",
    "    시간이 지남에 따라 검색 품질을 향상시키기 위해 고품질 피드백으로 벡터 저장소를 강화합니다.\n",
    "    \n",
    "    이 함수는 다음을 통해 지속적인 학습 프로세스를 구현합니다:\n",
    "    1. 고품질 피드백 식별 (높은 평가를 받은 Q&A 쌍)\n",
    "    2. 성공적인 상호 작용으로부터 새로운 검색 항목 생성\n",
    "    3. 관련성 가중치를 높여 벡터 저장소에 추가\n",
    "    \n",
    "    Args:\n",
    "        current_store (SimpleVectorStore): 원본 문서 청크를 포함하는 현재 벡터 저장소\n",
    "        chunks (List[str]): 원본 문서 텍스트 청크 \n",
    "        feedback_data (List[Dict]): 관련성 및 품질 평가가 포함된 과거 사용자 피드백\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: 원본 청크와 피드백 파생 콘텐츠를 모두 포함하는 강화된 벡터 저장소\n",
    "    \"\"\"\n",
    "    print(\"고품질 피드백으로 인덱스 미세 조정 중...\")\n",
    "    \n",
    "    # 고품질 응답만 필터링 (관련성 및 품질 모두 4점 또는 5점 평가)\n",
    "    # 가장 성공적인 상호 작용으로부터만 학습하도록 보장\n",
    "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    if not good_feedback:\n",
    "        print(\"미세 조정을 위한 고품질 피드백을 찾을 수 없습니다.\")\n",
    "        return current_store  # 좋은 피드백이 없으면 원본 저장소 변경 없이 반환\n",
    "    \n",
    "    # 원본 및 강화된 콘텐츠를 모두 포함할 새 저장소 초기화\n",
    "    new_store = SimpleVectorStore()\n",
    "    \n",
    "    # 먼저 기존 메타데이터와 함께 모든 원본 문서 청크 전송\n",
    "    for i in range(len(current_store.texts)):\n",
    "        new_store.add_item(\n",
    "            text=current_store.texts[i],\n",
    "            embedding=current_store.vectors[i],\n",
    "            metadata=current_store.metadata[i].copy()  # 참조 문제 방지를 위해 복사 사용\n",
    "        )\n",
    "    \n",
    "    # 좋은 피드백으로부터 강화된 콘텐츠 생성 및 추가\n",
    "    for feedback_item in good_feedback: # 변수명을 feedback에서 feedback_item으로 변경\n",
    "        # 질문과 해당 고품질 답변을 결합한 새 문서 형식 지정\n",
    "        # 이는 사용자 질의에 직접 답변하는 검색 가능한 콘텐츠 생성\n",
    "        enhanced_text = f\"질문: {feedback_item['query']}\\n답변: {feedback_item['response']}\"\n",
    "        \n",
    "        # 이 새 합성 문서에 대한 임베딩 벡터 생성\n",
    "        embedding = create_embeddings(enhanced_text)\n",
    "        \n",
    "        # 출처와 중요성을 식별하는 특수 메타데이터와 함께 벡터 저장소에 추가\n",
    "        new_store.add_item(\n",
    "            text=enhanced_text,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"type\": \"feedback_enhanced\",  # 피드백에서 파생되었음을 표시\n",
    "                \"query\": feedback_item[\"query\"],   # 참조를 위해 원본 질의 저장\n",
    "                \"relevance_score\": 1.2,       # 이러한 항목을 우선시하기 위해 초기 관련성 증폭\n",
    "                \"feedback_count\": 1,          # 피드백 통합 추적\n",
    "                \"original_feedback\": feedback_item # 전체 피드백 기록 보존\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"피드백으로부터 강화된 콘텐츠 추가됨: {feedback_item['query'][:50]}...\")\n",
    "    \n",
    "    # 강화에 대한 요약 통계 기록\n",
    "    print(f\"미세 조정된 인덱스에는 이제 {len(new_store.texts)}개의 항목이 있습니다 (원본: {len(chunks)})\")\n",
    "    return new_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 루프를 사용한 전체 RAG 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 검색된 문서의 컨텍스트 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # AI의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 도움이 되는 AI 어시스턴트입니다. 제공된 컨텍스트만을 기반으로 사용자의 질문에 답변하십시오. 컨텍스트에서 답변을 찾을 수 없으면 정보가 충분하지 않다고 명시하십시오.\"\"\"\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "        컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질문: {query}\n",
    "\n",
    "        위에 제공된 컨텍스트만을 기반으로 포괄적인 답변을 제공하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 시스템 및 사용자 프롬프트를 기반으로 응답을 생성하기 위해 OpenAI API 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  # 일관되고 결정론적인 응답을 위해 온도=0 사용\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    피드백 루프를 통합한 전체 RAG 파이프라인.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 문서 청크가 있는 벡터 저장소\n",
    "        feedback_data (List[Dict]): 피드백 기록\n",
    "        k (int): 검색할 문서 수\n",
    "        model (str): 응답 생성을 위한 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 검색된 문서 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 피드백 강화 RAG로 질의 처리 중 ===\")\n",
    "    print(f\"질의: {query}\")\n",
    "    \n",
    "    # 1단계: 질의 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 2단계: 질의 임베딩을 기반으로 초기 검색 수행\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 3단계: 피드백을 기반으로 검색된 문서의 관련성 점수 조정\n",
    "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
    "    \n",
    "    # 4단계: 컨텍스트 구축을 위해 조정된 결과에서 텍스트 추출\n",
    "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
    "    \n",
    "    # 5단계: 검색된 텍스트를 연결하여 응답 생성을 위한 컨텍스트 구축\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # 6단계: 컨텍스트와 질의를 사용하여 응답 생성\n",
    "    print(\"응답 생성 중...\")\n",
    "    response_text = generate_response(query, context, model) # response를 response_text로 변경\n",
    "    \n",
    "    # 7단계: 최종 결과 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": adjusted_results,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 응답 ===\")\n",
    "    print(response_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 워크플로: 초기 설정부터 피드백 수집까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_rag_workflow(pdf_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
    "    \"\"\"\n",
    "    지속적인 개선을 위한 피드백 통합을 갖춘 전체 RAG 워크플로를 실행합니다.\n",
    "    \n",
    "    이 함수는 전체 검색 증강 생성 프로세스를 조정합니다:\n",
    "    1. 과거 피드백 데이터 로드\n",
    "    2. 문서 처리 및 청킹\n",
    "    3. 선택적으로 이전 피드백으로 벡터 인덱스 미세 조정\n",
    "    4. 피드백 조정된 관련성 점수를 사용한 검색 및 생성 수행\n",
    "    5. 향후 개선을 위한 새 사용자 피드백 수집\n",
    "    6. 시간 경과에 따른 시스템 학습을 가능하게 하기 위해 피드백 저장\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 처리할 PDF 문서 경로\n",
    "        query (str): 사용자의 자연어 질의\n",
    "        feedback_data (List[Dict], optional): 미리 로드된 피드백 데이터, None이면 파일에서 로드\n",
    "        feedback_file (str): 피드백 기록을 저장하는 JSON 파일 경로\n",
    "        fine_tune (bool): 성공적인 과거 Q&A 쌍으로 인덱스를 강화할지 여부\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 응답 및 검색 메타데이터를 포함하는 결과\n",
    "    \"\"\"\n",
    "    # 1단계: 명시적으로 제공되지 않은 경우 관련성 조정을 위한 과거 피드백 로드\n",
    "    if feedback_data is None:\n",
    "        feedback_data = load_feedback_data(feedback_file)\n",
    "        print(f\"{feedback_file}에서 {len(feedback_data)}개의 피드백 항목 로드됨\")\n",
    "    \n",
    "    # 2단계: 추출, 청킹 및 임베딩 파이프라인을 통해 문서 처리\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 3단계: 고품질 과거 상호 작용을 통합하여 벡터 인덱스 미세 조정\n",
    "    # 이는 성공적인 Q&A 쌍으로부터 강화된 검색 가능한 콘텐츠 생성\n",
    "    if fine_tune and feedback_data:\n",
    "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    # 4단계: 피드백 인식 검색을 사용한 핵심 RAG 실행\n",
    "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "    \n",
    "    # 5단계: 향후 성능 개선을 위한 사용자 피드백 수집\n",
    "    print(\"\\n=== 이 응답에 대한 피드백을 제공하시겠습니까? ===\")\n",
    "    print(\"관련성 평가 (1-5, 5가 가장 관련성 높음):\")\n",
    "    relevance = input()\n",
    "    \n",
    "    print(\"품질 평가 (1-5, 5가 가장 높은 품질):\")\n",
    "    quality = input()\n",
    "    \n",
    "    print(\"의견이 있으십니까? (선택 사항, 건너뛰려면 Enter 키 누르십시오)\")\n",
    "    comments = input()\n",
    "    \n",
    "    # 6단계: 피드백을 구조화된 데이터로 형식 지정\n",
    "    new_feedback = get_user_feedback( # 변수명을 feedback에서 new_feedback으로 변경\n",
    "        query=query,\n",
    "        response=result[\"response\"],\n",
    "        relevance=int(relevance),\n",
    "        quality=int(quality),\n",
    "        comments=comments\n",
    "    )\n",
    "    \n",
    "    # 7단계: 지속적인 시스템 학습을 가능하게 하기 위해 피드백 유지\n",
    "    store_feedback(new_feedback, feedback_file)\n",
    "    print(\"피드백이 기록되었습니다. 감사합니다!\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 루프 평가\n",
    "피드백 통합 전후의 성능을 비교하여 RAG 품질에 대한 피드백 루프의 영향을 평가합니다.\n",
    "참조 답변이 있다면, 이를 사용하여 시스템 응답에 대한 가상 피드백을 생성하고, 이 피드백이 시스템 성능에 어떤 영향을 미치는지 관찰합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    피드백 통합 전후의 성능을 비교하여 RAG 품질에 대한 피드백 루프의 영향을 평가합니다.\n",
    "    \n",
    "    이 함수는 피드백 통합이 검색 및 생성에 어떤 영향을 미치는지 측정하기 위해 통제된 실험을 실행합니다:\n",
    "    1. 첫 번째 라운드: 피드백 없이 모든 테스트 질의 실행\n",
    "    2. (제공된 경우) 참조 답변을 기반으로 합성 피드백 생성\n",
    "    3. 두 번째 라운드: 피드백 강화 검색으로 동일한 질의 실행\n",
    "    4. 라운드 간 결과 비교하여 피드백 영향 정량화\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 지식 기반으로 사용되는 PDF 문서 경로\n",
    "        test_queries (List[str]): 시스템 성능을 평가하기 위한 테스트 질의 목록\n",
    "        reference_answers (List[str], optional): 평가 및 합성 피드백 생성을 위한 참조/정답 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 다음을 포함하는 평가 결과:\n",
    "            - round1_results: 피드백 없는 결과\n",
    "            - round2_results: 피드백 있는 결과\n",
    "            - comparison: 라운드 간 정량적 비교 메트릭\n",
    "    \"\"\"\n",
    "    print(\"=== 피드백 루프 영향 평가 ===\")\n",
    "    \n",
    "    # 이 평가 세션에만 사용할 임시 피드백 파일 생성\n",
    "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
    "    \n",
    "    # 피드백 수집 초기화 (시작 시 비어 있음)\n",
    "    feedback_data = []\n",
    "    \n",
    "    # ----------------------- 첫 번째 평가 라운드 -----------------------\n",
    "    # 기준 성능을 설정하기 위해 피드백 영향 없이 모든 질의 실행\n",
    "    print(\"\\n=== 1라운드: 피드백 없음 ===\")\n",
    "    round1_results = []\n",
    "    \n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n질의 {i+1}: {query_text}\")\n",
    "        \n",
    "        # 초기 벡터 저장소 생성을 위해 문서 처리\n",
    "        chunks, vector_store = process_document(pdf_path)\n",
    "        \n",
    "        # 피드백 영향 없이 RAG 실행 (빈 피드백 리스트)\n",
    "        result = rag_with_feedback_loop(query_text, vector_store, [])\n",
    "        round1_results.append(result)\n",
    "        \n",
    "        # 참조 답변을 사용할 수 있으면 합성 피드백 생성\n",
    "        # 이는 시스템 학습을 위한 사용자 피드백 시뮬레이션\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            # 참조 답변과의 유사도를 기반으로 합성 피드백 점수 계산\n",
    "            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n",
    "            # 유사도(0-1)를 평가 척도(1-5)로 변환\n",
    "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            \n",
    "            # 구조화된 피드백 항목 생성\n",
    "            feedback_entry = get_user_feedback( # feedback을 feedback_entry로 변경\n",
    "                query=query_text,\n",
    "                response=result[\"response\"],\n",
    "                relevance=relevance,\n",
    "                quality=quality,\n",
    "                comments=f\"참조 유사도 기반 합성 피드백: {similarity_to_ref:.2f}\"\n",
    "            )\n",
    "            \n",
    "            # 인메모리 컬렉션에 추가하고 임시 파일에 유지\n",
    "            feedback_data.append(feedback_entry)\n",
    "            store_feedback(feedback_entry, temp_feedback_file)\n",
    "    \n",
    "    # ----------------------- 두 번째 평가 라운드 -----------------------\n",
    "    # 개선 사항을 측정하기 위해 피드백 통합으로 동일한 질의 실행\n",
    "    print(\"\\n=== 2라운드: 피드백 포함 ===\")\n",
    "    round2_results = []\n",
    "    \n",
    "    # 피드백 파생 콘텐츠로 문서 처리 및 강화\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n질의 {i+1}: {query_text}\")\n",
    "        \n",
    "        # 피드백 영향으로 RAG 실행\n",
    "        result = rag_with_feedback_loop(query_text, vector_store, feedback_data)\n",
    "        round2_results.append(result)\n",
    "    \n",
    "    # ----------------------- 결과 분석 -----------------------\n",
    "    # 두 라운드 간의 성능 메트릭 비교\n",
    "    comparison_analysis = compare_results(test_queries, round1_results, round2_results, reference_answers) # comparison을 comparison_analysis로 변경\n",
    "    \n",
    "    # 임시 평가 아티팩트 정리\n",
    "    if os.path.exists(temp_feedback_file):\n",
    "        os.remove(temp_feedback_file)\n",
    "    \n",
    "    return {\n",
    "        \"round1_results\": round1_results,\n",
    "        \"round2_results\": round2_results,\n",
    "        \"comparison\": comparison_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가를 위한 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    임베딩을 사용하여 두 텍스트 간의 의미론적 유사도를 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): 첫 번째 텍스트\n",
    "        text2 (str): 두 번째 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        float: 0과 1 사이의 유사도 점수\n",
    "    \"\"\"\n",
    "    # 두 텍스트에 대한 임베딩 생성\n",
    "    embedding1 = create_embeddings(text1)\n",
    "    embedding2 = create_embeddings(text2)\n",
    "    \n",
    "    # 임베딩을 numpy 배열로 변환\n",
    "    vec1 = np.array(embedding1)\n",
    "    vec2 = np.array(embedding2)\n",
    "    \n",
    "    # 두 벡터 간의 코사인 유사도 계산\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
    "    \"\"\"\n",
    "    두 RAG 라운드의 결과를 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): 테스트 질의\n",
    "        round1_results (List[Dict]): 1라운드 결과\n",
    "        round2_results (List[Dict]): 2라운드 결과\n",
    "        reference_answers (List[str], optional): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 비교 분석\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 결과 비교 중 ===\")\n",
    "    \n",
    "    # AI의 평가 행동을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다. 두 가지 버전의 응답을 비교하십시오:\n",
    "        1. 표준 RAG: 피드백 사용 안 함\n",
    "        2. 피드백 강화 RAG: 검색 개선을 위해 피드백 루프 사용\n",
    "\n",
    "        다음 측면에서 어떤 버전이 더 나은 응답을 제공하는지 분석하십시오:\n",
    "        - 질의 관련성\n",
    "        - 정보 정확성\n",
    "        - 완전성\n",
    "        - 명확성 및 간결성\n",
    "    \"\"\"\n",
    "\n",
    "    comparisons = []\n",
    "    \n",
    "    # 각 질의와 해당 라운드 결과 반복\n",
    "    for i, (query_text, r1, r2) in enumerate(zip(queries, round1_results, round2_results)): # query를 query_text로 변경\n",
    "        # 응답 비교를 위한 프롬프트 생성\n",
    "        comparison_prompt = f\"\"\"\n",
    "        질의: {query_text}\n",
    "\n",
    "        표준 RAG 응답:\n",
    "        {r1[\"response\"]}\n",
    "\n",
    "        피드백 강화 RAG 응답:\n",
    "        {r2[\"response\"]}\n",
    "        \"\"\"\n",
    "\n",
    "        # 사용 가능한 경우 참조 답변 포함\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            comparison_prompt += f\"\"\"\n",
    "            참조 답변:\n",
    "            {reference_answers[i]}\n",
    "            \"\"\"\n",
    "\n",
    "        comparison_prompt += \"\"\"\n",
    "        이러한 응답을 비교하고 어떤 것이 더 나은지, 그 이유는 무엇인지 설명하십시오.\n",
    "        피드백 루프가 응답 품질을 어떻게 개선했는지(또는 개선하지 못했는지)에 특히 초점을 맞추십시오.\n",
    "        \"\"\"\n",
    "\n",
    "        # OpenAI API를 호출하여 비교 분석 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": comparison_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 결과에 비교 분석 추가\n",
    "        comparisons.append({\n",
    "            \"query\": query_text,\n",
    "            \"analysis\": response.choices[0].message.content\n",
    "        })\n",
    "        \n",
    "        # 각 질의에 대한 분석 일부 출력\n",
    "        print(f\"\\n질의 {i+1}: {query_text}\")\n",
    "        print(f\"분석: {response.choices[0].message.content[:200]}...\")\n",
    "    \n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 루프 평가 (사용자 정의 검증 질의)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 피드백 루프 영향 평가 ===\n",
      "\n",
      "=== 1라운드: 피드백 없음 ===\n",
      "\n",
      "질의 1: What is a neural network and how does it function?\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청킹 중...\n",
      "생성된 텍스트 청크 수: 42\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크 추가됨\n",
      "\n",
      "=== 피드백 강화 RAG로 질의 처리 중 ===\n",
      "질의: What is a neural network and how does it function?\n",
      "응답 생성 중...\n",
      "\n",
      "=== 응답 ===\n",
      "Based on the provided context, a neural network is a type of deep neural network that is particularly effective for processing data. The context does not provide a detailed explanation of how a neural network functions, but it does mention that neural networks are inspired by the structure and function of the human brain.\n",
      "\n",
      "However, it can be inferred that a neural network is a complex system that uses multiple layers to analyze data. The context mentions that deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers (deep neural networks) to analyze data.\n",
      "\n",
      "In the context of the provided text, neural networks are used in various applications such as image recognition, natural language processing, and speech recognition. They are also used in tasks like object detection, facial recognition, and medical image analysis.\n",
      "\n",
      "While the context does not provide a comprehensive explanation of how a neural network functions, it can be inferred that neural networks are designed to learn patterns and structures in the data through a process of trial and error, similar to reinforcement learning. However, the exact mechanism of how neural networks function is not explicitly stated in the provided context.\n",
      "\n",
      "=== 2라운드: 피드백 포함 ===\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청킹 중...\n",
      "생성된 텍스트 청크 수: 42\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크 추가됨\n",
      "고품질 피드백으로 인덱스 미세 조정 중...\n",
      "피드백으로부터 강화된 콘텐츠 추가됨: What is a neural network and how does it function?...\n",
      "미세 조정된 인덱스에는 이제 43개의 항목이 있습니다 (원본: 42)\n",
      "\n",
      "질의 1: What is a neural network and how does it function?\n",
      "\n",
      "=== 피드백 강화 RAG로 질의 처리 중 ===\n",
      "질의: What is a neural network and how does it function?\n",
      "피드백 기록을 기반으로 관련성 점수 조정 중...\n",
      "  문서 1: 1개의 피드백을 기반으로 점수가 0.8386에서 1.0902로 조정됨\n",
      "  문서 4: 1개의 피드백을 기반으로 점수가 0.6162에서 0.8010로 조정됨\n",
      "  문서 5: 1개의 피드백을 기반으로 점수가 0.6023에서 0.7830로 조정됨\n",
      "응답 생성 중...\n",
      "\n",
      "=== 응답 ===\n",
      "Based on the provided context, a neural network is a complex system that uses multiple layers to analyze data. It is inspired by the structure and function of the human brain and is particularly effective for processing data. Neural networks are used in various applications such as image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The context does not provide a detailed explanation of how a neural network functions, but it can be inferred that neural networks are designed to learn patterns and structures in the data through a process of trial and error. This process is similar to reinforcement learning, where the neural network receives feedback in the form of rewards or penalties, allowing it to adjust its parameters and improve its performance over time.\n",
      "\n",
      "Neural networks are composed of multiple layers, including convolutional layers, recurrent layers, and others. These layers work together to analyze the input data and make predictions or take actions. The exact mechanism of how neural networks function is not explicitly stated in the provided context, but it is clear that they are a powerful tool for analyzing and processing complex data.\n",
      "\n",
      "In the context of the provided text, neural networks are used in various applications such as:\n",
      "\n",
      "* Image recognition\n",
      "* Natural language processing\n",
      "* Speech recognition\n",
      "* Object detection\n",
      "* Facial recognition\n",
      "* Medical image analysis\n",
      "\n",
      "Overall, neural networks are a complex and powerful tool for analyzing and processing data, and their applications continue to expand across various industries and domains.\n",
      "\n",
      "=== 결과 비교 중 ===\n",
      "\n",
      "질의 1: What is a neural network and how does it function?\n",
      "분석: Comparing the two responses, the feedback-enhanced RAG response is significantly better than the standard RAG response. Here's a breakdown of the improvements:\n",
      "\n",
      "1. **Relevance to the query**: Both res...\n"
     ]
    }
   ],
   "source": [
    "# AI 문서 경로\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"신경망이란 무엇이며 어떻게 작동합니까?\",\n",
    "\n",
    "    #################################################################################\n",
    "    ### 테스트 목적으로 질의 수를 줄이기 위해 주석 처리된 질의 ###\n",
    "    \n",
    "    # \"강화 학습의 과정과 적용 분야를 설명하십시오.\",\n",
    "    # \"오늘날 기술에서 자연어 처리의 주요 적용 분야는 무엇입니까?\",\n",
    "    # \"기계 학습 모델에서 과적합의 영향과 이를 완화할 수 있는 방법을 설명하십시오.\"\n",
    "]\n",
    "\n",
    "# 평가를 위한 참조 답변 정의\n",
    "reference_answers = [\n",
    "    \"신경망은 인간 두뇌가 작동하는 방식을 모방한 프로세스를 통해 데이터 세트의 기본 관계를 인식하려고 시도하는 일련의 알고리즘입니다. 이는 노드 계층으로 구성되며 각 노드는 뉴런을 나타냅니다. 신경망은 예상 결과와 비교하여 출력 오류를 기반으로 노드 간 연결 가중치를 조정하여 작동합니다.\",\n",
    "\n",
    "    ############################################################################################\n",
    "    #### 테스트 목적으로 질의 수를 줄이기 위해 주석 처리된 참조 답변 ###\n",
    "\n",
    "#     \"강화 학습은 에이전트가 누적 보상을 최대화하기 위해 환경에서 작업을 수행하여 결정을 내리는 방법을 배우는 기계 학습의 한 유형입니다. 탐색, 활용 및 작업 결과로부터의 학습을 포함합니다. 응용 분야에는 로봇 공학, 게임 플레이 및 자율 주행 차량이 포함됩니다.\",\n",
    "#     \"오늘날 기술에서 자연어 처리의 주요 응용 분야에는 기계 번역, 감성 분석, 챗봇, 정보 검색, 텍스트 요약 및 음성 인식이 포함됩니다. NLP를 사용하면 기계가 인간 언어를 이해하고 생성하여 인간-컴퓨터 상호 작용을 용이하게 합니다.\",\n",
    "#     \"기계 학습 모델의 과적합은 모델이 훈련 데이터를 너무 잘 학습하여 노이즈와 이상치를 포착할 때 발생합니다. 이로 인해 모델이 훈련 데이터에서는 잘 수행되지만 보이지 않는 데이터에서는 제대로 수행되지 않아 새 데이터에 대한 일반화 성능이 저하됩니다. 완화 기술에는 교차 검증, 정규화, 가지치기 및 더 많은 훈련 데이터 사용이 포함됩니다.\"\n",
    "]\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = evaluate_feedback_loop(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# # 전체 RAG 워크플로 실행\n",
    "########################################\n",
    "\n",
    "# # 대화형 예제 실행\n",
    "# print(\"\\n\\n=== 대화형 예제 ===\")\n",
    "# print(\"AI에 대한 질문을 입력하십시오:\")\n",
    "# user_query = input()\n",
    "\n",
    "# # 누적된 피드백 로드\n",
    "# all_feedback = load_feedback_data()\n",
    "\n",
    "# # 전체 워크플로 실행\n",
    "# result = full_rag_workflow(\n",
    "#     pdf_path=pdf_path,\n",
    "#     query=user_query,\n",
    "#     feedback_data=all_feedback,\n",
    "#     fine_tune=True\n",
    "# )\n",
    "\n",
    "########################################\n",
    "# # 전체 RAG 워크플로 실행\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드백 영향 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 피드백 영향 분석 ===\n",
      "\n",
      "질의 1: 신경망이란 무엇이며 어떻게 작동합니까?\n",
      "\n",
      "피드백 영향 분석:\n",
      "Comparing the two responses, the feedback-enhanced RAG response is significantly better than the standard RAG response. Here's a breakdown of the improvements:\n",
      "\n",
      "1. **Relevance to the query**: Both res...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "응답 길이 비교 (완전성 대리 지표):\n",
      "1라운드: 1256.0 문자\n"
     ]
    }
   ],
   "source": [
    "# 피드백 영향 분석이 포함된 비교 데이터 추출\n",
    "comparisons = evaluation_results['comparison']\n",
    "\n",
    "# 피드백 영향 시각화를 위해 분석 결과 출력\n",
    "print(\"\\n=== 피드백 영향 분석 ===\\n\")\n",
    "for i, comparison_item in enumerate(comparisons): # comparison을 comparison_item으로 변경\n",
    "    print(f\"질의 {i+1}: {comparison_item['query']}\")\n",
    "    print(f\"\\n피드백 영향 분석:\")\n",
    "    print(comparison_item['analysis'])\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 추가적으로, 라운드 간 일부 메트릭 비교 가능\n",
    "# evaluation_results 딕셔너리 구조에 따라 round_results 접근 방식 수정 필요\n",
    "round_responses = []\n",
    "if 'round1_results' in evaluation_results:\n",
    "    round_responses.append(evaluation_results['round1_results'])\n",
    "if 'round2_results' in evaluation_results:\n",
    "    round_responses.append(evaluation_results['round2_results'])\n",
    "\n",
    "response_lengths = [[len(r[\"response\"]) for r in round_res] for round_res in round_responses]\n",
    "\n",
    "print(\"\\n응답 길이 비교 (완전성 대리 지표):\")\n",
    "avg_lengths = [sum(lengths) / len(lengths) if lengths else 0 for lengths in response_lengths]\n",
    "for round_num, avg_len in enumerate(avg_lengths, start=1):\n",
    "    print(f\"{round_num}라운드: {avg_len:.1f} 문자\")\n",
    "\n",
    "if len(avg_lengths) > 1 and avg_lengths[0] > 0: # avg_lengths[0]이 0보다 큰 경우에만 변화율 계산\n",
    "    changes = [(avg_lengths[i] - avg_lengths[i-1]) / avg_lengths[i-1] * 100 for i in range(1, len(avg_lengths))]\n",
    "    for round_num, change in enumerate(changes, start=2):\n",
    "        print(f\"{round_num-1}라운드에서 {round_num}라운드로의 변화: {change:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 11_feedback_loop_rag.ipynb]
