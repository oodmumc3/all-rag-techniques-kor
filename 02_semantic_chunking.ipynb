{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 시맨틱 청킹(Semantic Chunking) 소개\n",
    "텍스트 청킹은 검색 증강 생성(RAG)에서 필수적인 단계로, 큰 텍스트 본문을 의미 있는 세그먼트로 나누어 검색 정확도를 향상시킵니다.\n",
    "고정 길이 청킹과 달리, 시맨틱 청킹은 문장 간의 내용 유사성을 기반으로 텍스트를 분할합니다.\n",
    "\n",
    "### 분할 지점(Breakpoint) 결정 방법:\n",
    "- **백분위수(Percentile)**: 모든 유사도 차이의 X번째 백분위수를 찾아, 그 값보다 큰 차이가 나는 지점에서 청크를 분할합니다.\n",
    "- **표준편차(Standard Deviation)**: 유사도가 평균보다 X 표준편차 이상 떨어지는 지점에서 분할합니다.\n",
    "- **사분위수 범위(Interquartile Range, IQR)**: 사분위수 거리(Q3 - Q1)를 사용하여 분할 지점을 결정합니다.\n",
    "\n",
    "이 노트북은 **백분위수 방법을 사용**하여 시맨틱 청킹을 구현하고 샘플 텍스트에 대한 성능을 평가합니다.\n",
    "\n",
    "---\n",
    "#### 초보자를 위한 추가 설명\n",
    "**시맨틱 청킹(Semantic Chunking)이란?**\n",
    "단순히 글자 수나 문단 수로 텍스트를 나누는 것이 아니라, 문장들의 '의미'가 얼마나 가까운지를 파악해서 관련 있는 내용끼리 묶어주는 기술입니다. 예를 들어, AI의 역사에 대해 이야기하는 부분과 AI의 최신 기술을 설명하는 부분이 있다면, 이 두 부분을 별개의 '의미 덩어리(청크)'로 나누는 것입니다. 이렇게 하면 나중에 특정 주제에 대한 질문이 들어왔을 때 더 정확하고 관련성 높은 답변을 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz         # PyMuPDF 라이브러리, PDF 파일 처리를 위해 사용\n",
    "import os           # 운영체제와 상호작용하기 위한 라이브러리 (예: 환경 변수 접근)\n",
    "import numpy as np  # 수치 연산을 위한 라이브러리\n",
    "import json         # JSON 파일 처리를 위한 라이브러리\n",
    "from openai import OpenAI # OpenAI API 클라이언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하기 위해 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "    \n",
    "    # PDF의 각 페이지를 순회\n",
    "    for page in mypdf:\n",
    "        # 현재 페이지에서 텍스트를 추출하고 공백 추가\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # 앞뒤 공백을 제거한 추출된 텍스트 반환\n",
    "    return all_text.strip()\n",
    "\n",
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# PDF 파일에서 텍스트 추출\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 추출된 텍스트의 첫 500자 출력\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답 생성을 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 수준 임베딩 생성\n",
    "텍스트를 문장으로 나누고 각 문장에 대한 임베딩을 생성합니다.\n",
    "\n",
    "---\n",
    "#### 초보자를 위한 추가 설명\n",
    "**임베딩(Embedding)이란?**\n",
    "컴퓨터가 단어나 문장의 '의미'를 이해할 수 있도록 숫자로 이루어진 벡터(좌표)로 변환하는 과정입니다. 비슷한 의미를 가진 문장들은 벡터 공간에서 서로 가까운 위치에 있게 됩니다. 예를 들어, '고양이는 귀엽다'와 '강아지는 사랑스럽다'는 문장은 '자동차는 빠르다'는 문장보다 벡터 공간에서 더 가깝게 표현됩니다. 시맨틱 청킹은 바로 이 임베딩 벡터 간의 거리를 계산하여 의미적 유사도를 파악합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 257 sentence embeddings.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 OpenAI를 사용하여 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 입력 텍스트.\n",
    "    model (str): 임베딩 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: 임베딩 벡터.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(model=model, input=text)\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "# 텍스트를 문장으로 분할 (기본 분할)\n",
    "sentences = extracted_text.split(\". \")\n",
    "\n",
    "# 각 문장에 대한 임베딩 생성\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"총 {len(embeddings)}개의 문장 임베딩을 생성했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도 차이 계산\n",
    "연속된 문장 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "---\n",
    "#### 초보자를 위한 추가 설명\n",
    "**코사인 유사도(Cosine Similarity)란?**\n",
    "두 벡터가 얼마나 비슷한 방향을 가리키고 있는지를 측정하는 방법입니다. 값이 1에 가까울수록 두 벡터(즉, 두 문장의 의미)가 매우 유사하다는 뜻이고, 0에 가까울수록 관련성이 적다는 의미입니다. 바로 이전 문장과 다음 문장의 유사도를 계산했을 때 이 값이 갑자기 뚝 떨어진다면, 두 문장 사이에서 화제가 전환되었을 가능성이 높다고 보고 그 지점을 청크를 나누는 후보로 삼습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 첫 번째 벡터.\n",
    "    vec2 (np.ndarray): 두 번째 벡터.\n",
    "\n",
    "    Returns:\n",
    "    float: 코사인 유사도.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 연속된 문장 간의 유사도 계산\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 청킹 구현\n",
    "분할 지점을 찾기 위한 세 가지 다른 방법을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    유사도 하락을 기반으로 청킹 분할 지점을 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): 문장 간 유사도 점수 리스트.\n",
    "    method (str): 'percentile', 'standard_deviation', 또는 'interquartile'.\n",
    "    threshold (float): 임계값 (백분위수의 경우 백분위수, 표준편차의 경우 표준편차 배수).\n",
    "\n",
    "    Returns:\n",
    "    List[int]: 청크 분할이 발생해야 하는 인덱스 리스트.\n",
    "    \"\"\"\n",
    "    # 선택된 방법에 따라 임계값 결정\n",
    "    if method == \"percentile\":\n",
    "        # 유사도 점수의 X번째 백분위수 계산\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # 유사도 점수의 평균과 표준편차 계산\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # 임계값을 평균 - (X * 표준편차)로 설정\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # 1사분위수(Q1)와 3사분위수(Q3) 계산\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # IQR 규칙을 사용하여 이상치에 대한 임계값 설정\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # 잘못된 메서드가 제공되면 오류 발생\n",
    "        raise ValueError(\"잘못된 메서드입니다. 'percentile', 'standard_deviation', 'interquartile' 중에서 선택하세요.\")\n",
    "\n",
    "    # 유사도가 임계값 아래로 떨어지는 인덱스 식별\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# 백분위수 방법을 사용하여 임계값 90으로 분할 지점 계산\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트를 시맨틱 청크로 분할\n",
    "계산된 분할 지점을 기반으로 텍스트를 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 231\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings.\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    문장을 시맨틱 청크로 분할합니다.\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): 문장 리스트.\n",
    "    breakpoints (List[int]): 청킹이 발생해야 하는 인덱스 리스트.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    start = 0  # 시작 인덱스 초기화\n",
    "\n",
    "    # 각 분할 지점을 순회하며 청크 생성\n",
    "    for bp in breakpoints:\n",
    "        # 시작부터 현재 분할 지점까지의 문장들을 청크로 추가\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # 시작 인덱스를 분할 지점 다음 문장으로 업데이트\n",
    "\n",
    "    # 남은 문장들을 마지막 청크로 추가\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # 청크 리스트 반환\n",
    "\n",
    "# split_into_chunks 함수를 사용하여 청크 생성\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "# 생성된 청크 수 출력\n",
    "print(f\"시맨틱 청크 수: {len(text_chunks)}\")\n",
    "\n",
    "# 첫 번째 청크를 출력하여 결과 확인\n",
    "print(\"\n첫 번째 텍스트 청크:\")\n",
    "print(text_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 청크에 대한 임베딩 생성\n",
    "나중에 검색을 위해 각 청크에 대한 임베딩을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    각 텍스트 청크에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): 텍스트 청크 리스트.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: 임베딩 벡터 리스트.\n",
    "    \"\"\"\n",
    "    # get_embedding 함수를 사용하여 각 텍스트 청크에 대한 임베딩 생성\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\
",
    "\n",
    "# create_embeddings 함수를 사용하여 청크 임베딩 생성\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 검색 수행\n",
    "가장 관련성 높은 청크를 검색하기 위해 코사인 유사도를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    질의에 가장 관련성 높은 텍스트 청크를 찾습니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 검색 질의.\n",
    "    text_chunks (List[str]): 텍스트 청크 리스트.\n",
    "    chunk_embeddings (List[np.ndarray]): 청크 임베딩 리스트.\n",
    "    k (int): 반환할 상위 결과 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 상위 k개의 관련성 높은 청크.\n",
    "    \"\"\"\n",
    "    # 질의에 대한 임베딩 생성\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # 질의 임베딩과 각 청크 임베딩 간의 코사인 유사도 계산\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # 상위 k개의 가장 유사한 청크의 인덱스 가져오기\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # 상위 k개의 가장 관련성 높은 텍스트 청크 반환\n",
    "    return [text_chunks[i] for i in top_indices]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "\n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in \n",
      "XAI focuses on developing methods for explaining AI decisions, enhancing trust, and improving \n",
      "accountability.\n",
      "========================================\n",
      "Context 2:\n",
      "\n",
      "Transparency and Explainability \n",
      "Transparency and explainability are essential for building trust in AI systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 첫 번째 질의 추출\n",
    "query = data[0]['question']\n",
    "\n",
    "# 상위 2개의 관련성 높은 청크 가져오기\n",
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "# 질의 출력\n",
    "print(f\"질의: {query}\")\n",
    "\n",
    "# 상위 2개의 가장 관련성 높은 텍스트 청크 출력\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"컨텍스트 {i+1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 어시스턴트에 대한 시스템 프롬프트 정의\n",
    "system_prompt = \"당신은 주어진 컨텍스트를 기반으로만 엄격하게 답변하는 AI 어시스턴트입니다. 제공된 컨텍스트에서 직접적으로 답변을 도출할 수 없는 경우, '답변하기에 충분한 정보가 없습니다.'라고 응답하세요.\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    시스템 프롬프트와 사용자 메시지를 기반으로 AI 모델로부터 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): AI의 행동을 안내하는 시스템 프롬프트.\n",
    "    user_message (str): 사용자의 메시지 또는 질의.\n",
    "    model (str): 응답 생성에 사용할 모델. 기본값은 \"meta-llama/Llama-2-7B-chat-hf\".\n",
    "\n",
    "    Returns:\n",
    "    dict: AI 모델의 응답.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 상위 청크를 기반으로 사용자 프롬프트 생성\n",
    "user_prompt = \"\\n\".join([f\"컨텍스트 {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\n질문: {query}\"\n",
    "\n",
    "# AI 응답 생성\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 응답 평가\n",
    "AI 응답을 기대 답변과 비교하고 점수를 매깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the evaluation criteria, I would assign a score of 0.5 to the AI assistant's response.\n",
      "\n",
      "The response is partially aligned with the true response, as it correctly identifies the main goal of Explainable AI (XAI) as making AI systems more transparent and understandable. However, it lacks some key details and nuances present in the true response. For example, the true response mentions the importance of assessing fairness and accuracy, which is not explicitly mentioned in the AI assistant's response. Additionally, the true response uses more precise language, such as \"providing insights into how they make decisions,\" which is not present in the AI assistant's response.\n"
     ]
    }
   ],
   "source": [
    "# 평가 시스템에 대한 시스템 프롬프트 정의\n",
    "evaluate_system_prompt = \"당신은 AI 어시스턴트의 응답을 평가하는 지능형 평가 시스템입니다. AI 어시스턴트의 응답이 실제 응답과 매우 가까우면 1점을 부여하세요. 응답이 실제 응답과 관련하여 부정확하거나 만족스럽지 않으면 0점을 부여하세요. 응답이 실제 응답과 부분적으로 일치하면 0.5점을 부여하세요.\"\n",
    "\n",
    "# 사용자 질의, AI 응답, 실제 응답 및 평가 시스템 프롬프트를 결합하여 평가 프롬프트 생성\n",
    "evaluation_prompt = f\"사용자 질의: {query}\\nAI 응답:\\n{ai_response.choices[0].message.content}\\n실제 응답: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# 평가 시스템 프롬프트와 평가 프롬프트를 사용하여 평가 응답 생성\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 평가 응답 출력\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}