{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# RAG를 위한 계층적 인덱스\n",
    "\n",
    "이 노트북에서는 RAG 시스템을 위한 계층적 인덱싱 접근 방식을 구현합니다. 이 기술은 먼저 요약을 통해 관련 문서 섹션을 식별한 다음 해당 섹션에서 특정 세부 정보를 검색하는 2계층 검색 방법을 사용하여 검색을 개선합니다.\n",
    "계층적 인덱싱은 문서를 먼저 큰 단위(예: 페이지 또는 섹션)로 나누어 각 단위에 대한 요약을 생성합니다. 그런 다음, 각 큰 단위를 다시 작은 청크로 나눕니다. 검색 시에는 먼저 사용자 질의와 요약들을 비교하여 가장 관련성 높은 큰 단위를 찾고, 그 안에서만 상세 청크를 검색합니다. 이를 통해 검색 범위를 효과적으로 좁히고, 전체 문서를 대상으로 검색하는 것보다 효율적이고 정확한 결과를 얻을 수 있습니다.\n",
    "\n",
    "기존 RAG 접근 방식은 모든 텍스트 청크를 동일하게 처리하므로 다음과 같은 문제가 발생할 수 있습니다:\n",
    "\n",
    "- 청크가 너무 작으면 컨텍스트가 손실됩니다.\n",
    "- 문서 모음이 크면 관련 없는 결과가 나올 수 있습니다.\n",
    "- 전체 코퍼스에 대한 비효율적인 검색이 발생합니다.\n",
    "\n",
    "계층적 검색은 다음을 통해 이러한 문제를 해결합니다:\n",
    "\n",
    "- 더 큰 문서 섹션에 대한 간결한 요약을 생성합니다.\n",
    "- 먼저 이러한 요약을 검색하여 관련 섹션을 식별합니다.\n",
    "- 그런 다음 해당 섹션에서만 자세한 정보를 검색합니다.\n",
    "- 특정 세부 정보를 보존하면서 컨텍스트를 유지합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pickle # 객체 직렬화/역직렬화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    페이지별로 구분하여 PDF 파일에서 텍스트 내용을 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 텍스트 내용과 메타데이터를 포함하는 페이지 목록\n",
    "    \"\"\"\n",
    "    print(f\"{pdf_path}에서 텍스트 추출 중...\")  # 처리 중인 PDF 경로 출력\n",
    "    pdf_document = fitz.open(pdf_path)  # PyMuPDF를 사용하여 PDF 파일 열기\n",
    "    pages_data = []  # 텍스트 내용이 있는 페이지를 저장할 빈 리스트 초기화 (pages를 pages_data로 변경)\n",
    "    \n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page_content = pdf_document[page_num]  # 현재 페이지 가져오기 (page를 page_content로 변경)\n",
    "        text_from_page = page_content.get_text()  # 현재 페이지에서 텍스트 추출 (text를 text_from_page로 변경)\n",
    "        \n",
    "        # 텍스트가 매우 적은 페이지 건너뛰기 (50자 미만)\n",
    "        if len(text_from_page.strip()) > 50:\n",
    "            # 페이지 텍스트와 메타데이터를 리스트에 추가\n",
    "            pages_data.append({\n",
    "                \"text\": text_from_page,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # 출처 파일 경로\n",
    "                    \"page\": page_num + 1  # 페이지 번호 (1부터 시작하는 인덱스)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"내용이 있는 {len(pages_data)}페이지 추출됨\")  # 추출된 페이지 수 출력\n",
    "    return pages_data  # 텍스트 내용과 메타데이터가 있는 페이지 목록 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    메타데이터를 보존하면서 텍스트를 중첩되는 청크로 분할합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 청킹할 입력 텍스트\n",
    "        metadata (Dict): 보존할 메타데이터\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        overlap (int): 청크 간 문자 중첩\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 메타데이터를 포함하는 텍스트 청크 목록\n",
    "    \"\"\"\n",
    "    chunks_list = []  # 청크를 저장할 빈 리스트 초기화 (chunks를 chunks_list로 변경)\n",
    "    \n",
    "    # 지정된 청크 크기와 중첩으로 텍스트 반복\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text_segment = text[i:i + chunk_size]  # 텍스트 청크 추출 (chunk_text를 chunk_text_segment로 변경)\n",
    "        \n",
    "        # 매우 작은 청크 건너뛰기 (50자 미만)\n",
    "        if chunk_text_segment and len(chunk_text_segment.strip()) > 50:\n",
    "            # 메타데이터 복사본을 만들고 청크별 정보 추가\n",
    "            chunk_metadata_info = metadata.copy() # chunk_metadata를 chunk_metadata_info로 변경\n",
    "            chunk_metadata_info.update({\n",
    "                \"chunk_index\": len(chunks_list),  # 청크의 인덱스\n",
    "                \"start_char\": i,  # 청크의 시작 문자 인덱스\n",
    "                \"end_char\": i + len(chunk_text_segment),  # 청크의 끝 문자 인덱스\n",
    "                \"is_summary\": False  # 이 청크가 요약이 아님을 나타내는 플래그\n",
    "            })\n",
    "            \n",
    "            # 메타데이터와 함께 청크를 리스트에 추가\n",
    "            chunks_list.append({\n",
    "                \"text\": chunk_text_segment,\n",
    "                \"metadata\": chunk_metadata_info\n",
    "            })\n",
    "    \n",
    "    return chunks_list  # 메타데이터가 있는 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # 벡터 임베딩을 저장할 리스트\n",
    "        self.texts = []  # 텍스트 내용을 저장할 리스트\n",
    "        self.metadata = []  # 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            text (str): 텍스트 내용\n",
    "            embedding (List[float]): 벡터 임베딩\n",
    "            metadata (Dict, optional): 추가 메타데이터\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 추가\n",
    "        self.texts.append(text)  # 텍스트 내용 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터 추가 (None이면 빈 딕셔너리)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 결과 수\n",
    "            filter_func (callable, optional): 결과를 필터링하는 함수\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 가장 유사한 항목\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector_item in enumerate(self.vectors): # vector를 vector_item으로 변경\n",
    "            # 제공된 경우 필터 건너뛰기\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산\n",
    "            similarity_score = np.dot(query_vector, vector_item) / (np.linalg.norm(query_vector) * np.linalg.norm(vector_item)) # similarity를 similarity_score로 변경\n",
    "            similarities.append((i, similarity_score))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score_value = similarities[i] # score를 score_value로 변경\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 텍스트 내용 추가\n",
    "                \"metadata\": self.metadata[idx],  # 메타데이터 추가\n",
    "                \"similarity\": float(score_value)  # 유사도 점수 추가 (float으로 변환)\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 결과 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 입력 텍스트 목록\n",
    "        model (str): 임베딩 모델 이름\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 임베딩 벡터 목록\n",
    "    \"\"\"\n",
    "    # 빈 입력 처리\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # 필요한 경우 배치로 처리 (OpenAI API 제한)\n",
    "    batch_size = 100\n",
    "    all_embeddings_list = [] # all_embeddings를 all_embeddings_list로 변경\n",
    "    \n",
    "    # 입력 텍스트를 배치로 반복\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]  # 현재 텍스트 배치 가져오기 (batch를 batch_texts로 변경)\n",
    "        \n",
    "        # 현재 배치에 대한 임베딩 생성\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        # 응답에서 임베딩 추출\n",
    "        batch_embeddings_list = [item.embedding for item in response.data] # batch_embeddings를 batch_embeddings_list로 변경\n",
    "        all_embeddings_list.extend(batch_embeddings_list)  # 배치 임베딩을 리스트에 추가\n",
    "    \n",
    "    return all_embeddings_list  # 모든 임베딩 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 함수\n",
    "LLM을 사용하여 각 페이지(또는 큰 섹션)의 내용을 간결하게 요약합니다. 이 요약들은 1차 검색 대상이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_summary(page_text):\n",
    "    \"\"\"\n",
    "    페이지의 간결한 요약을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): 페이지의 텍스트 내용\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 요약\n",
    "    \"\"\"\n",
    "    # 요약 모델을 지시하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 전문 요약 시스템입니다.\n",
    "    제공된 텍스트의 상세한 요약을 만드십시오. \n",
    "    주요 주제, 핵심 정보 및 중요한 사실을 포착하는 데 중점을 두십시오.\n",
    "    요약은 페이지 내용을 이해할 수 있을 만큼 포괄적이어야 하지만 원본보다 간결해야 합니다.\"\"\"\n",
    "\n",
    "    # 최대 토큰 제한을 초과하면 입력 텍스트 자르기\n",
    "    max_tokens_limit = 6000 # max_tokens를 max_tokens_limit으로 변경\n",
    "    truncated_page_text = page_text[:max_tokens_limit] if len(page_text) > max_tokens_limit else page_text # truncated_text를 truncated_page_text로 변경\n",
    "\n",
    "    # OpenAI API에 요청하여 요약 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 사용할 모델 지정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # 어시스턴트를 안내하는 시스템 메시지\n",
    "            {\"role\": \"user\", \"content\": f\"이 텍스트를 요약하십시오:\\n\\n{truncated_page_text}\"}  # 요약할 텍스트가 포함된 사용자 메시지\n",
    "        ],\n",
    "        temperature=0.3  # 응답 생성 온도 설정 (낮을수록 결정적, 높을수록 창의적)\n",
    "    )\n",
    "    \n",
    "    # 생성된 요약 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계층적 문서 처리\n",
    "문서를 페이지별로 나누고, 각 페이지에 대한 요약을 생성합니다. 동시에 각 페이지의 텍스트를 더 작은 상세 청크로 나눕니다. 요약과 상세 청크는 각각 별도의 벡터 저장소에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    문서를 계층적 인덱스로 처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 상세 청크의 크기\n",
    "        chunk_overlap (int): 청크 간 중첩\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: 요약 및 상세 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF에서 페이지 추출\n",
    "    pages_list = extract_text_from_pdf(pdf_path) # pages를 pages_list로 변경\n",
    "    \n",
    "    # 각 페이지에 대한 요약 생성\n",
    "    print(\"페이지 요약 생성 중...\")\n",
    "    page_summaries = [] # summaries를 page_summaries로 변경\n",
    "    for i, page_item in enumerate(pages_list): # page를 page_item으로 변경\n",
    "        print(f\"페이지 {i+1}/{len(pages_list)} 요약 중...\")\n",
    "        summary_content = generate_page_summary(page_item[\"text\"]) # summary_text를 summary_content로 변경\n",
    "        \n",
    "        # 요약 메타데이터 생성\n",
    "        summary_metadata_info = page_item[\"metadata\"].copy() # summary_metadata를 summary_metadata_info로 변경\n",
    "        summary_metadata_info.update({\"is_summary\": True})\n",
    "        \n",
    "        # 요약 텍스트와 메타데이터를 summaries 리스트에 추가\n",
    "        page_summaries.append({\n",
    "            \"text\": summary_content,\n",
    "            \"metadata\": summary_metadata_info\n",
    "        })\n",
    "    \n",
    "    # 각 페이지에 대한 상세 청크 생성\n",
    "    detailed_chunk_list = [] # detailed_chunks를 detailed_chunk_list로 변경\n",
    "    for page_item in pages_list:\n",
    "        # 페이지 텍스트 청킹\n",
    "        page_chunk_list = chunk_text( # page_chunks를 page_chunk_list로 변경\n",
    "            page_item[\"text\"], \n",
    "            page_item[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # 현재 페이지의 청크를 detailed_chunks 리스트에 추가\n",
    "        detailed_chunk_list.extend(page_chunk_list)\n",
    "    \n",
    "    print(f\"생성된 상세 청크 수: {len(detailed_chunk_list)}\")\n",
    "    \n",
    "    # 요약에 대한 임베딩 생성\n",
    "    print(\"요약에 대한 임베딩 생성 중...\")\n",
    "    summary_texts_list = [summary_item[\"text\"] for summary_item in page_summaries] # summary_texts, summary를 summary_texts_list, summary_item으로 변경\n",
    "    summary_embeddings_list = create_embeddings(summary_texts_list) # summary_embeddings를 summary_embeddings_list로 변경\n",
    "    \n",
    "    # 상세 청크에 대한 임베딩 생성\n",
    "    print(\"상세 청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_texts_list = [chunk_item[\"text\"] for chunk_item in detailed_chunk_list] # chunk_texts, chunk를 chunk_texts_list, chunk_item으로 변경\n",
    "    chunk_embeddings_list = create_embeddings(chunk_texts_list) # chunk_embeddings를 chunk_embeddings_list로 변경\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    summary_vector_store = SimpleVectorStore() # summary_store를 summary_vector_store로 변경\n",
    "    detailed_vector_store = SimpleVectorStore() # detailed_store를 detailed_vector_store로 변경\n",
    "    \n",
    "    # 요약 저장소에 요약 추가\n",
    "    for i, summary_item in enumerate(page_summaries):\n",
    "        summary_vector_store.add_item(\n",
    "            text=summary_item[\"text\"],\n",
    "            embedding=summary_embeddings_list[i],\n",
    "            metadata=summary_item[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # 상세 저장소에 청크 추가\n",
    "    for i, chunk_item in enumerate(detailed_chunk_list):\n",
    "        detailed_vector_store.add_item(\n",
    "            text=chunk_item[\"text\"],\n",
    "            embedding=chunk_embeddings_list[i],\n",
    "            metadata=chunk_item[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"{len(page_summaries)}개의 요약과 {len(detailed_chunk_list)}개의 청크로 벡터 저장소 생성됨\")\n",
    "    return summary_vector_store, detailed_vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계층적 검색\n",
    "먼저 요약 저장소에서 질의와 관련된 요약들을 찾습니다. 그런 다음, 해당 요약들이 속한 페이지(또는 섹션)의 상세 청크들만을 대상으로 다시 검색하여 최종 결과를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    계층적 인덱스를 사용하여 정보를 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        summary_store (SimpleVectorStore): 문서 요약 저장소\n",
    "        detailed_store (SimpleVectorStore): 상세 청크 저장소\n",
    "        k_summaries (int): 검색할 요약 수\n",
    "        k_chunks (int): 요약당 검색할 청크 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 관련성 점수가 있는 검색된 청크\n",
    "    \"\"\"\n",
    "    print(f\"'{query}' 질의에 대한 계층적 검색 수행 중...\")\n",
    "    \n",
    "    # 질의 임베딩 생성\n",
    "    query_embedding_vector = create_embeddings(query) # query_embedding을 query_embedding_vector로 변경\n",
    "    \n",
    "    # 먼저 관련 요약 검색\n",
    "    summary_search_results = summary_store.similarity_search( # summary_results를 summary_search_results로 변경\n",
    "        query_embedding_vector, \n",
    "        k=k_summaries\n",
    "    )\n",
    "    \n",
    "    print(f\"{len(summary_search_results)}개의 관련 요약 검색됨\")\n",
    "    \n",
    "    # 관련 요약에서 페이지 수집\n",
    "    relevant_page_numbers = [result_item[\"metadata\"][\"page\"] for result_item in summary_search_results] # relevant_pages를 relevant_page_numbers로, result를 result_item으로 변경\n",
    "    \n",
    "    # 관련 페이지의 청크만 유지하는 필터 함수 생성\n",
    "    def page_filter_func(metadata_info): # page_filter를 page_filter_func로, metadata를 metadata_info로 변경\n",
    "        return metadata_info[\"page\"] in relevant_page_numbers\n",
    "    \n",
    "    # 그런 다음 해당 관련 페이지에서만 상세 청크 검색\n",
    "    detailed_search_results = detailed_store.similarity_search( # detailed_results를 detailed_search_results로 변경\n",
    "        query_embedding_vector, \n",
    "        k=k_chunks * len(relevant_page_numbers), # 각 관련 요약에 대해 k_chunks 만큼 검색\n",
    "        filter_func=page_filter_func\n",
    "    )\n",
    "    \n",
    "    print(f\"관련 페이지에서 {len(detailed_search_results)}개의 상세 청크 검색됨\")\n",
    "    \n",
    "    # 각 결과에 대해 어떤 요약/페이지에서 왔는지 추가\n",
    "    for result_item in detailed_search_results:\n",
    "        page_num_val = result_item[\"metadata\"][\"page\"] # page를 page_num_val로 변경\n",
    "        matching_summary_items = [s_item for s_item in summary_search_results if s_item[\"metadata\"][\"page\"] == page_num_val] # matching_summaries, s를 matching_summary_items, s_item으로 변경\n",
    "        if matching_summary_items:\n",
    "            result_item[\"summary\"] = matching_summary_items[0][\"text\"]\n",
    "    \n",
    "    return detailed_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컨텍스트를 사용한 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    질의와 검색된 청크를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        retrieved_chunks (List[Dict]): 계층적 검색에서 검색된 청크\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # 청크에서 텍스트 추출 및 컨텍스트 부분 준비\n",
    "    context_parts_list = [] # context_parts를 context_parts_list로 변경\n",
    "    \n",
    "    for i, chunk_item in enumerate(retrieved_chunks): # chunk를 chunk_item으로 변경\n",
    "        page_num_val = chunk_item[\"metadata\"][\"page\"]  # 메타데이터에서 페이지 번호 가져오기 (page_num을 page_num_val로 변경)\n",
    "        context_parts_list.append(f\"[페이지 {page_num_val}]: {chunk_item['text']}\")  # 페이지 번호와 함께 청크 텍스트 형식 지정\n",
    "    \n",
    "    # 모든 컨텍스트 부분을 단일 컨텍스트 문자열로 결합\n",
    "    context_text = \"\\n\\n\".join(context_parts_list) # context를 context_text로 변경\n",
    "    \n",
    "    # AI 어시스턴트를 안내하는 시스템 메시지 정의\n",
    "    system_message_text = \"\"\"당신은 제공된 컨텍스트를 기반으로 질문에 답변하는 도움이 되는 AI 어시스턴트입니다.\n",
    "    컨텍스트의 정보를 사용하여 사용자의 질문에 정확하게 답변하십시오.\n",
    "    컨텍스트에 관련 정보가 포함되어 있지 않으면 이를 인정하십시오.\n",
    "    특정 정보를 참조할 때 페이지 번호를 포함하십시오.\"\"\" # system_message를 system_message_text로 변경\n",
    "\n",
    "    # OpenAI API를 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 사용할 모델 지정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message_text},  # 어시스턴트를 안내하는 시스템 메시지\n",
    "            {\"role\": \"user\", \"content\": f\"컨텍스트:\\n\\n{context_text}\\n\\n질문: {query}\"}  # 컨텍스트와 질의가 포함된 사용자 메시지\n",
    "        ],\n",
    "        temperature=0.2  # 응답 생성 온도 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계층적 검색을 사용한 전체 RAG 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n",
    "                    k_summaries=3, k_chunks=5, regenerate=False):\n",
    "    \"\"\"\n",
    "    전체 계층적 RAG 파이프라인입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        chunk_size (int): 각 상세 청크의 크기\n",
    "        chunk_overlap (int): 청크 간 중첩\n",
    "        k_summaries (int): 검색할 요약 수\n",
    "        k_chunks (int): 요약당 검색할 청크 수\n",
    "        regenerate (bool): 벡터 저장소를 다시 생성할지 여부\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 응답 및 검색된 청크를 포함하는 결과\n",
    "    \"\"\"\n",
    "    # 캐싱을 위한 저장소 파일 이름 생성\n",
    "    summary_store_filename = f\"{os.path.basename(pdf_path)}_summary_store.pkl\" # summary_store_file을 summary_store_filename으로 변경\n",
    "    detailed_store_filename = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\" # detailed_store_file을 detailed_store_filename으로 변경\n",
    "    \n",
    "    # 필요한 경우 문서 처리 및 저장소 생성\n",
    "    if regenerate or not os.path.exists(summary_store_filename) or not os.path.exists(detailed_store_filename):\n",
    "        print(\"문서 처리 및 벡터 저장소 생성 중...\")\n",
    "        # 계층적 인덱스 및 벡터 저장소를 생성하기 위해 문서 처리\n",
    "        summary_vector_store, detailed_vector_store = process_document_hierarchically( # summary_store, detailed_store 변수명 변경\n",
    "            pdf_path, chunk_size, chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # 향후 사용을 위해 요약 저장소를 파일에 저장\n",
    "        with open(summary_store_filename, 'wb') as f_obj: # f를 f_obj로 변경\n",
    "            pickle.dump(summary_vector_store, f_obj)\n",
    "        \n",
    "        # 향후 사용을 위해 상세 저장소를 파일에 저장\n",
    "        with open(detailed_store_filename, 'wb') as f_obj:\n",
    "            pickle.dump(detailed_vector_store, f_obj)\n",
    "    else:\n",
    "        # 파일에서 기존 요약 저장소 로드\n",
    "        print(\"기존 벡터 저장소 로드 중...\")\n",
    "        with open(summary_store_filename, 'rb') as f_obj:\n",
    "            summary_vector_store = pickle.load(f_obj)\n",
    "        \n",
    "        # 파일에서 기존 상세 저장소 로드\n",
    "        with open(detailed_store_filename, 'rb') as f_obj:\n",
    "            detailed_vector_store = pickle.load(f_obj)\n",
    "    \n",
    "    # 질의를 사용하여 계층적으로 관련 청크 검색\n",
    "    retrieved_document_chunks = retrieve_hierarchically( # retrieved_chunks를 retrieved_document_chunks로 변경\n",
    "        query, summary_vector_store, detailed_vector_store, k_summaries, k_chunks\n",
    "    )\n",
    "    \n",
    "    # 검색된 청크를 기반으로 응답 생성\n",
    "    response_text = generate_response(query, retrieved_document_chunks) # response를 response_text로 변경\n",
    "    \n",
    "    # 질의, 응답, 검색된 청크 및 요약과 상세 청크 수를 포함하는 결과 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response_text,\n",
    "        \"retrieved_chunks\": retrieved_document_chunks,\n",
    "        \"summary_count\": len(summary_vector_store.texts),\n",
    "        \"detailed_count\": len(detailed_vector_store.texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교를 위한 표준 (비계층적) RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n",
    "    \"\"\"\n",
    "    계층적 검색 없는 표준 RAG 파이프라인.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        chunk_size (int): 각 청크의 크기\n",
    "        chunk_overlap (int): 청크 간 중첩\n",
    "        k (int): 검색할 청크 수\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 응답 및 검색된 청크를 포함하는 결과\n",
    "    \"\"\"\n",
    "    # PDF 문서에서 페이지 추출\n",
    "    pages_list = extract_text_from_pdf(pdf_path) # pages를 pages_list로 변경\n",
    "    \n",
    "    # 모든 페이지에서 직접 청크 생성\n",
    "    all_chunks = [] # chunks를 all_chunks로 변경\n",
    "    for page_item in pages_list: # page를 page_item으로 변경\n",
    "        # 페이지 텍스트 청킹\n",
    "        page_chunk_list = chunk_text( # page_chunks를 page_chunk_list로 변경\n",
    "            page_item[\"text\"], \n",
    "            page_item[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # 현재 페이지의 청크를 chunks 리스트에 추가\n",
    "        all_chunks.extend(page_chunk_list)\n",
    "    \n",
    "    print(f\"표준 RAG를 위해 {len(all_chunks)}개의 청크 생성됨\")\n",
    "    \n",
    "    # 청크를 담을 벡터 저장소 생성\n",
    "    vector_store_instance = SimpleVectorStore() # store를 vector_store_instance로 변경\n",
    "    \n",
    "    # 청크에 대한 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    texts_list = [chunk_item[\"text\"] for chunk_item in all_chunks] # texts, chunk를 texts_list, chunk_item으로 변경\n",
    "    embeddings_list = create_embeddings(texts_list) # embeddings를 embeddings_list로 변경\n",
    "    \n",
    "    # 벡터 저장소에 청크 추가\n",
    "    for i, chunk_item in enumerate(all_chunks):\n",
    "        vector_store_instance.add_item(\n",
    "            text=chunk_item[\"text\"],\n",
    "            embedding=embeddings_list[i],\n",
    "            metadata=chunk_item[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # 질의에 대한 임베딩 생성\n",
    "    query_embedding_vector = create_embeddings(query) # query_embedding을 query_embedding_vector로 변경\n",
    "    \n",
    "    # 질의 임베딩을 기반으로 가장 관련성 높은 청크 검색\n",
    "    retrieved_document_chunks = vector_store_instance.similarity_search(query_embedding_vector, k=k) # retrieved_chunks를 retrieved_document_chunks로 변경\n",
    "    print(f\"표준 RAG로 {len(retrieved_document_chunks)}개의 청크 검색됨\")\n",
    "    \n",
    "    # 검색된 청크를 기반으로 응답 생성\n",
    "    response_text = generate_response(query, retrieved_document_chunks) # response를 response_text로 변경\n",
    "    \n",
    "    # 질의, 응답 및 검색된 청크를 포함하는 결과 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response_text,\n",
    "        \"retrieved_chunks\": retrieved_document_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(query, pdf_path, reference_answer=None):\n",
    "    \"\"\"\n",
    "    계층적 RAG와 표준 RAG 접근 방식을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        reference_answer (str, optional): 평가를 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 비교 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== '{query}' 질의에 대한 RAG 접근 방식 비교 중 ===\")\n",
    "    \n",
    "    # 계층적 RAG 실행\n",
    "    print(\"\\n계층적 RAG 실행 중...\")\n",
    "    hierarchical_run_result = hierarchical_rag(query, pdf_path) # hierarchical_result를 hierarchical_run_result로 변경\n",
    "    hier_response_text = hierarchical_run_result[\"response\"] # hier_response를 hier_response_text로 변경\n",
    "    \n",
    "    # 표준 RAG 실행\n",
    "    print(\"\\n표준 RAG 실행 중...\")\n",
    "    standard_run_result = standard_rag(query, pdf_path) # standard_result를 standard_run_result로 변경\n",
    "    std_response_text = standard_run_result[\"response\"] # std_response를 std_response_text로 변경\n",
    "    \n",
    "    # 계층적 RAG와 표준 RAG의 결과 비교\n",
    "    comparison_analysis = compare_responses(query, hier_response_text, std_response_text, reference_answer) # comparison을 comparison_analysis로 변경\n",
    "    \n",
    "    # 비교 결과를 포함하는 딕셔너리 반환\n",
    "    return {\n",
    "        \"query\": query,  # 원본 질의\n",
    "        \"hierarchical_response\": hier_response_text,  # 계층적 RAG의 응답\n",
    "        \"standard_response\": std_response_text,  # 표준 RAG의 응답\n",
    "        \"reference_answer\": reference_answer,  # 평가를 위한 참조 답변\n",
    "        \"comparison\": comparison_analysis,  # 비교 분석\n",
    "        \"hierarchical_chunks_count\": len(hierarchical_run_result[\"retrieved_chunks\"]),  # 계층적 RAG로 검색된 청크 수\n",
    "        \"standard_chunks_count\": len(standard_run_result[\"retrieved_chunks\"])  # 표준 RAG로 검색된 청크 수\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, hierarchical_response, standard_response, reference=None):\n",
    "    \"\"\"\n",
    "    계층적 RAG와 표준 RAG의 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        hierarchical_response (str): 계층적 RAG의 응답\n",
    "        standard_response (str): 표준 RAG의 응답\n",
    "        reference (str, optional): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 비교 분석\n",
    "    \"\"\"\n",
    "    # 모델에 응답 평가 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 정보 검색 시스템의 전문 평가자입니다. \n",
    "    계층적 검색을 사용한 응답과 표준 검색을 사용한 응답이라는 동일한 질의에 대한 두 가지 응답을 비교하십시오.\n",
    "\n",
    "    다음을 기준으로 평가하십시오:\n",
    "    1. 정확성: 어떤 응답이 더 사실적으로 정확한 정보를 제공합니까?\n",
    "    2. 포괄성: 어떤 응답이 질의의 모든 측면을 더 잘 다룹니까?\n",
    "    3. 일관성: 어떤 응답이 더 논리적인 흐름과 구성을 가지고 있습니까?\n",
    "    4. 페이지 참조: 어느 응답이 페이지 참조를 더 잘 활용합니까?\n",
    "\n",
    "    각 접근 방식의 강점과 약점에 대해 구체적으로 설명하십시오.\"\"\"\n",
    "\n",
    "    # 질의와 비교할 두 응답을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "    계층적 RAG 응답:\n",
    "    {hierarchical_response}\n",
    "\n",
    "    표준 RAG 응답:\n",
    "    {standard_response}\"\"\"\n",
    "\n",
    "    # 참조 답변이 제공되면 사실 확인을 위해 사용자 프롬프트에 포함\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "    참조 답변 (사실 확인용):\n",
    "    {reference}\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트에 최종 지침 추가\n",
    "    user_prompt += \"\"\"\n",
    "\n",
    "    이 두 응답에 대한 자세한 비교를 제공하고 어떤 접근 방식이 더 나은 성과를 냈는지, 그 이유는 무엇인지 강조하십시오.\"\"\"\n",
    "\n",
    "    # OpenAI API에 요청하여 비교 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # 어시스턴트를 안내하는 시스템 메시지\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # 질의와 응답이 포함된 사용자 메시지\n",
    "        ],\n",
    "        temperature=0  # 응답 생성 온도 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 비교 분석 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    여러 테스트 질의로 전체 평가를 실행합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        test_queries (List[str]): 테스트 질의 목록\n",
    "        reference_answers (List[str], optional): 질의에 대한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 평가 결과\n",
    "    \"\"\"\n",
    "    evaluation_results_list = []  # 결과를 저장할 빈 리스트 초기화 (results를 evaluation_results_list로 변경)\n",
    "    \n",
    "    # 각 테스트 질의 반복\n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"질의: {query_text}\")  # 현재 질의 출력\n",
    "        \n",
    "        # 사용 가능한 경우 참조 답변 가져오기\n",
    "        reference_text = None # reference를 reference_text로 변경\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference_text = reference_answers[i]  # 현재 질의에 대한 참조 답변 검색\n",
    "        \n",
    "        # 계층적 RAG와 표준 RAG 접근 방식 비교\n",
    "        comparison_result = compare_approaches(query_text, pdf_path, reference_text) # result를 comparison_result로 변경\n",
    "        evaluation_results_list.append(comparison_result)  # 결과 리스트에 결과 추가\n",
    "    \n",
    "    # 평가 결과에 대한 전체 분석 생성\n",
    "    overall_analysis_text = generate_overall_analysis(evaluation_results_list) # overall_analysis를 overall_analysis_text로 변경\n",
    "    \n",
    "    return {\n",
    "        \"results\": evaluation_results_list,  # 개별 결과 반환\n",
    "        \"overall_analysis\": overall_analysis_text  # 전체 분석 반환\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    평가 결과에 대한 전체 분석을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): 개별 질의 평가 결과\n",
    "        \n",
    "    Returns:\n",
    "        str: 전체 분석\n",
    "    \"\"\"\n",
    "    # 모델에 결과 평가 방법을 지시하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 정보 검색 시스템의 전문 평가자입니다.\n",
    "여러 테스트 질의를 기반으로 계층적 RAG와 표준 RAG를 비교하는 전체 분석을 제공하십시오.\n",
    "\n",
    "다음에 초점을 맞추십시오:\n",
    "1. 계층적 검색이 더 나은 성능을 보이는 경우와 그 이유\n",
    "2. 표준 검색이 더 나은 성능을 보이는 경우와 그 이유\n",
    "3. 각 접근 방식의 전반적인 강점과 약점\n",
    "4. 각 접근 방식을 언제 사용해야 하는지에 대한 권장 사항\"\"\"\n",
    "\n",
    "    # 평가 요약 생성\n",
    "    evaluations_summary_text = \"\" # evaluations_summary를 evaluations_summary_text로 변경\n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        evaluations_summary_text += f\"질의 {i+1}: {result_item['query']}\\n\"\n",
    "        evaluations_summary_text += f\"계층적 청크: {result_item['hierarchical_chunks_count']}, 표준 청크: {result_item['standard_chunks_count']}\\n\"\n",
    "        evaluations_summary_text += f\"비교 요약: {result_item['comparison'][:200]}...\\n\\n\" # 너무 길 경우를 대비해 요약본 사용\n",
    "\n",
    "    # 평가 요약을 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"다음 {len(results)}개 질의에 대한 계층적 대 표준 RAG 비교 평가를 바탕으로, \n",
    "    이 두 접근 방식을 비교하는 전체 분석을 제공하십시오:\n",
    "\n",
    "{evaluations_summary_text}\n",
    "\n",
    "RAG 시스템에 대한 계층적 RAG와 표준 RAG의 상대적인 강점과 약점에 대한 포괄적인 분석을 제공하고,\n",
    "검색 품질 및 응답 생성에 특히 중점을 두십시오.\"\"\"\n",
    "\n",
    "    # OpenAI API에 요청하여 전체 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # 어시스턴트를 안내하는 시스템 메시지\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # 평가 요약이 포함된 사용자 메시지\n",
    "        ],\n",
    "        temperature=0  # 응답 생성 온도 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 전체 분석 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계층적 및 표준 RAG 접근 방식 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 계층적 대 표준 RAG 평가 ===\n",
      "\n",
      "\n",
      "질의 1: 자연어 처리에서 트랜스포머 모델의 주요 적용 분야는 무엇입니까?\n",
      "\n",
      "계층적 RAG 실행 중...\n",
      "문서 처리 및 벡터 저장소 생성 중...\n",
      "data/AI_Information.pdf에서 텍스트 추출 중...\n",
      "내용이 있는 15페이지 추출됨\n",
      "페이지 요약 생성 중...\n",
      "페이지 1/15 요약 중...\n",
      "페이지 2/15 요약 중...\n",
      "페이지 3/15 요약 중...\n",
      "페이지 4/15 요약 중...\n",
      "페이지 5/15 요약 중...\n",
      "페이지 6/15 요약 중...\n",
      "페이지 7/15 요약 중...\n",
      "페이지 8/15 요약 중...\n",
      "페이지 9/15 요약 중...\n",
      "페이지 10/15 요약 중...\n",
      "페이지 11/15 요약 중...\n",
      "페이지 12/15 요약 중...\n",
      "페이지 13/15 요약 중...\n",
      "페이지 14/15 요약 중...\n",
      "페이지 15/15 요약 중...\n",
      "생성된 상세 청크 수: 47\n",
      "요약에 대한 임베딩 생성 중...\n",
      "상세 청크에 대한 임베딩 생성 중...\n",
      "15개의 요약과 47개의 청크로 벡터 저장소 생성됨\n",
      "'자연어 처리에서 트랜스포머 모델의 주요 적용 분야는 무엇입니까?' 질의에 대한 계층적 검색 수행 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_15572\\2918097221.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Add the similarity score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3개의 관련 요약 검색됨\n",
      "관련 페이지에서 10개의 상세 청크 검색됨\n",
      "\n",
      "=== 응답 ===\n",
      "제공된 컨텍스트에서는 트랜스포머 모델에 대한 정보가 없습니다. 컨텍스트는 인공 지능(AI)과 기계 학습(ML)의 다양한 적용 분야, 예를 들어 컴퓨터 비전, 딥 러닝, 강화 학습 등을 주로 다루고 있습니다. 그러나 트랜스포머 모델은 언급되지 않았습니다.\n",
      "\n",
      "트랜스포머 모델에 대한 정보를 찾으신다면, 기꺼이 찾아드리겠습니다. 또는 제공된 컨텍스트를 기반으로 다른 질문이 있으시면 기꺼이 도와드리겠습니다.\n",
      "\n",
      "표준 RAG 실행 중...\n",
      "data/AI_Information.pdf에서 텍스트 추출 중...\n",
      "내용이 있는 15페이지 추출됨\n",
      "표준 RAG를 위해 47개의 청크 생성됨\n",
      "청크에 대한 임베딩 생성 중...\n",
      "표준 RAG로 15개의 청크 검색됨\n",
      "\n",
      "=== 응답 ===\n",
      "제공된 컨텍스트는 트랜스포머 모델이나 자연어 처리에서의 적용에 대해 언급하지 않습니다. 컨텍스트는 딥 러닝, 컨볼루션 신경망, 순환 신경망, 자연어 처리 및 기계 학습과 같은 다양한 주제를 다루지만 트랜스포머 모델에 대해서는 구체적으로 논의하지 않습니다.\n",
      "\n",
      "트랜스포머 모델에 대한 정보를 찾고 있다면 이 주제에 대한 일반적인 정보를 제공할 수 있습니다. 트랜스포머 모델은 기계 번역, 텍스트 생성 및 언어 이해와 같은 자연어 처리 작업에서 인기를 얻은 신경망 아키텍처 유형입니다. 순차적 데이터의 장거리 의존성을 처리하는 데 특히 효과적이며 많은 NLP 응용 프로그램에서 널리 채택되었습니다. 그러나 이 정보는 제공된 컨텍스트에 없습니다.\n",
      "\n",
      "=== 비교 ===\n",
      "**계층적 RAG 응답과 표준 RAG 응답 비교**\n",
      "\n",
      "두 응답 모두 트랜스포머 모델에 대한 정보를 제공하지 않으므로 정확성, 포괄성, 일관성 및 페이지 참조 측면에서 두 접근 방식 모두 유사합니다. 그러나 몇 가지 미묘한 차이점이 있습니다.\n",
      "\n",
      "**정확성:**\n",
      "두 응답 모두 트랜스포머 모델에 대한 정보를 제공하지 않으므로 정확성 측면에서 유사합니다.\n",
      "\n",
      "**포괄성:**\n",
      "계층적 RAG 응답은 트랜스포머 모델에 대한 정보를 제공하지 않지만, 제공된 컨텍스트가 인공 지능(AI)과 기계 학습(ML)의 다양한 적용 분야에 초점을 맞추고 있음을 명시합니다. 표준 RAG 응답은 트랜스포머 모델에 대한 정보를 제공하지 않지만, 트랜스포머 모델이 기계 번역, 텍스트 생성 및 언어 이해와 같은 자연어 처리 작업에서 인기를 얻었다고 언급합니다.\n",
      "\n",
      "**일관성:**\n",
      "두 응답 모두 일관성이 있지만, 계층적 RAG 응답은 제공된 컨텍스트가 트랜스포머 모델에 대한 정보를 제공하지 않음을 명시하므로 약간 더 일관성이 있습니다. 표준 RAG 응답은 트랜스포머 모델에 대한 정보를 제공하지 않지만, 트랜스포머 모델이 자연어 처리 작업에서 인기를 얻었다고 언급합니다.\n",
      "\n",
      "**페이지 참조:**\n",
      "두 응답 모두 페이지 참조를 제공하지 않습니다.\n",
      "\n",
      "**결론:**\n",
      "두 접근 방식 모두 트랜스포머 모델에 대한 정보를 제공하지 않으므로 정확성, 포괄성, 일관성 및 페이지 참조 측면에서 유사합니다. 그러나 계층적 RAG 응답은 제공된 컨텍스트가 트랜스포머 모델에 대한 정보를 제공하지 않음을 명시하므로 약간 더 일관성이 있습니다. 표준 RAG 응답은 트랜스포머 모델에 대한 정보를 제공하지 않지만, 트랜스포머 모델이 자연어 처리 작업에서 인기를 얻었다고 언급합니다.\n",
      "\n",
      "\n",
      "=== 전체 분석 ===\n",
      "제공된 평가를 바탕으로 계층적 RAG와 표준 RAG를 비교하는 전체 분석은 다음과 같습니다.\n",
      "\n",
      "**1. 계층적 검색이 더 나은 성능을 보이는 경우와 그 이유**\n",
      "\n",
      "이 특정 질의(\"자연어 처리에서 트랜스포머 모델의 주요 적용 분야는 무엇입니까?\")에 대해 계층적 RAG는 제공된 컨텍스트에 트랜스포머 모델에 대한 정보가 없음을 명시적으로 언급하므로 약간 더 나은 성능을 보였습니다. 이를 통해 사용자는 응답이 컨텍스트의 한계에 의해 제한됨을 이해할 수 있습니다.\n",
      "\n",
      "**2. 표준 검색이 더 나은 성능을 보이는 경우와 그 이유**\n",
      "\n",
      "표준 RAG는 컨텍스트에 트랜스포머 모델에 대한 정보가 없음을 명시적으로 언급하지는 않았지만, 트랜스포머 모델이 자연어 처리 작업에서 인기를 얻었다는 일반적인 지식을 제공했습니다. 이는 컨텍스트에 직접적으로 포함되어 있지 않더라도 사용자에게 약간의 가치를 제공할 수 있습니다.\n",
      "\n",
      "**3. 각 접근 방식의 전반적인 강점과 약점**\n",
      "\n",
      "**계층적 RAG**\n",
      "\n",
      "* **강점:**\n",
      "\t+ 컨텍스트의 한계를 명확하게 전달하여 사용자가 응답의 범위를 이해하도록 돕습니다.\n",
      "\t+ 긴 문서에서 관련 정보를 찾는 데 더 효율적일 수 있습니다.\n",
      "* **약점:**\n",
      "\t+ 컨텍스트에 없는 일반적인 지식을 제공하지 않을 수 있습니다.\n",
      "\t+ 요약 생성 및 다단계 검색으로 인해 계산적으로 더 복잡할 수 있습니다.\n",
      "\n",
      "**표준 RAG**\n",
      "\n",
      "* **강점:**\n",
      "\t+ 컨텍스트에 직접적으로 포함되어 있지 않더라도 일반적인 지식을 제공할 수 있습니다.\n",
      "\t+ 구현이 더 간단하고 계산적으로 덜 복잡할 수 있습니다.\n",
      "* **약점:**\n",
      "\t+ 컨텍스트의 한계를 명확하게 전달하지 못할 수 있습니다.\n",
      "\t+ 긴 문서에서 관련 정보를 찾는 데 덜 효율적일 수 있습니다.\n",
      "\n",
      "**4. 각 접근 방식을 언제 사용해야 하는지에 대한 권장 사항**\n",
      "\n",
      "* **계층적 RAG:**\n",
      "\t+ 긴 문서나 복잡한 정보 구조를 다룰 때 권장됩니다.\n",
      "\t+ 컨텍스트의 한계를 명확하게 전달하는 것이 중요할 때 유용합니다.\n",
      "\t+ 계산 리소스가 충분할 때 고려할 수 있습니다.\n",
      "* **표준 RAG:**\n",
      "\t+ 짧거나 간단한 문서를 다룰 때 적합합니다.\n",
      "\t+ 컨텍스트에 없는 일반적인 지식을 제공하는 것이 유용할 때 고려할 수 있습니다.\n",
      "\t+ 계산 리소스가 제한적일 때 더 나은 선택일 수 있습니다.\n",
      "\n",
      "이 특정 질의의 경우, 두 접근 방식 모두 컨텍스트에 트랜스포머 모델에 대한 정보가 없었기 때문에 제한적이었습니다. 그러나 계층적 RAG는 이러한 한계를 명확하게 전달하는 데 약간 더 나은 성능을 보였습니다.\n"
     ]
    }
   ],
   "source": [
    "# AI 정보가 포함된 PDF 문서 경로\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 계층적 RAG 접근 방식 테스트를 위한 AI 관련 예제 질의\n",
    "query = \"자연어 처리에서 트랜스포머 모델의 주요 적용 분야는 무엇입니까?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "# 계층적 RAG 시스템에서 생성된 응답 출력\n",
    "print(\"\\n=== 응답 ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# 정식 평가를 위한 테스트 질의 (요청대로 하나의 질의만 사용)\n",
    "test_queries = [\n",
    "    \"트랜스포머는 RNN과 비교하여 순차 데이터를 어떻게 처리합니까?\"\n",
    "]\n",
    "\n",
    "# 비교를 위한 테스트 질의에 대한 참조 답변\n",
    "reference_answers = [\n",
    "    \"트랜스포머는 순환 연결 대신 자기 주의 메커니즘을 사용하여 RNN과 다르게 순차 데이터를 처리합니다. 이를 통해 트랜스포머는 순차적으로가 아닌 병렬로 모든 토큰을 처리하여 장거리 의존성을 더 효율적으로 포착하고 훈련 중 병렬화를 더 잘 수행할 수 있습니다. RNN과 달리 트랜스포머는 긴 시퀀스에서 소실 그래디언트 문제로 어려움을 겪지 않습니다.\"\n",
    "]\n",
    "\n",
    "# 계층적 RAG와 표준 RAG 접근 방식 비교 평가 실행\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# 비교에 대한 전체 분석 출력\n",
    "print(\"\\n=== 전체 분석 ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 18_hierarchy_rag.ipynb]
