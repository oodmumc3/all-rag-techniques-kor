{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG 시스템을 위한 질의 변환\n",
    "\n",
    "이 노트북은 LangChain과 같은 특수 라이브러리에 의존하지 않고 RAG 시스템의 검색 성능을 향상시키기 위한 세 가지 질의 변환 기술을 구현합니다. 사용자 질의를 수정함으로써 검색된 정보의 관련성과 포괄성을 크게 향상시킬 수 있습니다.\n",
    "사용자의 원래 질문을 그대로 사용하기보다, 질문의 의도를 더 잘 파악하거나 검색 범위를 확장/축소하는 방식으로 질문을 변형시켜 더 좋은 검색 결과를 얻으려는 시도입니다.\n",
    "\n",
    "## 주요 변환 기술\n",
    "\n",
    "1. **질의 재작성 (Query Rewriting)**: 더 나은 검색 정밀도를 위해 질의를 더 구체적이고 상세하게 만듭니다. 사용자의 모호하거나 짧은 질문을 명확하고 풍부한 정보가 담긴 질문으로 바꾸어 검색 엔진이 의도를 더 잘 파악하도록 합니다.\n",
    "2. **스텝-백 프롬프팅 (Step-back Prompting)**: 유용한 문맥 정보를 검색하기 위해 더 광범위한 질의를 생성합니다. 원래 질문보다 한 단계 물러서서 더 일반적이고 상위 수준의 질문을 생성하여, 해당 주제에 대한 배경 지식이나 전반적인 문맥을 파악하는 데 도움을 줍니다.\n",
    "3. **하위 질의 분해 (Sub-query Decomposition)**: 포괄적인 검색을 위해 복잡한 질의를 더 간단한 구성 요소로 나눕니다. 하나의 복잡한 질문을 여러 개의 단순한 하위 질문으로 분해하여, 각 하위 질문에 대한 검색 결과를 종합함으로써 원래 질문에 대한 다각적이고 완전한 답변을 얻으려고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환 기술 구현\n",
    "### 1. 질의 재작성 (Query Rewriting)\n",
    "이 기술은 검색의 정밀도를 향상시키기 위해 질의를 더 구체적이고 상세하게 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    더 나은 검색을 위해 질의를 더 구체적이고 상세하게 재작성합니다.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 사용자 질의\n",
    "        model (str): 질의 재작성에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 재작성된 질의\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"당신은 검색 질의 개선에 특화된 AI 어시스턴트입니다. 당신의 임무는 사용자 질의를 더 구체적이고, 상세하며, 관련 정보를 검색할 가능성이 높도록 재작성하는 것입니다.\"\n",
    "    \n",
    "    # 재작성할 원본 질의를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 질의를 더 구체적이고 상세하게 재작성하십시오. 정확한 정보를 검색하는 데 도움이 될 수 있는 관련 용어와 개념을 포함하십시오.\n",
    "    \n",
    "    원본 질의: {original_query}\n",
    "    \n",
    "    재작성된 질의:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 재작성된 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # 결정론적 출력을 위한 낮은 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 재작성된 질의 반환 (앞뒤 공백 제거)\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 스텝-백 프롬프팅 (Step-back Prompting)\n",
    "이 기술은 문맥적 배경 정보를 검색하기 위해 더 광범위한 질의를 생성합니다.\n",
    "질문이 너무 구체적이어서 원하는 정보를 찾기 어려울 때, 한 단계 물러서서 더 일반적인 질문을 통해 관련된 배경 지식을 얻고, 이를 바탕으로 원래 질문에 대한 답을 찾는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    더 넓은 문맥을 검색하기 위해 더 일반적인 '스텝-백' 질의를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 사용자 질의\n",
    "        model (str): 스텝-백 질의 생성에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 스텝-백 질의\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"당신은 검색 전략에 특화된 AI 어시스턴트입니다. 당신의 임무는 관련 배경 정보를 검색하기 위해 특정 질의의 더 넓고 일반적인 버전을 생성하는 것입니다.\"\n",
    "    \n",
    "    # 일반화할 원본 질의를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    유용한 배경 정보를 검색하는 데 도움이 될 수 있는 다음 질의의 더 넓고 일반적인 버전을 생성하십시오.\n",
    "    \n",
    "    원본 질의: {original_query}\n",
    "    \n",
    "    스텝-백 질의:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 스텝-백 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # 약간의 변형을 위한 약간 높은 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 스텝-백 질의 반환 (앞뒤 공백 제거)\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 하위 질의 분해 (Sub-query Decomposition)\n",
    "이 기술은 포괄적인 검색을 위해 복잡한 질의를 더 간단한 구성 요소로 나눕니다.\n",
    "예를 들어, \"AI가 일자리 자동화와 고용에 미치는 영향은 무엇인가?\"라는 복잡한 질문을 \"AI로 인해 자동화될 가능성이 높은 직무는 무엇인가?\", \"AI 자동화가 새로운 일자리 창출에 미치는 영향은 무엇인가?\" 등과 같이 여러 개의 구체적인 하위 질문으로 나누어 각각 검색하고 그 결과를 종합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    복잡한 질의를 더 간단한 하위 질의로 분해합니다.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 복잡한 질의\n",
    "        num_subqueries (int): 생성할 하위 질의 수\n",
    "        model (str): 질의 분해에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 더 간단한 하위 질의 목록\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"당신은 복잡한 질문을 분해하는 데 특화된 AI 어시스턴트입니다. 당신의 임무는 복잡한 질의를 함께 답변했을 때 원본 질의를 다루는 더 간단한 하위 질문으로 분해하는 것입니다.\"\n",
    "    \n",
    "    # 분해할 원본 질의를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 복잡한 질의를 {num_subqueries}개의 더 간단한 하위 질의로 분해하십시오. 각 하위 질의는 원본 질문의 다른 측면에 초점을 맞춰야 합니다.\n",
    "    \n",
    "    원본 질의: {original_query}\n",
    "    \n",
    "    다음 형식으로 한 줄에 하나씩 {num_subqueries}개의 하위 질의를 생성하십시오:\n",
    "    1. [첫 번째 하위 질의]\n",
    "    2. [두 번째 하위 질의]\n",
    "    등등...\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 하위 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # 약간의 변형을 위한 약간 높은 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 응답을 처리하여 하위 질의 추출\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 간단한 파싱을 사용하여 번호 매기기된 질의 추출\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # 줄의 시작이 '숫자.' 패턴인지 확인 (예: '1.', '2.')\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # 숫자와 선행 공백 제거\n",
    "            query_text = line.strip()\n",
    "            query_text = query_text[query_text.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query_text)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환 기술 시연\n",
    "예제 질의에 이러한 기술을 적용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the impacts of AI on job automation and employment?\n",
      "\n",
      "1. Rewritten Query:\n",
      "Here's a rewritten query:\n",
      "\n",
      "\"What are the current and projected impacts of artificial intelligence (AI) on job automation and employment, including the types of jobs most susceptible to automation, the skills required to remain employable in an AI-driven economy, and the potential effects on unemployment rates, social welfare systems, and the gig economy?\"\n",
      "\n",
      "This rewritten query is more specific and detailed because it:\n",
      "\n",
      "1. Includes the term \"artificial intelligence\" to specify the type of technology being discussed.\n",
      "2. Mentions \"current and projected impacts\" to indicate that the query is looking for both immediate and long-term effects.\n",
      "3. Specifies the types of jobs most susceptible to automation, which can help retrieve information on the most affected industries and occupations.\n",
      "4. Includes the skills required to remain employable in an AI-driven economy, which can help retrieve information on upskilling and reskilling programs.\n",
      "5. Covers the potential effects on unemployment rates, social welfare systems, and the gig economy, which can help retrieve information on the broader societal implications of AI on employment.\n",
      "\n",
      "By including these specific terms and concepts, the rewritten query is more likely to retrieve accurate and relevant information on the topic.\n",
      "\n",
      "2. Step-back Query:\n",
      "Here's a broader, more general version of the original query:\n",
      "\n",
      "\"Effects of automation and artificial intelligence on the modern workforce and labor market, including trends, challenges, and potential implications for employment and economic growth.\"\n",
      "\n",
      "This step-back query can help retrieve relevant background information on the topic, including:\n",
      "\n",
      "* Historical context of automation and AI on employment\n",
      "* Current trends and statistics on job automation and AI adoption\n",
      "* Expert opinions and research on the impact of AI on the workforce\n",
      "* Potential solutions and strategies for mitigating the negative effects of AI on employment\n",
      "* Broader societal implications of AI on the economy and labor market.\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. What are the primary job roles that are most susceptible to automation by AI?\n",
      "   2. How does AI-driven automation affect the overall job market, including the creation of new job opportunities?\n",
      "   3. What are the potential consequences of widespread AI-driven job automation on employment rates and workforce demographics?\n",
      "   4. How do governments, industries, and individuals respond to and mitigate the impacts of AI on job automation and employment?\n"
     ]
    }
   ],
   "source": [
    "# 예제 질의\n",
    "original_query = \"AI가 일자리 자동화와 고용에 미치는 영향은 무엇인가?\"\n",
    "\n",
    "# 질의 변환 적용\n",
    "print(\"원본 질의:\", original_query)\n",
    "\n",
    "# 질의 재작성\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. 재작성된 질의:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# 스텝-백 프롬프팅\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. 스텝-백 질의:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# 하위 질의 분해\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. 하위 질의:\")\n",
    "for i, query_text in enumerate(sub_queries, 1): # 변수명을 query에서 query_text로 변경\n",
    "    print(f\"   {i}. {query_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구축\n",
    "질의 변환이 검색과 어떻게 통합되는지 보여주기 위해 간단한 벡터 저장소를 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []  # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 numpy 배열로 변환하여 벡터 리스트에 추가\n",
    "        self.texts.append(text)  # 원본 텍스트를 텍스트 리스트에 추가\n",
    "        self.metadata.append(metadata or {})  # 메타데이터를 메타데이터 리스트에 추가 (None이면 빈 딕셔너리 사용)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 질의 임베딩 벡터.\n",
    "        k (int): 반환할 결과 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트 및 메타데이터와 함께 상위 k개의 가장 유사한 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 질의 벡터와 저장된 벡터 간의 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도 기준으로 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 해당 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 해당 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 유사 항목 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    지정된 OpenAI 모델을 사용하여 주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트 (단일 문자열 또는 문자열 리스트).\n",
    "    model (str): 임베딩 생성에 사용할 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[float] 또는 List[List[float]]: 임베딩 벡터 (입력이 문자열이면 단일 벡터, 리스트면 벡터 리스트).\n",
    "    \"\"\"\n",
    "    # 문자열 입력을 리스트로 변환하여 문자열 및 리스트 입력 모두 처리\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델을 사용하여 입력 텍스트에 대한 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 그렇지 않으면 모든 임베딩을 벡터 리스트로 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환을 사용한 RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n개의 문자로 된 세그먼트로 나누고 중첩을 허용합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 각 청크의 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap) 크기의 단계로 텍스트 반복\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트 청크를 청크 목록에 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 텍스트 청크 목록 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG를 위해 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "    chunk_size (int): 각 청크의 문자 크기.\n",
    "    chunk_overlap (int): 청크 간 문자 중첩.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: 문서 청크와 해당 임베딩을 포함하는 벡터 저장소.\n",
    "    \"\"\"\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"텍스트 청킹 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "    \n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    # 효율성을 위해 모든 청크에 대한 임베딩을 한 번에 생성\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 벡터 저장소에 청크 추가\n",
    "    for i, (chunk_text_content, embedding) in enumerate(zip(chunks, chunk_embeddings)): # chunk를 chunk_text_content로 변경\n",
    "        store.add_item(\n",
    "            text=chunk_text_content,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크 추가됨\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환을 사용한 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    변환된 질의를 사용하여 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        vector_store (SimpleVectorStore): 검색할 벡터 저장소\n",
    "        transformation_type (str): 변환 유형 ('rewrite', 'step_back' 또는 'decompose')\n",
    "        top_k (int): 반환할 결과 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색 결과\n",
    "    \"\"\"\n",
    "    print(f\"변환 유형: {transformation_type}\")\n",
    "    print(f\"원본 질의: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # 질의 재작성\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"재작성된 질의: {transformed_query}\")\n",
    "        \n",
    "        # 변환된 질의에 대한 임베딩 생성\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # 재작성된 질의로 검색\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # 스텝-백 프롬프팅\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"스텝-백 질의: {transformed_query}\")\n",
    "        \n",
    "        # 변환된 질의에 대한 임베딩 생성\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # 스텝-백 질의로 검색\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # 하위 질의 분해\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"다음 하위 질의로 분해됨:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # 모든 하위 질의에 대한 임베딩 생성\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # 각 하위 질의로 검색하고 결과 결합\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # 하위 질의당 더 적은 결과 가져오기\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # 중복 제거 (가장 높은 유사도 점수 유지)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text_content = result[\"text\"] # 변수명 text에서 text_content로 변경\n",
    "            if text_content not in seen_texts or result[\"similarity\"] > seen_texts[text_content][\"similarity\"]:\n",
    "                seen_texts[text_content] = result\n",
    "        \n",
    "        # 유사도 기준으로 정렬하고 상위 top_k개 가져오기\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # 변환 없는 일반 검색\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변환된 질의를 사용하여 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    질의와 검색된 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 검색된 컨텍스트\n",
    "        model (str): 응답 생성에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"당신은 도움이 되는 AI 어시스턴트입니다. 제공된 컨텍스트만을 기반으로 사용자의 질문에 답변하십시오. 컨텍스트에서 답변을 찾을 수 없으면 정보가 충분하지 않다고 명시하십시오.\"\n",
    "    \n",
    "    # 컨텍스트와 질의를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "        컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질문: {query}\n",
    "\n",
    "        위에 제공된 컨텍스트만을 기반으로 포괄적인 답변을 제공하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # 결정론적 출력을 위한 낮은 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 반환 (앞뒤 공백 제거)\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환을 사용한 전체 RAG 파이프라인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    선택적 질의 변환을 사용하여 전체 RAG 파이프라인을 실행합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        transformation_type (str): 변환 유형 (None, 'rewrite', 'step_back' 또는 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 질의, 변환된 질의, 컨텍스트 및 응답을 포함하는 결과\n",
    "    \"\"\"\n",
    "    # 문서를 처리하여 벡터 저장소 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 질의 변환 및 검색 적용\n",
    "    if transformation_type:\n",
    "        # 변환된 질의로 검색 수행\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # 변환 없이 일반 검색 수행\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # 검색 결과에서 컨텍스트 결합\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # 질의와 결합된 컨텍스트를 기반으로 응답 생성\n",
    "    response_text = generate_response(query, context) # 변수명을 response에서 response_text로 변경\n",
    "    \n",
    "    # 원본 질의, 변환 유형, 컨텍스트 및 응답을 포함하는 결과 반환\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변환 기술 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    다양한 질의 변환 기술의 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): 다양한 변환 기술의 결과\n",
    "        reference_answer (str): 비교를 위한 참조 답변\n",
    "        model (str): 평가용 모델\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 행동을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템의 전문 평가자입니다. \n",
    "    당신의 임무는 다양한 질의 변환 기술을 사용하여 생성된 여러 응답을 비교하고 \n",
    "    참조 답변과 비교하여 어떤 기술이 가장 좋은 응답을 생성했는지 결정하는 것입니다.\"\"\"\n",
    "    \n",
    "    # 각 기술의 참조 답변과 응답을 포함하는 비교 텍스트 준비\n",
    "    comparison_text = f\"\"\"참조 답변: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} 질의 응답:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # 비교 텍스트를 포함하는 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    다양한 질의 변환 기술로 생성된 응답을 참조 답변과 비교하십시오.\n",
    "    \n",
    "    각 기술(original, rewrite, step_back, decompose)에 대해:\n",
    "    1. 정확성, 완전성 및 관련성을 기준으로 1-10점 척도로 응답 점수를 매기십시오.\n",
    "    2. 강점과 약점을 식별하십시오.\n",
    "    \n",
    "    그런 다음 기술을 최고에서 최악 순으로 순위를 매기고 전반적으로 어떤 기술이 가장 좋은 성과를 냈는지, 그 이유는 무엇인지 설명하십시오.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 평가 응답 생성\n",
    "    evaluation_response = client.chat.completions.create( # 변수명을 response에서 evaluation_response로 변경\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 평가 결과 출력\n",
    "    print(\"\\n===== 평가 결과 =====\")\n",
    "    print(evaluation_response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    동일한 질의에 대해 다양한 변환 기술을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로\n",
    "        query (str): 평가할 질의\n",
    "        reference_answer (str): 비교를 위한 선택적 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 평가 결과\n",
    "    \"\"\"\n",
    "    # 평가할 변환 기술 정의\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # 각 변환 기술로 RAG 실행\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== {type_name} 질의로 RAG 실행 중 =====\")\n",
    "        \n",
    "        # 현재 변환 유형에 대한 결과 가져오기\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # 현재 변환 유형에 대한 응답 출력\n",
    "        print(f\"{type_name} 질의 응답:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # 참조 답변이 제공되면 결과 비교\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 변환 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Response with original query:\n",
      "Explainable AI (XAI) refers to techniques used to make AI decisions more understandable and transparent. The primary goal of XAI is to enable users to assess the fairness and accuracy of AI systems. This is achieved by providing insights into how AI models make decisions, thereby enhancing trust and accountability in AI systems.\n",
      "\n",
      "XAI is considered important for several reasons. Firstly, it addresses concerns about the fairness and accuracy of AI decisions, which is crucial for ensuring that AI systems are reliable and trustworthy. Secondly, XAI helps to establish accountability and responsibility for AI systems, which is essential for addressing potential harms and ensuring ethical behavior.\n",
      "\n",
      "Furthermore, XAI is critical in the context of AI development and deployment, as it helps to mitigate the risks associated with AI systems, such as unintended consequences and potential misuse. By making AI systems more transparent and understandable, XAI can help to build trust in AI and ensure that it is developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "In the context of various domains, including environmental monitoring, AI-powered systems can be made more transparent and understandable through XAI techniques, which can provide insights into how these systems make decisions and support environmental protection efforts. Similarly, in the development of autonomous weapons systems, XAI can help to address the ethical and security concerns associated with AI-powered weapons.\n",
      "\n",
      "Overall, Explainable AI is a crucial area of research that aims to make AI systems more transparent, trustworthy, and accountable. Its importance lies in its ability to enhance trust, accountability, and responsibility in AI systems, while mitigating the risks associated with AI development and deployment.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Rewritten query: Here's a rewritten query:\n",
      "\n",
      "\"What is Explainable AI (XAI) and its significance in the context of machine learning, artificial intelligence, and data science, including its applications, benefits, and limitations, as well as the current state of research and development in this field?\"\n",
      "\n",
      "This rewritten query is more specific and detailed because it:\n",
      "\n",
      "1. Includes the term \"Explainable AI\" (XAI) to ensure the search results are focused on the specific concept.\n",
      "2. Provides context by mentioning machine learning, artificial intelligence, and data science, which are relevant fields that XAI is associated with.\n",
      "3. Specifies the applications, benefits, and limitations of XAI, which will help retrieve information that addresses the user's specific interests.\n",
      "4. Includes the current state of research and development in XAI, which will provide a more comprehensive understanding of the field.\n",
      "5. Uses relevant terms and concepts, such as \"XAI,\" \"machine learning,\" \"artificial intelligence,\" and \"data science,\" to help the search engine understand the user's intent and retrieve more accurate results.\n",
      "\n",
      "By rewriting the query in this way, the user is more likely to retrieve information that is relevant, accurate, and up-to-date.\n",
      "Response with rewrite query:\n",
      "Explainable AI (XAI) is a subfield of artificial intelligence that aims to make AI systems more transparent and understandable. The primary goal of XAI is to provide insights into how AI models make decisions, thereby enhancing trust, accountability, and fairness in AI systems.\n",
      "\n",
      "XAI techniques are being developed to explain AI decisions, making it possible for users to assess the reliability and fairness of AI systems. This is crucial in various domains, including environmental monitoring, healthcare, and finance, where AI systems are increasingly being used to make critical decisions.\n",
      "\n",
      "The importance of XAI lies in its potential to address several concerns associated with AI, such as:\n",
      "\n",
      "1. Lack of transparency: AI systems can be complex and difficult to understand, making it challenging to assess their decision-making processes.\n",
      "2. Bias and fairness: AI systems can perpetuate biases and unfairness if their decision-making processes are not transparent and explainable.\n",
      "3. Accountability: XAI can help establish accountability for AI systems, enabling developers, deployers, and users to take responsibility for their actions.\n",
      "\n",
      "By making AI systems more transparent and explainable, XAI can help build trust in AI, ensure fairness and accuracy, and mitigate potential risks associated with AI. As AI continues to advance and become more pervasive, the importance of XAI will only grow, and it is likely to play a critical role in shaping the future of AI research and development.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Step-back query: Here's a broader, more general version of the original query:\n",
      "\n",
      "\"Background information on the concept and significance of Explainable AI in the field of Artificial Intelligence.\"\n",
      "\n",
      "This step-back query can help retrieve useful background information on Explainable AI, including its definition, applications, benefits, and challenges, as well as its relevance to the broader field of Artificial Intelligence.\n",
      "Response with step_back query:\n",
      "Explainable AI (XAI) is a subfield of artificial intelligence that aims to make AI systems more transparent and understandable. The primary goal of XAI is to provide insights into how AI models make decisions, thereby enhancing trust, accountability, and fairness in AI systems.\n",
      "\n",
      "XAI techniques are being developed to explain AI decisions, making it possible for users to assess the reliability and fairness of AI systems. This is crucial in various domains, including environmental monitoring, healthcare, and finance, where AI systems are increasingly being used to make critical decisions.\n",
      "\n",
      "The importance of XAI lies in its potential to address several concerns associated with AI, such as:\n",
      "\n",
      "1. Lack of transparency: AI systems can be complex and difficult to understand, making it challenging to assess their decision-making processes.\n",
      "2. Bias and fairness: AI systems can perpetuate biases and unfairness if their decision-making processes are not transparent and explainable.\n",
      "3. Accountability: XAI can help establish accountability for AI systems, enabling developers, deployers, and users to take responsibility for their actions.\n",
      "\n",
      "By making AI systems more transparent and explainable, XAI can help build trust in AI, ensure fairness and accuracy, and mitigate potential risks associated with AI. As AI continues to advance and become more pervasive, the importance of XAI will only grow, and it is likely to play a critical role in shaping the future of AI research and development.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Decomposed into sub-queries:\n",
      "1. What is the definition of 'Explainable AI' and how does it differ from traditional machine learning models?\n",
      "2. What are the primary goals and objectives of Explainable AI, and how do they align with broader societal needs?\n",
      "3. What are the key challenges and limitations in developing and deploying Explainable AI systems, and how can they be addressed?\n",
      "4. How does the importance of Explainable AI relate to broader societal concerns, such as trust, accountability, and fairness in AI decision-making?\n",
      "Response with decompose query:\n",
      "Explainable AI (XAI) is a set of techniques aimed at making AI decisions more understandable and transparent. The primary goal of XAI is to enable users to assess the fairness and accuracy of AI systems. This is achieved by providing insights into the decision-making processes of AI systems, thereby enhancing trust and accountability.\n",
      "\n",
      "XAI is considered important for several reasons. Firstly, it addresses concerns about the fairness and accuracy of AI systems, which are crucial for building trust in AI. By making AI decisions more understandable, XAI helps users to evaluate the reliability and fairness of AI systems.\n",
      "\n",
      "Secondly, XAI is essential for ensuring accountability and responsibility in AI systems. By providing explanations for AI decisions, XAI enables developers, deployers, and users to take ownership of AI systems and address potential harms.\n",
      "\n",
      "Lastly, XAI is critical for ensuring the responsible handling of data in AI systems. By making AI systems more transparent, XAI helps to mitigate concerns about privacy and data protection, and ensures that data is handled in a way that is compliant with regulations.\n",
      "\n",
      "In summary, Explainable AI is a crucial aspect of building trust in AI, ensuring accountability and responsibility, and promoting responsible data handling. Its importance lies in its ability to make AI systems more transparent, understandable, and accountable, thereby enhancing trust and reliability in AI.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "**Comparison of Query Transformation Techniques**\n",
      "\n",
      "I will evaluate the responses generated by different query transformation techniques (original, rewrite, step_back, decompose) and compare them to the reference answer.\n",
      "\n",
      "**1. Original Query Response**\n",
      "\n",
      "* Score: 8/10\n",
      "* Strengths:\n",
      "\t+ Accurately conveys the main idea of Explainable AI (XAI)\n",
      "\t+ Provides some supporting details about the importance of XAI\n",
      "* Weaknesses:\n",
      "\t+ Lacks clarity and concision in some sentences\n",
      "\t+ Some sentences are wordy and could be rephrased for better clarity\n",
      "\n",
      "**2. Rewrite Query Response**\n",
      "\n",
      "* Score: 9/10\n",
      "* Strengths:\n",
      "\t+ More concise and clear than the original response\n",
      "\t+ Provides a clearer structure and organization\n",
      "\t+ Accurately conveys the main idea of XAI\n",
      "* Weaknesses:\n",
      "\t+ Some sentences are still wordy and could be rephrased for better clarity\n",
      "\t+ Lacks supporting details about the importance of XAI\n",
      "\n",
      "**3. Step_back Query Response**\n",
      "\n",
      "* Score: 8.5/10\n",
      "* Strengths:\n",
      "\t+ Provides a clear and concise overview of XAI\n",
      "\t+ Accurately conveys the main idea of XAI\n",
      "\t+ Lacks some supporting details about the importance of XAI\n",
      "* Weaknesses:\n",
      "\t+ Some sentences are wordy and could be rephrased for better clarity\n",
      "\t+ Lacks a clear conclusion or summary\n",
      "\n",
      "**4. Decompose Query Response**\n",
      "\n",
      "* Score: 9.5/10\n",
      "* Strengths:\n",
      "\t+ Provides a clear and concise overview of XAI\n",
      "\t+ Accurately conveys the main idea of XAI\n",
      "\t+ Provides more supporting details about the importance of XAI\n",
      "\t+ Lacks some clarity in the introduction\n",
      "* Weaknesses:\n",
      "\t+ Some sentences are wordy and could be rephrased for better clarity\n",
      "\t+ Lacks a clear conclusion or summary\n",
      "\n",
      "**Ranking of Techniques**\n",
      "\n",
      "1. Decompose Query Response (9.5/10)\n",
      "2. Rewrite Query Response (9/10)\n",
      "3. Step_back Query Response (8.5/10)\n",
      "4. Original Query Response (8/10)\n",
      "\n",
      "**Why Decompose Query Response Performed Best**\n",
      "\n",
      "The Decompose Query Response performed best overall because it provided a clear and concise overview of XAI, accurately conveying the main idea of the topic. It also provided more supporting details about the importance of XAI, which helped to strengthen the response. Additionally, the Decompose Query Response was well-organized and easy to follow, making it a pleasure to read. While the Rewrite Query Response was close in score, it lacked some supporting details about the importance of XAI, which made it slightly less effective than the Decompose Query Response.\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 첫 번째 질의 추출\n",
    "query = data[0]['question']\n",
    "\n",
    "# 검증 데이터에서 참조 답변 추출\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 07_query_transform.ipynb]
