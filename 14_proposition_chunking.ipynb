{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 향상된 RAG를 위한 명제 청킹 (Proposition Chunking)\n",
    "\n",
    "이 노트북에서는 문서를 원자적이고 사실적인 진술(명제)로 분해하여 보다 정확한 검색을 수행하는 고급 기술인 명제 청킹을 구현합니다. 단순히 문자 수로 텍스트를 나누는 기존 청킹과 달리, 명제 청킹은 개별 사실의 의미론적 무결성을 보존합니다.\n",
    "명제 청킹은 텍스트를 더 작은 단위의 '명제'로 나누는 방식입니다. 각 명제는 하나의 완전한 사실이나 주장을 담고 있어, 검색 시 사용자의 질문과 관련된 핵심 정보를 더 정확하게 찾아낼 수 있도록 돕습니다. 예를 들어, \"대한민국의 수도는 서울이고, 서울은 대한민국의 최대 도시이다.\"라는 문장이 있다면, 이를 \"대한민국의 수도는 서울이다.\"와 \"서울은 대한민국의 최대 도시이다.\"라는 두 개의 명제로 나눌 수 있습니다. 이렇게 하면 사용자가 \"대한민국의 수도는 어디인가?\"라고 질문했을 때, 첫 번째 명제가 더 직접적으로 관련되어 검색될 가능성이 높아집니다.\n",
    "\n",
    "명제 청킹은 다음을 통해 보다 정확한 검색을 제공합니다:\n",
    "\n",
    "1. 내용을 원자적이고 독립적인 사실로 분해합니다.\n",
    "2. 검색을 위해 더 작고 세분화된 단위를 만듭니다.  \n",
    "3. 질의와 관련 콘텐츠 간의 보다 정확한 일치를 가능하게 합니다.\n",
    "4. 품질이 낮거나 불완전한 명제를 필터링합니다.\n",
    "\n",
    "LangChain이나 FAISS에 의존하지 않고 완전한 구현을 구축해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import re # 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 여기서는 PyMuPDF 라이브러리를 사용하여 PDF 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출하고 처음 `num_chars`개의 문자를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지 반복\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 all_text 문자열에 추가\n",
    "\n",
    "    return all_text  # 추출된 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "추출된 텍스트가 있으면 검색 정확도를 높이기 위해 더 작고 중첩되는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=100):\n",
    "    \"\"\"\n",
    "    텍스트를 중첩되는 청크로 분할합니다.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 청킹할 입력 텍스트\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        overlap (int): 청크 간 문자 중첩\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터를 포함하는 청크 딕셔너리 목록\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # 지정된 청크 크기와 중첩으로 텍스트 반복\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_content = text[i:i + chunk_size]  # 지정된 크기의 청크 추출 (변수명 변경)\n",
    "        if chunk_content:  # 빈 청크를 추가하지 않도록 확인\n",
    "            chunks.append({\n",
    "                \"text\": chunk_content,  # 청크 텍스트\n",
    "                \"chunk_id\": len(chunks) + 1,  # 청크의 고유 ID\n",
    "                \"start_char\": i,  # 청크의 시작 문자 인덱스\n",
    "                \"end_char\": i + len(chunk_content)  # 청크의 끝 문자 인덱스\n",
    "            })\n",
    "    \n",
    "    print(f\"생성된 텍스트 청크 수: {len(chunks)}\")  # 생성된 청크 수 출력\n",
    "    return chunks  # 청크 목록 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 벡터 저장소 구현\n",
    "문서 청크와 해당 임베딩을 관리하기 위한 기본적인 벡터 저장소를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 벡터, 텍스트 및 메타데이터를 저장할 리스트 초기화\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            text (str): 텍스트 내용\n",
    "            embedding (List[float]): 임베딩 벡터\n",
    "            metadata (Dict, optional): 추가 메타데이터\n",
    "        \"\"\"\n",
    "        # 각 리스트에 임베딩, 텍스트 및 메타데이터 추가\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, texts, embeddings, metadata_list=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 여러 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): 텍스트 내용 목록\n",
    "            embeddings (List[List[float]]): 임베딩 벡터 목록\n",
    "            metadata_list (List[Dict], optional): 메타데이터 딕셔너리 목록\n",
    "        \"\"\"\n",
    "        # 메타데이터 목록이 제공되지 않으면 각 텍스트에 대해 빈 딕셔너리 생성\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{} for _ in range(len(texts))]\n",
    "        \n",
    "        # 각 텍스트, 임베딩 및 메타데이터를 저장소에 추가\n",
    "        for text_content, embedding_vector, meta_info in zip(texts, embeddings, metadata_list): # 변수명 변경\n",
    "            self.add_item(text_content, embedding_vector, meta_info)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 항목을 찾습니다.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 결과 수\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 가장 유사한 항목\n",
    "        \"\"\"\n",
    "        # 저장소에 벡터가 없으면 빈 리스트 반환\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 질의 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector_item in enumerate(self.vectors): # vector를 vector_item으로 변경\n",
    "            similarity = np.dot(query_vector, vector_item) / (np.linalg.norm(query_vector) * np.linalg.norm(vector_item))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 수집\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score_value = similarities[i] # score를 score_value로 변경\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score_value)  # JSON 직렬화를 위해 float으로 변환\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (str 또는 List[str]): 입력 텍스트\n",
    "        model (str): 임베딩 모델 이름\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 임베딩 벡터\n",
    "    \"\"\"\n",
    "    # 문자열 및 리스트 입력 모두 처리\n",
    "    input_texts_list = texts if isinstance(texts, list) else [texts] # input_texts를 input_texts_list로 변경\n",
    "    \n",
    "    # 필요한 경우 배치로 처리 (OpenAI API 제한)\n",
    "    batch_size = 100\n",
    "    all_embeddings_list = [] # all_embeddings를 all_embeddings_list로 변경\n",
    "    \n",
    "    # 입력 텍스트를 배치로 반복\n",
    "    for i in range(0, len(input_texts_list), batch_size):\n",
    "        batch_texts = input_texts_list[i:i + batch_size]  # 현재 텍스트 배치 가져오기 (batch를 batch_texts로 변경)\n",
    "        \n",
    "        # 현재 배치에 대한 임베딩 생성\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        # 응답에서 임베딩 추출\n",
    "        batch_embeddings_list = [item.embedding for item in response.data] # batch_embeddings를 batch_embeddings_list로 변경\n",
    "        all_embeddings_list.extend(batch_embeddings_list)  # 배치 임베딩을 리스트에 추가\n",
    "    \n",
    "    # 입력이 단일 문자열이었으면 첫 번째 임베딩만 반환\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings_list[0]\n",
    "    \n",
    "    # 그렇지 않으면 모든 임베딩 반환\n",
    "    return all_embeddings_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명제 생성 (Proposition Generation)\n",
    "LLM을 사용하여 텍스트 청크를 원자적이고 독립적인 사실(명제)로 분해합니다. 각 명제는 그 자체로 이해 가능해야 하며, 모호한 대명사 대신 전체 이름을 사용하고, 필요한 경우 날짜나 한정자를 포함해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_propositions(chunk):\n",
    "    \"\"\"\n",
    "    텍스트 청크에서 원자적이고 독립적인 명제를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunk (Dict): 내용과 메타데이터를 포함하는 텍스트 청크\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 생성된 명제 목록\n",
    "    \"\"\"\n",
    "    # AI에 명제 생성 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"다음 텍스트를 간단하고 독립적인 명제로 분해하십시오. \n",
    "    각 명제가 다음 기준을 충족하는지 확인하십시오:\n",
    "\n",
    "    1. 단일 사실 표현: 각 명제는 하나의 특정 사실이나 주장을 진술해야 합니다.\n",
    "    2. 컨텍스트 없이 이해 가능: 명제는 독립적이어야 하며, 추가 컨텍스트 없이 이해할 수 있어야 합니다.\n",
    "    3. 대명사 대신 전체 이름 사용: 대명사나 모호한 참조를 피하고 전체 엔티티 이름을 사용하십시오.\n",
    "    4. 관련 날짜/한정자 포함: 해당되는 경우 사실을 정확하게 만들기 위해 필요한 날짜, 시간 및 한정자를 포함하십시오.\n",
    "    5. 단일 주어-술어 관계 포함: 접속사나 여러 절 없이 단일 주어와 해당 동작 또는 속성에 초점을 맞추십시오.\n",
    "\n",
    "    추가 텍스트나 설명 없이 명제 목록만 출력하십시오.\"\"\"\n",
    "\n",
    "    # 명제로 변환할 텍스트 청크를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"명제로 변환할 텍스트:\\n\\n{chunk['text']}\"\n",
    "    \n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 정확한 명제 생성을 위해 더 강력한 모델 사용 고려\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 일관된 명제 생성을 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 응답에서 명제 추출\n",
    "    raw_propositions_text = response.choices[0].message.content.strip().split('\\n') # raw_propositions를 raw_propositions_text로 변경\n",
    "    \n",
    "    # 명제 정리 (번호 매기기, 글머리 기호 등 제거)\n",
    "    clean_propositions_list = [] # clean_propositions를 clean_propositions_list로 변경\n",
    "    for prop_text in raw_propositions_text: # prop을 prop_text로 변경\n",
    "        # 번호 매기기 (1., 2. 등) 및 글머리 기호 제거\n",
    "        cleaned_text = re.sub(r'^\\s*(\\d+\\.|\\-|\\*)\\s*', '', prop_text).strip() # cleaned를 cleaned_text로 변경\n",
    "        if cleaned_text and len(cleaned_text) > 10:  # 비어 있거나 매우 짧은 명제에 대한 간단한 필터\n",
    "            clean_propositions_list.append(cleaned_text)\n",
    "    \n",
    "    return clean_propositions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명제 품질 검사\n",
    "생성된 각 명제가 정확하고, 명확하며, 완전하고, 간결한지 LLM을 통해 평가합니다. 일정 기준 이하의 품질을 가진 명제는 필터링될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_proposition(proposition, original_text):\n",
    "    \"\"\"\n",
    "    정확성, 명확성, 완전성 및 간결성을 기준으로 명제의 품질을 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        proposition (str): 평가할 명제\n",
    "        original_text (str): 비교를 위한 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 각 평가 차원에 대한 점수\n",
    "    \"\"\"\n",
    "    # AI에 명제 평가 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 텍스트에서 추출된 명제의 품질을 평가하는 전문가입니다.\n",
    "    다음 기준(척도 1-10)에 따라 주어진 명제를 평가하십시오:\n",
    "\n",
    "    - 정확성: 명제가 원본 텍스트의 정보를 얼마나 잘 반영하는가\n",
    "    - 명확성: 추가 컨텍스트 없이 명제를 이해하기 얼마나 쉬운가\n",
    "    - 완전성: 명제가 필요한 세부 정보(날짜, 한정자 등)를 포함하는가\n",
    "    - 간결성: 명제가 중요한 정보를 잃지 않고 간결한가\n",
    "\n",
    "    응답은 각 기준에 대한 숫자 점수를 포함하는 유효한 JSON 형식이어야 합니다:\n",
    "    {\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n",
    "    \"\"\"\n",
    "\n",
    "    # 명제와 원본 텍스트를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"명제: {proposition}\n",
    "\n",
    "    원본 텍스트: {original_text}\n",
    "\n",
    "    JSON 형식으로 평가 점수를 제공하십시오.\"\"\"\n",
    "\n",
    "    # 모델에서 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}, # JSON 형식으로 응답 요청\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # JSON 응답 파싱\n",
    "    try:\n",
    "        scores_dict = json.loads(response.choices[0].message.content.strip()) # scores를 scores_dict로 변경\n",
    "        return scores_dict\n",
    "    except json.JSONDecodeError:\n",
    "        # JSON 파싱 실패 시 대체값\n",
    "        return {\n",
    "            \"accuracy\": 5,\n",
    "            \"clarity\": 5,\n",
    "            \"completeness\": 5,\n",
    "            \"conciseness\": 5\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 명제 처리 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100, \n",
    "                                      quality_thresholds=None):\n",
    "    \"\"\"\n",
    "    문서를 품질 검사된 명제로 처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 크기\n",
    "        chunk_overlap (int): 청크 간 문자 중첩\n",
    "        quality_thresholds (Dict): 명제 품질에 대한 임계값 점수\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: 원본 청크 및 명제 청크\n",
    "    \"\"\"\n",
    "    # 제공되지 않은 경우 기본 품질 임계값 설정\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = {\n",
    "            \"accuracy\": 7,\n",
    "            \"clarity\": 7,\n",
    "            \"completeness\": 7,\n",
    "            \"conciseness\": 7\n",
    "        }\n",
    "    \n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    text_content = extract_text_from_pdf(pdf_path) # text를 text_content로 변경\n",
    "    \n",
    "    # 추출된 텍스트에서 청크 생성\n",
    "    document_chunks = chunk_text(text_content, chunk_size, chunk_overlap) # chunks를 document_chunks로 변경\n",
    "    \n",
    "    # 모든 명제를 저장할 리스트 초기화\n",
    "    all_propositions_list = [] # all_propositions를 all_propositions_list로 변경\n",
    "    \n",
    "    print(\"청크에서 명제 생성 중...\")\n",
    "    for i, chunk_item in enumerate(document_chunks): # chunk를 chunk_item으로 변경\n",
    "        print(f\"청크 {i+1}/{len(document_chunks)} 처리 중...\")\n",
    "        \n",
    "        # 현재 청크에 대한 명제 생성\n",
    "        chunk_propositions_list = generate_propositions(chunk_item) # chunk_propositions를 chunk_propositions_list로 변경\n",
    "        print(f\"{len(chunk_propositions_list)}개의 명제 생성됨\")\n",
    "        \n",
    "        # 생성된 각 명제 처리\n",
    "        for prop_text in chunk_propositions_list: # prop을 prop_text로 변경\n",
    "            proposition_data_item = { # proposition_data를 proposition_data_item으로 변경\n",
    "                \"text\": prop_text,\n",
    "                \"source_chunk_id\": chunk_item[\"chunk_id\"],\n",
    "                \"source_text\": chunk_item[\"text\"]\n",
    "            }\n",
    "            all_propositions_list.append(proposition_data_item)\n",
    "    \n",
    "    # 생성된 명제의 품질 평가\n",
    "    print(\"\\n명제 품질 평가 중...\")\n",
    "    quality_propositions_list = [] # quality_propositions를 quality_propositions_list로 변경\n",
    "    \n",
    "    for i, prop_item in enumerate(all_propositions_list): # prop을 prop_item으로 변경\n",
    "        if i % 10 == 0:  # 10개 명제마다 상태 업데이트\n",
    "            print(f\"명제 {i+1}/{len(all_propositions_list)} 평가 중...\")\n",
    "            \n",
    "        # 현재 명제의 품질 평가\n",
    "        scores_dict = evaluate_proposition(prop_item[\"text\"], prop_item[\"source_text\"]) # scores를 scores_dict로 변경\n",
    "        prop_item[\"quality_scores\"] = scores_dict\n",
    "        \n",
    "        # 명제가 품질 임계값을 통과하는지 확인\n",
    "        passes_quality_check = True # passes_quality를 passes_quality_check로 변경\n",
    "        for metric_name, threshold_value in quality_thresholds.items(): # metric, threshold를 metric_name, threshold_value로 변경\n",
    "            if scores_dict.get(metric_name, 0) < threshold_value:\n",
    "                passes_quality_check = False\n",
    "                break\n",
    "        \n",
    "        if passes_quality_check:\n",
    "            quality_propositions_list.append(prop_item)\n",
    "        else:\n",
    "            print(f\"명제가 품질 검사를 통과하지 못했습니다: {prop_item['text'][:50]}...\")\n",
    "    \n",
    "    print(f\"\\n품질 필터링 후 {len(quality_propositions_list)}/{len(all_propositions_list)}개의 명제 유지됨\")\n",
    "    \n",
    "    return document_chunks, quality_propositions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 가지 접근 방식 모두에 대한 벡터 저장소 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_stores(chunks, propositions):\n",
    "    \"\"\"\n",
    "    청크 기반 및 명제 기반 접근 방식 모두에 대한 벡터 저장소를 구축합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): 원본 문서 청크\n",
    "        propositions (List[Dict]): 품질 필터링된 명제\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: 청크 및 명제 벡터 저장소\n",
    "    \"\"\"\n",
    "    # 청크용 벡터 저장소 생성\n",
    "    chunk_vector_store = SimpleVectorStore() # chunk_store를 chunk_vector_store로 변경\n",
    "    \n",
    "    # 청크 텍스트 추출 및 임베딩 생성\n",
    "    chunk_texts_list = [chunk_item[\"text\"] for chunk_item in chunks] # chunk_texts를 chunk_texts_list로, chunk를 chunk_item으로 변경\n",
    "    print(f\"{len(chunk_texts_list)}개의 청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings_list = create_embeddings(chunk_texts_list) # chunk_embeddings를 chunk_embeddings_list로 변경\n",
    "    \n",
    "    # 메타데이터와 함께 벡터 저장소에 청크 추가\n",
    "    chunk_metadata_list = [{\"chunk_id\": chunk_item[\"chunk_id\"], \"type\": \"chunk\"} for chunk_item in chunks] # chunk_metadata를 chunk_metadata_list로 변경\n",
    "    chunk_vector_store.add_items(chunk_texts_list, chunk_embeddings_list, chunk_metadata_list)\n",
    "    \n",
    "    # 명제용 벡터 저장소 생성\n",
    "    prop_vector_store = SimpleVectorStore() # prop_store를 prop_vector_store로 변경\n",
    "    \n",
    "    # 명제 텍스트 추출 및 임베딩 생성\n",
    "    prop_texts_list = [prop_item[\"text\"] for prop_item in propositions] # prop_texts를 prop_texts_list로, prop을 prop_item으로 변경\n",
    "    print(f\"{len(prop_texts_list)}개의 명제에 대한 임베딩 생성 중...\")\n",
    "    prop_embeddings_list = create_embeddings(prop_texts_list) # prop_embeddings를 prop_embeddings_list로 변경\n",
    "    \n",
    "    # 메타데이터와 함께 벡터 저장소에 명제 추가\n",
    "    prop_metadata_list = [ # prop_metadata를 prop_metadata_list로 변경\n",
    "        {\n",
    "            \"type\": \"proposition\", \n",
    "            \"source_chunk_id\": prop_item[\"source_chunk_id\"],\n",
    "            \"quality_scores\": prop_item[\"quality_scores\"]\n",
    "        } \n",
    "        for prop_item in propositions\n",
    "    ]\n",
    "    prop_vector_store.add_items(prop_texts_list, prop_embeddings_list, prop_metadata_list)\n",
    "    \n",
    "    return chunk_vector_store, prop_vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질의 및 검색 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_store(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    질의를 기반으로 벡터 저장소에서 관련 항목을 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 검색할 벡터 저장소\n",
    "        k (int): 검색할 결과 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 점수 및 메타데이터를 포함하는 검색된 항목\n",
    "    \"\"\"\n",
    "    # 질의 임베딩 생성\n",
    "    query_embedding_vector = create_embeddings(query) # query_embedding을 query_embedding_vector로 변경\n",
    "    \n",
    "    # 상위 k개의 가장 유사한 항목에 대해 벡터 저장소 검색\n",
    "    retrieved_results = vector_store.similarity_search(query_embedding_vector, k=k) # results를 retrieved_results로 변경\n",
    "    \n",
    "    return retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_approaches(query, chunk_store, prop_store, k=5):\n",
    "    \"\"\"\n",
    "    질의에 대해 청크 기반 검색과 명제 기반 검색을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        chunk_store (SimpleVectorStore): 청크 기반 벡터 저장소\n",
    "        prop_store (SimpleVectorStore): 명제 기반 벡터 저장소\n",
    "        k (int): 각 저장소에서 검색할 결과 수\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 비교 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 질의: {query} ===\")\n",
    "    \n",
    "    # 명제 기반 벡터 저장소에서 결과 검색\n",
    "    print(\"\\n명제 기반 접근 방식으로 검색 중...\")\n",
    "    prop_retrieval_results = retrieve_from_store(query, prop_store, k) # prop_results를 prop_retrieval_results로 변경\n",
    "    \n",
    "    # 청크 기반 벡터 저장소에서 결과 검색\n",
    "    print(\"청크 기반 접근 방식으로 검색 중...\")\n",
    "    chunk_retrieval_results = retrieve_from_store(query, chunk_store, k) # chunk_results를 chunk_retrieval_results로 변경\n",
    "    \n",
    "    # 명제 기반 결과 표시\n",
    "    print(\"\\n=== 명제 기반 결과 ===\")\n",
    "    for i, result_item in enumerate(prop_retrieval_results): # result를 result_item으로 변경\n",
    "        print(f\"{i+1}) {result_item['text']} (점수: {result_item['similarity']:.4f})\")\n",
    "    \n",
    "    # 청크 기반 결과 표시\n",
    "    print(\"\\n=== 청크 기반 결과 ===\")\n",
    "    for i, result_item in enumerate(chunk_retrieval_results): # result를 result_item으로 변경\n",
    "        # 출력을 관리 가능하게 유지하기 위해 텍스트 자르기\n",
    "        truncated_text_content = result_item['text'][:150] + \"...\" if len(result_item['text']) > 150 else result_item['text'] # truncated_text를 truncated_text_content로 변경\n",
    "        print(f\"{i+1}) {truncated_text_content} (점수: {result_item['similarity']:.4f})\")\n",
    "    \n",
    "    # 비교 결과 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"proposition_results\": prop_retrieval_results,\n",
    "        \"chunk_results\": chunk_retrieval_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 응답 생성 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, result_type=\"proposition\"):\n",
    "    \"\"\"\n",
    "    검색된 결과를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 검색된 항목\n",
    "        result_type (str): 결과 유형 ('proposition' 또는 'chunk')\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # 검색된 텍스트를 단일 컨텍스트 문자열로 결합\n",
    "    context_text = \"\\n\\n\".join([result_item[\"text\"] for result_item in results]) # context를 context_text로, result를 result_item으로 변경\n",
    "    \n",
    "    # AI에 응답 생성 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = f\"\"\"당신은 검색된 정보를 기반으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "당신의 답변은 지식 기반에서 검색된 다음 {result_type}을 기반으로 해야 합니다.\n",
    "검색된 정보가 질문에 답변하지 않으면 이 한계를 인정하십시오.\"\"\"\n",
    "\n",
    "    # 질의와 검색된 컨텍스트를 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "검색된 {result_type}:\n",
    "{context_text}\n",
    "\n",
    "검색된 정보를 기반으로 질의에 답변하십시오.\"\"\"\n",
    "\n",
    "    # OpenAI 클라이언트를 사용하여 응답 생성\n",
    "    api_response = client.chat.completions.create( # response를 api_response로 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2 # 약간의 창의성을 허용하면서 일관성 유지\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 텍스트 반환\n",
    "    return api_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, prop_response, chunk_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    두 가지 접근 방식의 응답을 평가하고 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        prop_response (str): 명제 기반 접근 방식의 응답\n",
    "        chunk_response (str): 청크 기반 접근 방식의 응답\n",
    "        reference_answer (str, optional): 비교를 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 평가 분석\n",
    "    \"\"\"\n",
    "    # AI에 응답 평가 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 정보 검색 시스템의 전문 평가자입니다. \n",
    "    명제 기반 검색과 청크 기반 검색이라는 두 가지 응답을 동일한 질의에 대해 비교하십시오.\n",
    "\n",
    "    다음을 기준으로 평가하십시오:\n",
    "    1. 정확성: 어떤 응답이 더 사실적으로 정확한 정보를 제공합니까?\n",
    "    2. 관련성: 어떤 응답이 특정 질의를 더 잘 다룹니까?\n",
    "    3. 간결성: 어떤 응답이 완전성을 유지하면서 더 간결합니까?\n",
    "    4. 명확성: 어떤 응답이 이해하기 더 쉽습니까?\n",
    "\n",
    "    각 접근 방식의 강점과 약점에 대해 구체적으로 설명하십시오.\"\"\"\n",
    "\n",
    "    # 질의와 비교할 응답을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"질의: {query}\n",
    "\n",
    "    명제 기반 검색 응답:\n",
    "    {prop_response}\n",
    "\n",
    "    청크 기반 검색 응답:\n",
    "    {chunk_response}\"\"\"\n",
    "\n",
    "    # 참조 답변이 제공되면 사실 확인을 위해 사용자 프롬프트에 포함\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "    참조 답변 (사실 확인용):\n",
    "    {reference_answer}\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트에 최종 지침 추가\n",
    "    user_prompt += \"\"\"\n",
    "    이 두 응답에 대한 자세한 비교를 제공하고 어떤 접근 방식이 더 나은 성과를 냈는지, 그 이유는 무엇인지 강조하십시오.\"\"\"\n",
    "\n",
    "    # OpenAI 클라이언트를 사용하여 평가 분석 생성\n",
    "    api_response = client.chat.completions.create( # response를 api_response로 변경\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 일관된 평가를 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 평가 분석 반환\n",
    "    return api_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 엔드투엔드 평가 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proposition_chunking_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    명제 청킹 대 표준 청킹에 대한 전체 평가를 실행합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        test_queries (List[str]): 테스트 질의 목록\n",
    "        reference_answers (List[str], optional): 질의에 대한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"=== 명제 청킹 평가 시작 ===\\n\")\n",
    "    \n",
    "    # 문서를 명제와 청크로 처리\n",
    "    document_chunks, quality_propositions = process_document_into_propositions(pdf_path) # chunks, propositions 변수명 변경\n",
    "    \n",
    "    # 청크와 명제에 대한 벡터 저장소 구축\n",
    "    chunk_vector_store, prop_vector_store = build_vector_stores(document_chunks, quality_propositions) # chunk_store, prop_store 변수명 변경\n",
    "    \n",
    "    # 각 질의에 대한 결과를 저장할 리스트 초기화\n",
    "    evaluation_results_list = [] # results를 evaluation_results_list로 변경\n",
    "    \n",
    "    # 각 질의에 대해 테스트 실행\n",
    "    for i, query_text in enumerate(test_queries): # query를 query_text로 변경\n",
    "        print(f\"\\n\\n=== 테스트 질의 {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"질의: {query_text}\")\n",
    "        \n",
    "        # 청크 기반 및 명제 기반 접근 방식 모두에서 검색 결과 가져오기\n",
    "        retrieval_comparison_results = compare_retrieval_approaches(query_text, chunk_vector_store, prop_vector_store) # retrieval_results를 retrieval_comparison_results로 변경\n",
    "        \n",
    "        # 검색된 명제 기반 결과로 응답 생성\n",
    "        print(\"\\n명제 기반 결과로 응답 생성 중...\")\n",
    "        prop_response_text = generate_response( # prop_response를 prop_response_text로 변경\n",
    "            query_text, \n",
    "            retrieval_comparison_results[\"proposition_results\"], \n",
    "            \"proposition\"\n",
    "        )\n",
    "        \n",
    "        # 검색된 청크 기반 결과로 응답 생성\n",
    "        print(\"청크 기반 결과로 응답 생성 중...\")\n",
    "        chunk_response_text = generate_response( # chunk_response를 chunk_response_text로 변경\n",
    "            query_text, \n",
    "            retrieval_comparison_results[\"chunk_results\"], \n",
    "            \"chunk\"\n",
    "        )\n",
    "        \n",
    "        # 사용 가능한 경우 참조 답변 가져오기\n",
    "        reference_text = None # reference를 reference_text로 변경\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference_text = reference_answers[i]\n",
    "        \n",
    "        # 생성된 응답 평가\n",
    "        print(\"\\n응답 평가 중...\")\n",
    "        evaluation_analysis = evaluate_responses(query_text, prop_response_text, chunk_response_text, reference_text) # evaluation을 evaluation_analysis로 변경\n",
    "        \n",
    "        # 현재 질의에 대한 결과 컴파일\n",
    "        query_result_data = { # query_result를 query_result_data로 변경\n",
    "            \"query\": query_text,\n",
    "            \"proposition_results\": retrieval_comparison_results[\"proposition_results\"],\n",
    "            \"chunk_results\": retrieval_comparison_results[\"chunk_results\"],\n",
    "            \"proposition_response\": prop_response_text,\n",
    "            \"chunk_response\": chunk_response_text,\n",
    "            \"reference_answer\": reference_text,\n",
    "            \"evaluation\": evaluation_analysis\n",
    "        }\n",
    "        \n",
    "        # 전체 결과 리스트에 결과 추가\n",
    "        evaluation_results_list.append(query_result_data)\n",
    "        \n",
    "        # 현재 질의에 대한 응답 및 평가 출력\n",
    "        print(\"\\n=== 명제 기반 응답 ===\")\n",
    "        print(prop_response_text)\n",
    "        \n",
    "        print(\"\\n=== 청크 기반 응답 ===\")\n",
    "        print(chunk_response_text)\n",
    "        \n",
    "        print(\"\\n=== 평가 ===\")\n",
    "        print(evaluation_analysis)\n",
    "    \n",
    "    # 평가에 대한 전체 분석 생성\n",
    "    print(\"\\n\\n=== 전체 분석 생성 중 ===\")\n",
    "    overall_analysis_text = generate_overall_analysis(evaluation_results_list) # overall_analysis를 overall_analysis_text로 변경\n",
    "    print(\"\\n\" + overall_analysis_text)\n",
    "    \n",
    "    # 평가 결과, 전체 분석 및 명제와 청크 수 반환\n",
    "    return {\n",
    "        \"results\": evaluation_results_list,\n",
    "        \"overall_analysis\": overall_analysis_text,\n",
    "        \"proposition_count\": len(quality_propositions),\n",
    "        \"chunk_count\": len(document_chunks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    명제 대 청크 접근 방식에 대한 전체 분석을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): 각 테스트 질의의 결과\n",
    "        \n",
    "    Returns:\n",
    "        str: 전체 분석\n",
    "    \"\"\"\n",
    "    # AI에 전체 분석 생성 방법을 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 정보 검색 시스템의 전문 평가자입니다.\n",
    "    여러 테스트 질의를 기반으로 명제 기반 검색과 청크 기반 검색을 RAG (검색 증강 생성) 시스템에 대해 비교하는 전체 분석을 제공하십시오.\n",
    "\n",
    "    다음에 초점을 맞추십시오:\n",
    "    1. 명제 기반 검색이 더 나은 성능을 보이는 경우\n",
    "    2. 청크 기반 검색이 더 나은 성능을 보이는 경우\n",
    "    3. 각 접근 방식의 전반적인 강점과 약점\n",
    "    4. 각 접근 방식을 언제 사용해야 하는지에 대한 권장 사항\"\"\"\n",
    "\n",
    "    # 각 질의에 대한 평가 요약 생성\n",
    "    evaluations_summary_text = \"\" # evaluations_summary를 evaluations_summary_text로 변경\n",
    "    for i, result_item in enumerate(results): # result를 result_item으로 변경\n",
    "        evaluations_summary_text += f\"질의 {i+1}: {result_item['query']}\\n\"\n",
    "        evaluations_summary_text += f\"평가 요약: {result_item['evaluation'][:200]}...\\n\\n\" # 너무 길 경우를 대비해 요약본 사용\n",
    "\n",
    "    # 평가 요약을 포함하는 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"다음 {len(results)}개 질의에 대한 명제 기반 대 청크 기반 검색 평가를 바탕으로, \n",
    "    이 두 접근 방식을 비교하는 전체 분석을 제공하십시오:\n",
    "\n",
    "{evaluations_summary_text}\n",
    "\n",
    "RAG 시스템에 대한 명제 기반 및 청크 기반 검색의 상대적인 강점과 약점에 대한 포괄적인 분석을 제공하십시오.\"\"\"\n",
    "\n",
    "    # OpenAI 클라이언트를 사용하여 전체 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0 # 일관된 분석을 위해 온도 0 설정\n",
    "    )\n",
    "    \n",
    "    # 생성된 분석 텍스트 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명제 청킹 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리할 AI 정보 문서 경로\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 명제 청킹을 평가하기 위해 AI의 다양한 측면을 다루는 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"AI 개발의 주요 윤리적 문제는 무엇입니까?\",\n",
    "    # \"설명 가능한 AI는 AI 시스템에 대한 신뢰를 어떻게 향상시킵니까?\",\n",
    "    # \"공정한 AI 시스템 개발의 주요 과제는 무엇입니까?\",\n",
    "    # \"AI 안전에서 인간 감독은 어떤 역할을 합니까?\"\n",
    "]\n",
    "\n",
    "# 보다 철저한 평가 및 결과 비교를 위한 참조 답변\n",
    "# 생성된 응답의 품질을 측정하기 위한 정답 제공\n",
    "reference_answers = [\n",
    "    \"AI 개발의 주요 윤리적 문제에는 편향과 공정성, 개인 정보 보호, 투명성, 책임성, 안전성, 그리고 오용 또는 유해한 적용 가능성이 포함됩니다.\",\n",
    "    # \"설명 가능한 AI는 AI 의사 결정 과정을 사용자에게 투명하고 이해하기 쉽게 만들어 신뢰를 향상시키며, 공정성을 확인하고 잠재적 편향을 식별하며 AI의 한계를 더 잘 이해하도록 돕습니다.\",\n",
    "    # \"공정한 AI 시스템 개발의 주요 과제에는 데이터 편향 해결, 훈련 데이터의 다양한 표현 보장, 투명한 알고리즘 생성, 다양한 컨텍스트에 걸쳐 공정성 정의, 경쟁하는 공정성 기준 균형 조정 등이 포함됩니다.\",\n",
    "    # \"인간 감독은 시스템 행동 모니터링, 결과 확인, 필요시 개입, 윤리적 경계 설정, 그리고 운영 전반에 걸쳐 AI 시스템이 인간의 가치와 의도에 부합하도록 보장함으로써 AI 안전에서 중요한 역할을 합니다.\"\n",
    "]\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = run_proposition_chunking_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# 전체 분석 출력\n",
    "print(\"\\n\\n=== 전체 분석 ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[end of 14_proposition_chunking.ipynb]
