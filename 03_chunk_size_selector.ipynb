{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 간단한 RAG에서 청크 크기 평가\n",
    "\n",
    "검색 증강 생성(RAG) 파이프라인에서 검색 정확도를 향상시키려면 적절한 청크 크기를 선택하는 것이 중요합니다. 목표는 검색 성능과 응답 품질 간의 균형을 맞추는 것입니다.\n",
    "청크 크기가 너무 작으면 문맥 정보가 부족하여 검색된 내용만으로는 충분한 답변을 생성하기 어려울 수 있습니다. 반대로 청크 크기가 너무 크면 관련 없는 정보가 많이 포함되어 답변의 정확성과 관련성이 떨어질 수 있습니다. 따라서 데이터의 특성과 사용 사례에 맞는 최적의 청크 크기를 찾는 것이 중요합니다.\n",
    "\n",
    "이 섹션에서는 다음을 통해 다양한 청크 크기를 평가합니다:\n",
    "\n",
    "1. PDF에서 텍스트를 추출합니다.\n",
    "2. 텍스트를 다양한 크기의 청크로 분할합니다.\n",
    "3. 각 청크에 대한 임베딩을 생성합니다.\n",
    "4. 질의에 대한 관련 청크를 검색합니다.\n",
    "5. 검색된 청크를 사용하여 응답을 생성합니다.\n",
    "6. 충실도(Faithfulness)와 관련성(Relevancy)을 평가합니다.\n",
    "7. 다양한 청크 크기에 대한 결과를 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져오는 것으로 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF 라이브러리\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답을 생성하기 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF에서 텍스트 추출\n",
    "먼저 `AI_Information.pdf` 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "    \n",
    "    # PDF의 각 페이지 반복\n",
    "    for page in mypdf:\n",
    "        # 현재 페이지에서 텍스트를 추출하고 공백 추가\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # 앞뒤 공백을 제거한 추출된 텍스트 반환\n",
    "    return all_text.strip()\n",
    "\n",
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# PDF 파일에서 텍스트 추출\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 추출된 텍스트의 처음 500자 출력\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹\n",
    "검색 성능을 향상시키기 위해 추출된 텍스트를 다양한 크기의 중첩되는 청크로 분할합니다.\n",
    "여기서는 여러 청크 크기(예: 128, 256, 512 문자)를 실험하여 어떤 크기가 가장 좋은 성능을 내는지 비교합니다. 중첩(overlap)은 청크 간의 문맥적 연결성을 유지하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128, Number of Chunks: 326\n",
      "Chunk Size: 256, Number of Chunks: 164\n",
      "Chunk Size: 512, Number of Chunks: 82\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    텍스트를 중첩되는 청크로 분할합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 청크당 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 목록.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 현재 인덱스부터 인덱스 + 청크 크기까지의 텍스트 청크 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  # 텍스트 청크 목록 반환\n",
    "\n",
    "# 평가할 다양한 청크 크기 정의\n",
    "chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# 각 청크 크기에 대한 텍스트 청크를 저장할 딕셔너리 생성\n",
    "text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes} # 중첩 크기는 청크 크기의 1/5로 설정\n",
    "\n",
    "# 각 청크 크기에 대해 생성된 청크 수 출력\n",
    "for size, chunks in text_chunks_dict.items():\n",
    "    print(f\"청크 크기: {size}, 청크 수: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 청크에 대한 임베딩 생성\n",
    "임베딩은 텍스트를 유사도 검색을 위한 숫자 표현으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 3/3 [00:11<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # 진행률 표시를 위한 라이브러리\n",
    "\n",
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    텍스트 목록에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    texts (List[str]): 입력 텍스트 목록.\n",
    "    model (str): 임베딩 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: 숫자 임베딩 목록.\n",
    "    \"\"\"\n",
    "    # 지정된 모델을 사용하여 임베딩 생성\n",
    "    response = client.embeddings.create(model=model, input=texts)\n",
    "    # 응답을 numpy 배열 목록으로 변환하여 반환\n",
    "    return [np.array(embedding.embedding) for embedding in response.data]\n",
    "\n",
    "# 각 청크 크기에 대한 임베딩 생성\n",
    "# text_chunks_dict의 각 청크 크기와 해당 청크에 대해 반복\n",
    "chunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"임베딩 생성 중\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 검색 수행\n",
    "사용자 질의에 가장 관련성 높은 텍스트 청크를 찾기 위해 코사인 유사도를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 첫 번째 벡터.\n",
    "    vec2 (np.ndarray): 두 번째 벡터.\n",
    "\n",
    "    Returns:\n",
    "    float: 코사인 유사도 점수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 두 벡터의 내적 계산\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    상위 k개의 가장 관련성 높은 텍스트 청크를 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "    query (str): 사용자 질의.\n",
    "    text_chunks (List[str]): 텍스트 청크 목록.\n",
    "    chunk_embeddings (List[np.ndarray]): 텍스트 청크의 임베딩.\n",
    "    k (int): 반환할 상위 청크 수.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: 가장 관련성 높은 텍스트 청크.\n",
    "    \"\"\"\n",
    "    # 질의에 대한 임베딩 생성 - 질의를 리스트로 전달하고 첫 번째 항목 가져오기\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 질의 임베딩과 각 청크 임베딩 간의 코사인 유사도 계산\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # 상위 k개의 가장 유사한 청크의 인덱스 가져오기\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # 상위 k개의 가장 관련성 높은 텍스트 청크 반환\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI enables personalized medicine by analyzing individual patient data, predicting treatment \\nresponses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \\nand reduces adverse effects. \\nRobotic Surgery \\nAI-powered robotic s', ' analyzing biological data, predicting drug \\nefficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \\nof bringing new treatments to market. \\nPersonalized Medicine \\nAI enables personalized medicine by analyzing indiv', 'g \\npatient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \\nefficiency, and patient care. \\nDrug Discovery and Development \\nAI accelerates drug discovery and development by analyzing biological data, predicting drug \\neffica', 'mains. \\nThese applications include: \\nHealthcare \\nAI is transforming healthcare through applications such as medical diagnosis, drug discovery, \\npersonalized medicine, and robotic surgery. AI-powered tools can analyze medical images, \\npredict patient outcom', 'Personalized Learning \\nAI enables personalized learning experiences by adapting to individual student needs and \\nlearning styles. AI-powered platforms provide customized content, feedback, and pacing, \\nenhancing student engagement and outcomes. \\nAdaptive A']\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 네 번째(인덱스 3) 질의 추출\n",
    "query = data[3]['question']\n",
    "\n",
    "# 각 청크 크기에 대해 관련 청크 검색\n",
    "retrieved_chunks_dict = {size: retrieve_relevant_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "# 청크 크기 256에 대해 검색된 청크 출력\n",
    "print(retrieved_chunks_dict[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 응답 생성\n",
    "청크 크기 `256`에 대해 검색된 텍스트를 기반으로 응답을 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enables personalized medicine to enhance treatment effectiveness and reduce adverse effects.\n"
     ]
    }
   ],
   "source": [
    "# AI 어시스턴트에 대한 시스템 프롬프트 정의\n",
    "system_prompt = \"당신은 주어진 문맥에 기반하여 엄격하게 답변하는 AI 어시스턴트입니다. 제공된 문맥에서 직접적으로 답변을 도출할 수 없는 경우, '답변하기에 충분한 정보가 없습니다.'라고 응답하십시오.\"\n",
    "\n",
    "def generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    검색된 청크를 기반으로 AI 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 사용자 질의.\n",
    "    retrieved_chunks (List[str]): 검색된 텍스트 청크 목록.\n",
    "    model (str): AI 모델.\n",
    "\n",
    "    Returns:\n",
    "    str: AI 생성 응답.\n",
    "    \"\"\"\n",
    "    # 검색된 청크를 단일 컨텍스트 문자열로 결합\n",
    "    context = \"\\n\".join([f\"문맥 {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"{context}\\n\\n질문: {query}\"\n",
    "\n",
    "    # 지정된 모델을 사용하여 AI 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0, # 온도를 0으로 설정하여 결정론적 답변 생성\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # AI 응답 내용 반환\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 각 청크 크기에 대한 AI 응답 생성\n",
    "ai_responses_dict = {size: generate_response(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "# 청크 크기 256에 대한 응답 출력\n",
    "print(ai_responses_dict[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 응답 평가\n",
    "강력한 LLM(거대 언어 모델)을 사용하여 충실도와 관련성을 기준으로 응답에 점수를 매깁니다.\n",
    "**충실도(Faithfulness)**: 생성된 답변이 제공된 문맥(검색된 청크)에 얼마나 충실한지를 평가합니다. 즉, 답변이 문맥에 없는 내용을 지어내거나(환각, Hallucination) 문맥과 모순되는 내용이 없는지를 확인합니다.\n",
    "**관련성(Relevancy)**: 생성된 답변이 사용자의 원래 질문에 얼마나 관련 있는지를 평가합니다. 질문의 의도를 정확히 파악하고 핵심적인 답변을 제공하는지를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 점수 시스템 상수 정의\n",
    "SCORE_FULL = 1.0     # 완전 일치 또는 매우 만족\n",
    "SCORE_PARTIAL = 0.5  # 부분 일치 또는 다소 만족\n",
    "SCORE_NONE = 0.0     # 불일치 또는 불만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엄격한 평가 프롬프트 템플릿 정의\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "실제 답변과 비교하여 AI 응답의 충실도를 평가하십시오.\n",
    "사용자 질의: {question}\n",
    "AI 응답: {response}\n",
    "실제 답변: {true_answer}\n",
    "\n",
    "충실도는 AI 응답이 환각 없이 실제 답변의 사실과 얼마나 잘 일치하는지를 측정합니다.\n",
    "\n",
    "지침:\n",
    "- 다음 값만을 사용하여 엄격하게 점수를 매기십시오:\n",
    "    * {full} = 완전히 충실하며, 실제 답변과 모순 없음\n",
    "    * {partial} = 부분적으로 충실하며, 사소한 모순 있음\n",
    "    * {none} = 충실하지 않으며, 주요 모순 또는 환각 있음\n",
    "- 설명이나 추가 텍스트 없이 숫자 점수({full}, {partial} 또는 {none})만 반환하십시오.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "사용자 질의에 대한 AI 응답의 관련성을 평가하십시오.\n",
    "사용자 질의: {question}\n",
    "AI 응답: {response}\n",
    "\n",
    "관련성은 응답이 사용자의 질문을 얼마나 잘 다루는지를 측정합니다.\n",
    "\n",
    "지침:\n",
    "- 다음 값만을 사용하여 엄격하게 점수를 매기십시오:\n",
    "    * {full} = 완전히 관련 있으며, 질의를 직접적으로 다룸\n",
    "    * {partial} = 부분적으로 관련 있으며, 일부 측면을 다룸\n",
    "    * {none} = 관련 없으며, 질의를 다루지 못함\n",
    "- 설명이나 추가 텍스트 없이 숫자 점수({full}, {partial} 또는 {none})만 반환하십시오.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score (Chunk Size 256): 0.5\n",
      "Relevancy Score (Chunk Size 256): 0.5\n",
      "\n",
      "\n",
      "Faithfulness Score (Chunk Size 128): 0.5\n",
      "Relevancy Score (Chunk Size 128): 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_response(question, response, true_answer):\n",
    "        \"\"\"\n",
    "        충실도와 관련성을 기반으로 AI 생성 응답의 품질을 평가합니다.\n",
    "\n",
    "        Args:\n",
    "        question (str): 사용자의 원본 질문.\n",
    "        response (str): 평가할 AI 생성 응답.\n",
    "        true_answer (str): 정답으로 사용되는 실제 답변.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[float, float]: (충실도_점수, 관련성_점수)를 포함하는 튜플.\n",
    "                                                각 점수는 1.0(완전), 0.5(부분), 또는 0.0(없음) 중 하나입니다.\n",
    "        \"\"\"\n",
    "        # 평가 프롬프트 포맷팅\n",
    "        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response, \n",
    "                true_answer=true_answer,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "        \n",
    "        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "\n",
    "        # 모델에 충실도 평가 요청\n",
    "        faithfulness_response = client.chat.completions.create(\n",
    "               model=\"meta-llama/Llama-3.2-3B-Instruct\", # 평가에 사용할 모델\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"당신은 객관적인 평가자입니다. 숫자 점수만 반환하십시오.\"},\n",
    "                        {\"role\": \"user\", \"content\": faithfulness_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # 모델에 관련성 평가 요청\n",
    "        relevancy_response = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.2-3B-Instruct\", # 평가에 사용할 모델\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"당신은 객관적인 평가자입니다. 숫자 점수만 반환하십시오.\"},\n",
    "                        {\"role\": \"user\", \"content\": relevancy_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # 점수 추출 및 잠재적 파싱 오류 처리\n",
    "        try:\n",
    "                faithfulness_score = float(faithfulness_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"경고: 충실도 점수를 파싱할 수 없어 기본값 0으로 설정합니다.\")\n",
    "                faithfulness_score = 0.0\n",
    "                \n",
    "        try:\n",
    "                relevancy_score = float(relevancy_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"경고: 관련성 점수를 파싱할 수 없어 기본값 0으로 설정합니다.\")\n",
    "                relevancy_score = 0.0\n",
    "\n",
    "        return faithfulness_score, relevancy_score\n",
    "\n",
    "# 네 번째(인덱스 3) 검증 데이터의 실제 답변\n",
    "true_answer = data[3]['ideal_answer']\n",
    "\n",
    "# 청크 크기 256 및 128에 대한 응답 평가\n",
    "faithfulness, relevancy = evaluate_response(query, ai_responses_dict[256], true_answer)\n",
    "faithfulness2, relevancy2 = evaluate_response(query, ai_responses_dict[128], true_answer)\n",
    "\n",
    "# 평가 점수 출력\n",
    "print(f\"충실도 점수 (청크 크기 256): {faithfulness}\")\n",
    "print(f\"관련성 점수 (청크 크기 256): {relevancy}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"충실도 점수 (청크 크기 128): {faithfulness2}\")\n",
    "print(f\"관련성 점수 (청크 크기 128): {relevancy2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
