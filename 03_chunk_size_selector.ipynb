{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 간단한 RAG에서 청크 크기 평가하기\n",
    "\n",
    "검색 증강 생성(RAG) 파이프라인에서 올바른 청크 크기를 선택하는 것은 검색 정확도를 향상시키는 데 매우 중요합니다. 목표는 검색 성능과 응답 품질 사이의 균형을 맞추는 것입니다.\n",
    "\n",
    "이 섹션에서는 다음과 같은 방법으로 다양한 청크 크기를 평가합니다:\n",
    "\n",
    "1. PDF에서 텍스트 추출하기.\n",
    "2. 텍스트를 다양한 크기의 청크로 분할하기.\n",
    "3. 각 청크에 대한 임베딩 생성하기.\n",
    "4. 질의에 대한 관련 청크 검색하기.\n",
    "5. 검색된 청크를 사용하여 응답 생성하기.\n",
    "6. 충실도(Faithfulness)와 관련성(Relevancy) 평가하기.\n",
    "7. 다양한 청크 크기에 대한 결과 비교하기.\n",
    "\n",
    "---",
    "#### 초보자를 위한 추가 설명\n",
    "**청크 크기는 왜 중요할까요?**\n",
    "청크 크기는 RAG 시스템의 성능에 직접적인 영향을 미치는 중요한 요소입니다. 어떤 크기가 가장 좋은지는 정해져 있지 않으며, 데이터의 종류와 사용자의 질문 유형에 따라 달라집니다.\n",
    "*   **작은 청크 (예: 128)**: \n",
    "    *   **장점**: 매우 구체적이고 집중된 정보를 담고 있어, 특정 사실(Fact)에 대한 질문에 답할 때 정확도가 높을 수 있습니다. 관련 없는 내용이 섞일 가능성이 적습니다.\n",
    "    *   **단점**: 전체적인 맥락을 잃어버리기 쉽습니다. 여러 문장에 걸쳐 설명되는 복잡한 개념을 이해하기 어려울 수 있습니다.\n",
    "*   **큰 청크 (예: 512)**: \n",
    "    *   **장점**: 더 넓은 맥락을 포함하므로, 요약이나 복잡한 주제에 대한 질문에 더 나은 답변을 제공할 수 있습니다.\n",
    "    *   **단점**: 관련 없는 정보(노이즈)가 포함될 가능성이 커져, 오히려 LLM이 정확한 답변을 생성하는 데 방해가 될 수 있습니다.\n",
    "\n",
    "이 노트북에서는 다양한 크기의 청크를 만들어보고, 어떤 크기가 주어진 질문에 대해 가장 '충실하고' '관련성 높은' 답변을 만드는지 실험해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz         # PyMuPDF 라이브러리, PDF 파일 처리를 위해 사용\n",
    "import os           # 운영체제와 상호작용하기 위한 라이브러리 (예: 환경 변수 접근)\n",
    "import numpy as np  # 수치 연산을 위한 라이브러리\n",
    "import json         # JSON 파일 처리를 위한 라이브러리\n",
    "from openai import OpenAI # OpenAI API 클라이언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API 클라이언트 설정\n",
    "임베딩과 응답 생성을 위해 OpenAI 클라이언트를 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 URL과 API 키로 OpenAI 클라이언트 초기화\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 환경 변수에서 API 키 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF에서 텍스트 추출\n",
    "먼저 `AI_Information.pdf` 파일에서 텍스트를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "    str: PDF에서 추출된 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 빈 문자열 초기화\n",
    "    \n",
    "    # PDF의 각 페이지를 순회\n",
    "    for page in mypdf:\n",
    "        # 현재 페이지에서 텍스트를 추출하고 공백 추가\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # 앞뒤 공백을 제거한 추출된 텍스트 반환\n",
    "    return all_text.strip()\n",
    "\n",
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# PDF 파일에서 텍스트 추출\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 추출된 텍스트의 첫 500자 출력\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청킹하기\n",
    "검색 성능을 향상시키기 위해, 추출된 텍스트를 다양한 크기의 중첩된 청크로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청크 크기: 128, 청크 수: 326\n",
      "청크 크기: 256, 청크 수: 164\n",
      "청크 크기: 512, 청크 수: 82\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    텍스트를 중첩된 청크로 분할합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청킹할 텍스트.\n",
    "    n (int): 청크당 문자 수.\n",
    "    overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 현재 인덱스부터 인덱스 + 청크 크기까지의 텍스트 청크를 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  # 텍스트 청크 리스트 반환\n",
    "\n",
    "# 평가할 다양한 청크 크기 정의\n",
    "chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# 각 청크 크기별 텍스트 청크를 저장할 딕셔너리 생성\n",
    "text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n",
    "\n",
    "# 각 청크 크기별로 생성된 청크 수 출력\n",
    "for size, chunks in text_chunks_dict.items():\n",
    "    print(f\"청크 크기: {size}, 청크 수: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 청크에 대한 임베딩 생성\n",
    "임베딩은 유사도 검색을 위해 텍스트를 숫자 표현으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "임베딩 생성 중: 100%|██████████| 3/3 [00:11<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    텍스트 리스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    texts (List[str]): 입력 텍스트 리스트.\n",
    "    model (str): 임베딩 모델.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: 숫자 임베딩 리스트.\n",
    "    \"\"\"\n",
    "    # 지정된 모델을 사용하여 임베딩 생성\n",
    "    response = client.embeddings.create(model=model, input=texts)\n",
    "    # 응답을 numpy 배열 리스트로 변환하여 반환\n",
    "    return [np.array(embedding.embedding) for embedding in response.data]\n",
    "\n",
    "# 각 청크 크기별 임베딩 생성\n",
    "# text_chunks_dict의 각 청크 크기와 해당 청크들을 순회\n",
    "chunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"임베딩 생성 중\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시맨틱 검색 수행\n",
    "사용자 질의에 가장 관련성 높은 텍스트 청크를 찾기 위해 코사인 유사도를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 첫 번째 벡터.\n",
    "    vec2 (np.ndarray): 두 번째 벡터.\n",
    "\n",
    "    Returns:\n",
    "    float: 코사인 유사도 점수.\n",
    "    \"\"\"\n",
    "\n",
    "    # 두 벡터의 내적 계산\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    상위 k개의 가장 관련성 높은 텍스트 청크를 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "    query (str): 사용자 질의.\n",
    "    text_chunks (List[str]): 텍스트 청크 리스트.\n",
    "    chunk_embeddings (List[np.ndarray]): 텍스트 청크의 임베딩.\n",
    "    k (int): 반환할 상위 청크 수.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: 가장 관련성 높은 텍스트 청크.\n",
    "    \"\"\"\n",
    "    # 질의에 대한 임베딩 생성 - 질의를 리스트로 전달하고 첫 번째 항목을 가져옴\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 질의 임베딩과 각 청크 임베딩 간의 코사인 유사도 계산\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # 상위 k개의 가장 유사한 청크의 인덱스 가져오기\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # 상위 k개의 가장 관련성 높은 텍스트 청크 반환\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI enables personalized medicine by analyzing individual patient data, predicting treatment \\nresponses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \\nand reduces adverse effects. \\nRobotic Surgery \\nAI-powered robotic s', ' analyzing biological data, predicting drug \\nefficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \\nof bringing new treatments to market. \\nPersonalized Medicine \\nAI enables personalized medicine by analyzing indiv', 'g \\npatient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \\nefficiency, and patient care. \\nDrug Discovery and Development \\nAI accelerates drug discovery and development by analyzing biological data, predicting drug \\neffica', 'mains. \\nThese applications include: \\nHealthcare \\nAI is transforming healthcare through applications such as medical diagnosis, drug discovery, \\npersonalized medicine, and robotic surgery. AI-powered tools can analyze medical images, \\npredict patient outcom', 'Personalized Learning \\nAI enables personalized learning experiences by adapting to individual student needs and \\nlearning styles. AI-powered platforms provide customized content, feedback, and pacing, \\nenhancing student engagement and outcomes. \\nAdaptive A']\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 검증 데이터 로드\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 검증 데이터에서 네 번째 질문 추출\n",
    "query = data[3]['question']\n",
    "\n",
    "# 각 청크 크기별로 관련 청크 검색\n",
    "retrieved_chunks_dict = {size: retrieve_relevant_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "# 청크 크기 256에 대해 검색된 청크 출력\n",
    "print(retrieved_chunks_dict[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 응답 생성\n",
    "청크 크기 `256`에 대해 검색된 텍스트를 기반으로 응답을 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enables personalized medicine to enhance treatment effectiveness and reduce adverse effects.\n"
     ]
    }
   ],
   "source": [
    "# AI 어시스턴트에 대한 시스템 프롬프트 정의\n",
    "system_prompt = \"당신은 주어진 컨텍스트를 기반으로만 엄격하게 답변하는 AI 어시스턴트입니다. 제공된 컨텍스트에서 직접적으로 답변을 도출할 수 없는 경우, '답변하기에 충분한 정보가 없습니다.'라고 응답하세요.\"\n",
    "\n",
    "def generate_response(query, system_prompt, retrieved_chunks, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    검색된 청크를 기반으로 AI 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 사용자 질의.\n",
    "    retrieved_chunks (List[str]): 검색된 텍스트 청크 리스트.\n",
    "    model (str): AI 모델.\n",
    "\n",
    "    Returns:\n",
    "    str: AI가 생성한 응답.\n",
    "    \"\"\"\n",
    "    # 검색된 청크를 단일 컨텍스트 문자열로 결합\n",
    "    context = \"\\n\".join([f\"컨텍스트 {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
    "    \n",
    "    # 컨텍스트와 질의를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"{context}\\n\\n질문: {query}\"\n",
    "\n",
    "    # 지정된 모델을 사용하여 AI 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # AI 응답의 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI 응답 평가\n",
    "강력한 LLM을 사용하여 충실도와 관련성을 기준으로 응답에 점수를 매깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 점수 시스템 상수 정의\n",
    "SCORE_FULL = 1.0     # 완전 일치 또는 완전히 만족\n",
    "SCORE_PARTIAL = 0.5  # 부분 일치 또는 다소 만족\n",
    "SCORE_NONE = 0.0     # 불일치 또는 불만족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엄격한 평가 프롬프트 템플릿 정의\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "AI 응답이 실제 답변과 비교하여 얼마나 충실한지 평가하세요.\n",
    "사용자 질의: {question}\n",
    "AI 응답: {response}\n",
    "실제 답변: {true_answer}\n",
    "\n",
    "충실도는 AI 응답이 환각(hallucination) 없이 실제 답변의 사실과 얼마나 잘 일치하는지를 측정합니다.\n",
    "\n",
    "지침:\n",
    "- 다음 값만을 사용하여 엄격하게 점수를 매기세요:\n",
    "    * {full} = 완전히 충실함, 실제 답변과 모순 없음\n",
    "    * {partial} = 부분적으로 충실함, 사소한 모순 있음\n",
    "    * {none} = 충실하지 않음, 주요 모순 또는 환각 있음\n",
    "- 설명이나 추가 텍스트 없이 숫자 점수({full}, {partial}, 또는 {none})만 반환하세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "사용자 질의에 대한 AI 응답의 관련성을 평가하세요.\n",
    "사용자 질의: {question}\n",
    "AI 응답: {response}\n",
    "\n",
    "관련성은 응답이 사용자의 질문을 얼마나 잘 다루는지를 측정합니다.\n",
    "\n",
    "지침:\n",
    "- 다음 값만을 사용하여 엄격하게 점수를 매기세요:\n",
    "    * {full} = 완전히 관련성 높음, 질의를 직접적으로 다룸\n",
    "    * {partial} = 부분적으로 관련성 있음, 일부 측면을 다룸\n",
    "    * {none} = 관련성 없음, 질의를 다루지 못함\n",
    "- 설명이나 추가 텍스트 없이 숫자 점수({full}, {partial}, 또는 {none})만 반환하세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "충실도 점수 (청크 크기 256): 0.5\n",
      "관련성 점수 (청크 크기 256): 0.5\n",
      "\n",
      "\n",
      "충실도 점수 (청크 크기 128): 0.5\n",
      "관련성 점수 (청크 크기 128): 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_response(question, response, true_answer):\n",
    "        \"\"\"\n",
    "        충실도와 관련성을 기반으로 AI 생성 응답의 품질을 평가합니다.\n",
    "\n",
    "        Args:\n",
    "        question (str): 사용자의 원본 질문.\n",
    "        response (str): 평가 대상인 AI 생성 응답.\n",
    "        true_answer (str): 정답으로 사용되는 실제 답변.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[float, float]: (충실도_점수, 관련성_점수)를 포함하는 튜플.\n",
    "                                                각 점수는 1.0 (전체), 0.5 (부분), 또는 0.0 (없음) 중 하나입니다.\n",
    "        \"\"\"\n",
    "        # 평가 프롬프트 포맷팅\n",
    "        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response, \n",
    "                true_answer=true_answer,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "        \n",
    "        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "\n",
    "        # 모델에 충실도 평가 요청\n",
    "        faithfulness_response = client.chat.completions.create(\n",
    "               model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"당신은 객관적인 평가자입니다. 숫자 점수만 반환하세요.\"},\n",
    "                        {\"role\": \"user\", \"content\": faithfulness_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # 모델에 관련성 평가 요청\n",
    "        relevancy_response = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"당신은 객관적인 평가자입니다. 숫자 점수만 반환하세요.\"},\n",
    "                        {\"role\": \"user\", \"content\": relevancy_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # 점수 추출 및 잠재적 파싱 오류 처리\n",
    "        try:\n",
    "                faithfulness_score = float(faithfulness_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"경고: 충실도 점수를 파싱할 수 없어 기본값 0으로 설정합니다.\")\n",
    "                faithfulness_score = 0.0\n",
    "                \n",
    "        try:\n",
    "                relevancy_score = float(relevancy_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"경고: 관련성 점수를 파싱할 수 없어 기본값 0으로 설정합니다.\")\n",
    "                relevancy_score = 0.0\n",
    "\n",
    "        return faithfulness_score, relevancy_score\n",
    "\n",
    "# 첫 번째 검증 데이터에 대한 실제 답변\n",
    "true_answer = data[3]['ideal_answer']\n",
    "\n",
    "# 청크 크기 256과 128에 대한 응답 평가\n",
    "faithfulness, relevancy = evaluate_response(query, ai_responses_dict[256], true_answer)\n",
    "faithfulness2, relevancy2 = evaluate_response(query, ai_responses_dict[128], true_answer)\n",
    "\n",
    "# 평가 점수 출력\n",
    "print(f\"충실도 점수 (청크 크기 256): {faithfulness}\")\n",
    "print(f\"관련성 점수 (청크 크기 256): {relevancy}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"충실도 점수 (청크 크기 128): {faithfulness2}\")\n",
    "print(f\"관련성 점수 (청크 크기 128): {relevancy2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}