{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# ê°„ë‹¨í•œ RAGì—ì„œì˜ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ í—¤ë”(CCH)\n",
    "\n",
    "ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì€ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ì „ì— ê´€ë ¨ ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ì  ì •í™•ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í‘œì¤€ì ì¸ ì²­í‚¹ ë°©ì‹ì€ ì¢…ì¢… ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì–´ë²„ë ¤ ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ë–¨ì–´ëœ¨ë¦½ë‹ˆë‹¤.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸ ì²­í¬ í—¤ë”(CCH)ëŠ” ê° ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê¸° ì „ì— ë¬¸ì„œ ì œëª©ì´ë‚˜ ì„¹ì…˜ í—¤ë”ì™€ ê°™ì€ ìƒìœ„ ìˆ˜ì¤€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì•ì— ì¶”ê°€í•˜ì—¬ RAGë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ëŠ” ê²€ìƒ‰ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê³  ë¬¸ë§¥ì—ì„œ ë²—ì–´ë‚œ ì‘ë‹µì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "\n",
    "> ğŸ’¡ **ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…:** ì¼ë°˜ì ì¸ RAGëŠ” ì±…ì˜ ë‚´ìš©ì„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ í•„ìš”í•  ë•Œ ì°¾ì•„ë³´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ ê° í˜ì´ì§€ê°€ ì–´ë–¤ ì¥ì— ì†í•´ ìˆì—ˆëŠ”ì§€ ìŠì–´ë²„ë¦¬ê¸° ì‰½ìŠµë‹ˆë‹¤. 'ì»¨í…ìŠ¤íŠ¸ ì²­í¬ í—¤ë”'ëŠ” ê° í˜ì´ì§€(ì²­í¬) ì•ì— '1ì¥: ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬'ì™€ ê°™ì€ ì¥ ì œëª©(í—¤ë”)ì„ ë¶™ì—¬ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ AIê°€ ê²€ìƒ‰í•  ë•Œ ê° ë‚´ìš© ì¡°ê°ì´ ì–´ë–¤ ë§¥ë½ì— ì†í•˜ëŠ”ì§€ ë” ì˜ ì´í•´í•˜ê²Œ ë˜ì–´, í›¨ì”¬ ë” ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ì´ ë…¸íŠ¸ë¶ì˜ ë‹¨ê³„:\n",
    "\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "2. **ì»¨í…ìŠ¤íŠ¸ í—¤ë”ë¥¼ ì‚¬ìš©í•œ ì²­í‚¹**: ì„¹ì…˜ ì œëª©ì„ ì¶”ì¶œí•˜ì—¬ ì²­í¬ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "3. **ì„ë² ë”© ìƒì„±**: ì»¨í…ìŠ¤íŠ¸ê°€ ê°•í™”ëœ ì²­í¬ë¥¼ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "4. **ì‹œë§¨í‹± ê²€ìƒ‰**: ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "5. **ì‘ë‹µ ìƒì„±**: ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "6. **í‰ê°€**: ì±„ì  ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„¹ì…˜ í—¤ë” ì‹ë³„\n",
    "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ë©´ì„œ ì„¹ì…˜ ì œëª©(ì²­í¬ì˜ ì ì¬ì  í—¤ë”)ë„ ì‹ë³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ.\n",
    "\n",
    "    Returns:\n",
    "    str: PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸.\n",
    "    \"\"\"\n",
    "    # PDF íŒŒì¼ì„ ì—½ë‹ˆë‹¤\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¹ˆ ë¬¸ìì—´ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "\n",
    "    # PDFì˜ ê° í˜ì´ì§€ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
    "        text = page.get_text(\"text\")  # í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
    "        all_text += text  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ all_text ë¬¸ìì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "\n",
    "    return all_text  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "ì„ë² ë”©ê³¼ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ìœ„í•´ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ URLê³¼ API í‚¤ë¡œ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì»¨í…ìŠ¤íŠ¸ í—¤ë”ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì²­í‚¹\n",
    "ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ì²­í¬ì— ëŒ€í•œ ì„¤ëª…ì ì¸ í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "> ğŸ’¡ **ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…:** ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¨ìˆœíˆ ê¸°ì¡´ ë¬¸ì„œì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ë„˜ì–´, LLMì„ ì‹œì¼œ ê° í…ìŠ¤íŠ¸ ì¡°ê°ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ìƒˆë¡œìš´ 'ë¯¸ë‹ˆ ì œëª©(í—¤ë”)'ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì— ëŒ€í•œ ê¸´ ë‹¨ë½ì´ ìˆë‹¤ë©´, LLMì´ \"AI ìœ¤ë¦¬ì˜ ì¤‘ìš”ì„±ê³¼ í¸í–¥ ë¬¸ì œ\"ì™€ ê°™ì€ ê°„ê²°í•œ í—¤ë”ë¥¼ ìƒì„±í•´ì¤ë‹ˆë‹¤. ì´ í—¤ë”ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì„ë² ë”©ë˜ì–´, ê²€ìƒ‰ ì‹œ ë” ì •í™•í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_header(chunk, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    LLMì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì²­í¬ì— ëŒ€í•œ ì œëª©/í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    chunk (str): í—¤ë”ë¡œ ìš”ì•½í•  í…ìŠ¤íŠ¸ ì²­í¬.\n",
    "    model (str): í—¤ë” ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸. ê¸°ë³¸ê°’ì€ \"meta-llama/Llama-3.2-3B-Instruct\"ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "    str: ìƒì„±ëœ í—¤ë”/ì œëª©.\n",
    "    \"\"\"\n",
    "    # AIì˜ í–‰ë™ì„ ì•ˆë‚´í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n",
    "    system_prompt = \"ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •ë³´ì„± ìˆëŠ” ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.\"\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ìƒì„±ëœ í—¤ë”/ì œëª©ì„ ë°˜í™˜í•˜ë˜, ì•ë’¤ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_headers(text, n, overlap):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ê³  í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    text (str): ì²­í¬ë¡œ ë‚˜ëˆŒ ì „ì²´ í…ìŠ¤íŠ¸.\n",
    "    n (int): ì²­í¬ í¬ê¸°(ë¬¸ì ìˆ˜).\n",
    "    overlap (int): ì²­í¬ ê°„ì˜ ì¤‘ì²© ë¬¸ì ìˆ˜.\n",
    "\n",
    "    Returns:\n",
    "    List[dict]: 'header'ì™€ 'text' í‚¤ë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.\n",
    "    \"\"\"\n",
    "    chunks = []  # ì²­í¬ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "\n",
    "    # ì§€ì •ëœ ì²­í¬ í¬ê¸°ì™€ ì¤‘ì²©ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunk = text[i:i + n]  # í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
    "        header = generate_chunk_header(chunk)  # LLMì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ì— ëŒ€í•œ í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n",
    "        chunks.append({\"header\": header, \"text\": chunk})  # í—¤ë”ì™€ ì²­í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "\n",
    "    return chunks  # í—¤ë”ê°€ ìˆëŠ” ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹\n",
    "ì´ì œ PDFë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë‹¤ìŒ, ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Chunk:\n",
      "Header: \"Introduction to Artificial Intelligence: Understanding the Foundations and Evolution\"\n",
      "Content: Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "# PDF íŒŒì¼ ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# í—¤ë”ì™€ í•¨ê»˜ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤\n",
    "# ì²­í¬ í¬ê¸°ëŠ” 1000ì, ì¤‘ì²©ì€ 200ìë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "text_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
    "\n",
    "# ìƒì„±ëœ í—¤ë”ì™€ í•¨ê»˜ ìƒ˜í”Œ ì²­í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤\n",
    "print(\"Sample Chunk:\")\n",
    "print(\"Header:\", text_chunks[0]['header']) \n",
    "print(\"Content:\", text_chunks[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í—¤ë”ì™€ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ í—¤ë”ì™€ í…ìŠ¤íŠ¸ ëª¨ë‘ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    text (str): ì„ë² ë”©í•  ì…ë ¥ í…ìŠ¤íŠ¸.\n",
    "    model (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸. ê¸°ë³¸ê°’ì€ \"BAAI/bge-en-icl\"ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "    dict: ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ í¬í•¨í•˜ëŠ” ì‘ë‹µ.\n",
    "    \"\"\"\n",
    "    # ì§€ì •ëœ ëª¨ë¸ê³¼ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    # ì‘ë‹µì—ì„œ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [02:56<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "embeddings = []  # ì„ë² ë”©ì„ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "\n",
    "# ì§„í–‰ë¥  í‘œì‹œì¤„ê³¼ í•¨ê»˜ ê° í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤\n",
    "for chunk in tqdm(text_chunks, desc=\"Generating embeddings\"):\n",
    "    # ì²­í¬ì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "    text_embedding = create_embeddings(chunk[\"text\"])\n",
    "    # ì²­í¬ì˜ í—¤ë”ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "    header_embedding = create_embeddings(chunk[\"header\"])\n",
    "    # ì²­í¬ì˜ í—¤ë”, í…ìŠ¤íŠ¸ ë° í•´ë‹¹ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹œë§¨í‹± ê²€ìƒ‰ ìˆ˜í–‰\n",
    "ì‚¬ìš©ì ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì°¾ê¸° ìœ„í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "> ğŸ’¡ **ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…:** ê²€ìƒ‰ ì‹œ, ìš°ë¦¬ëŠ” ì§ˆë¬¸(ì¿¼ë¦¬)ê³¼ (1)ì²­í¬ì˜ ì›ë³¸ ë‚´ìš©, (2)ì²­í¬ì˜ ìš”ì•½ í—¤ë” ê°ê°ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ ë‘ ìœ ì‚¬ë„ ì ìˆ˜ì˜ í‰ê· ì„ ë‚´ì–´ ìµœì¢… ê´€ë ¨ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì›ë³¸ ë‚´ìš©ì˜ ì„¸ë¶€ ì •ë³´ì™€ í—¤ë”ì˜ í•µì‹¬ ìš”ì•½ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆì–´, ê²€ìƒ‰ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): ì²« ë²ˆì§¸ ë²¡í„°.\n",
    "    vec2 (np.ndarray): ë‘ ë²ˆì§¸ ë²¡í„°.\n",
    "\n",
    "    Returns:\n",
    "    float: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, chunks, k=5):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    query (str): ì‚¬ìš©ì ì¿¼ë¦¬.\n",
    "    chunks (List[dict]): ì„ë² ë”©ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸.\n",
    "    k (int): ìƒìœ„ ê²°ê³¼ ìˆ˜.\n",
    "\n",
    "    Returns:\n",
    "    List[dict]: ìƒìœ„ kê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬.\n",
    "    \"\"\"\n",
    "    # ì¿¼ë¦¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    similarities = []  # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "    \n",
    "    # ê° ì²­í¬ë¥¼ ë°˜ë³µí•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\n",
    "    for chunk in chunks:\n",
    "        # ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ì²­í¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\n",
    "        sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"]))\n",
    "        # ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ì²­í¬ í—¤ë” ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\n",
    "        sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[\"header_embedding\"]))\n",
    "        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\n",
    "        avg_similarity = (sim_text + sim_header) / 2\n",
    "        # ì²­í¬ì™€ í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "        similarities.append((chunk, avg_similarity))\n",
    "\n",
    "    # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # ìƒìœ„ kê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\n",
    "    return [x[0] for x in similarities[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ì¶œëœ ì²­í¬ì— ëŒ€í•´ ì¿¼ë¦¬ ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Header 1: \"Building Trust in AI: Addressing Transparency, Explainability, and Accountability\"\n",
      "Content:\n",
      "systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy. \n",
      "Privacy and Data Protection \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
      "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
      "and complying with data protection regulations are crucial. \n",
      "Accountability and Responsibility \n",
      "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
      "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
      "developers, deployers, and users of AI systems. \n",
      "Chapter 20: Building Trust in AI \n",
      "Transparency and Explainability \n",
      "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "\n",
      "\n",
      "Header 2: \"Building Trust in AI: Essential Factors for Reliability and Fairness\"\n",
      "Content:\n",
      "to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "Ensuring that AI systems are robust and reliable is essential for building trust. This includes \n",
      "testing and validating AI models, monitoring their performance, and addressing potential \n",
      "vulnerabilities. \n",
      "User Control and Agency \n",
      "Empowering users with control over AI systems and providing them with agency in their \n",
      "interactions with AI enhances trust. This includes allowing users to customize AI settings, \n",
      "understand how their data is used, and opt out of AI-driven features. \n",
      "Ethical Design and Development \n",
      "Incorporating ethical considerations into the design and development of AI systems is crucial for \n",
      "building trust. This includes conducting ethical impact assessments, engaging stakeholders, and \n",
      "adhering to ethical guidelines and standards. \n",
      "Public Engagement and Education \n",
      "Engaging th\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query = data[0]['question']\n",
    "\n",
    "# ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ 2ê°œ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤\n",
    "top_chunks = semantic_search(query, embeddings, k=2)\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤\n",
    "print(\"Query:\", query)\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Header {i+1}: {chunk['header']}\")\n",
    "    print(f\"Content:\n{chunk['text']}\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI ì–´ì‹œìŠ¤í„´íŠ¸ì— ëŒ€í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n",
    "system_prompt = \"ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ì—„ê²©í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ë‹µë³€ì„ ë„ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš°, 'ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): AIì˜ í–‰ë™ì„ ì•ˆë‚´í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸.\n",
    "    user_message (str): ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ë˜ëŠ” ì¿¼ë¦¬.\n",
    "    model (str): ì‘ë‹µ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸. ê¸°ë³¸ê°’ì€ \"meta-llama/Llama-2-7B-chat-hf\"ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "    dict: AI ëª¨ë¸ì˜ ì‘ë‹µ.\n",
    "    \"\"\"\n",
    "    response = client.chat.com.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# ìƒìœ„ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n",
    "user_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI ì‘ë‹µ í‰ê°€í•˜ê¸°\n",
    "AIì˜ ì‘ë‹µì„ ê¸°ëŒ€ ë‹µë³€ê³¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n",
    "evaluate_system_prompt = \"\"\"ë‹¹ì‹ ì€ ì§€ëŠ¥í˜• í‰ê°€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. \n",
    "Assess the AI assistant's response based on the provided context. \n",
    "- Assign a score of 1 if the response is very close to the true answer. \n",
    "- Assign a score of 0.5 if the response is partially correct. \n",
    "- Assign a score of 0 if the response is incorrect.\n",
    "Return only the score (0, 0.5, or 1).\"\"\"\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì—ì„œ ì •ë‹µì„ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
    "true_answer = data[0]['ideal_answer']\n",
    "\n",
    "# í‰ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤\n",
    "evaluation_prompt = f\"\"\"\n",
    "User Query: {query}\n",
    "AI Response: {ai_response}\n",
    "True Answer: {true_answer}\n",
    "{evaluate_system_prompt}\n",
    "\"\"\"\n",
    "\n",
    "# í‰ê°€ ì ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# í‰ê°€ ì ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤\n",
    "print(\"Evaluation Score:\", evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}